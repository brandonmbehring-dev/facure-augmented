{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# 19.1 The Evaluation Challenge\n\n**Chapter**: 19 - Evaluating Causal Models  \n**Section**: 1 - The Evaluation Challenge  \n**Facure Source**: 19-Evaluating-Causal-Models.ipynb  \n**Version**: 1.0.0  \n**Last Validated**: 2026-01-16\n\n---\n\n## Table of Contents\n\n1. [Facure's Intuition](#1-facures-intuition)\n   - 1.1 [The Fundamental Problem](#11-the-fundamental-problem)\n   - 1.2 [Why It's Hard](#12-why-its-hard)\n2. [Formal Treatment](#2-formal-treatment)\n   - 2.1 [What We Want vs What We Can See](#21-what-we-want-vs-what-we-can-see)\n   - 2.2 [Aggregate Estimation](#22-aggregate-estimation)\n3. [Setup: Train Causal, Evaluate Causal](#3-setup-train-causal-evaluate-causal)\n   - 3.1 [Data Setup](#31-data-setup)\n   - 3.2 [Build Candidate Models](#32-build-candidate-models)\n4. [Numeric Demonstration](#4-numeric-demonstration)\n5. [Interview Appendix](#5-interview-appendix)\n6. [References](#6-references)\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(123)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## 1. Facure's Intuition\n\n> **Interview Relevance**: Evaluating causal models is a critical skill that distinguishes practitioners from academics. Expect questions about how to validate CATE models in practice.\n\n### 1.1 The Fundamental Problem\n\nIn ML, evaluation is straightforward:\n1. Train model on train set\n2. Predict on test set\n3. Compare predictions to actual outcomes\n\n**But for causal inference, what is the \"actual outcome\"?**\n\nWe want to evaluate how good our model is at predicting $\\tau_i = Y_i(1) - Y_i(0)$.\n\nBut we **never observe both** $Y_i(1)$ and $Y_i(0)$ for the same unit!\n\n```\nML vs Causal Evaluation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n  ML Prediction:\n    - Predict \u0176\n    - Compare to actual Y\n    - Compute RMSE, R\u00b2, etc.\n    \n  Causal (CATE):\n    - Predict \u03c4\u0302(x) = E[Y(1) - Y(0) | X]\n    - Compare to... what?\n    - \u03c4\u1d62 = Y(1) - Y(0) is NEVER OBSERVED!\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n\n### 1.2 Why It's Hard\n\nThis is why causal inference is harder to adopt than ML:\n\n- **ML**: Easy to prove your model works (\"look at the test set accuracy!\")\n- **Causal**: How do you convince management your CATE model is good?\n\nFacure calls this the missing piece:\n\n> \"It's beyond my comprehension why we don't see any material whatsoever explaining how we should evaluate causal inference models with real data.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fundamental problem\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: ML evaluation (easy)\n",
    "ax = axes[0]\n",
    "np.random.seed(42)\n",
    "y_true = np.random.uniform(100, 300, 50)\n",
    "y_pred = y_true + np.random.normal(0, 15, 50)\n",
    "ax.scatter(y_true, y_pred, alpha=0.6, s=40, c=COLORS['blue'])\n",
    "ax.plot([100, 300], [100, 300], 'k--', lw=1.5, label='Perfect')\n",
    "ax.set_xlabel('Actual Y')\n",
    "ax.set_ylabel('Predicted \u0176')\n",
    "ax.set_title('(a) ML Prediction: Easy to Evaluate', fontweight='bold')\n",
    "r2 = 1 - np.var(y_pred - y_true) / np.var(y_true)\n",
    "ax.text(0.05, 0.95, f'R\u00b2 = {r2:.2f}', transform=ax.transAxes, fontsize=11)\n",
    "\n",
    "# Panel 2: Causal evaluation (hard)\n",
    "ax = axes[1]\n",
    "tau_pred = np.random.uniform(-5, 2, 50)\n",
    "ax.scatter(np.arange(50), tau_pred, alpha=0.6, s=40, c=COLORS['green'], label='\u03c4\u0302(x) predicted')\n",
    "ax.axhline(0, color='black', ls=':', lw=1)\n",
    "ax.text(25, 1.5, '\u03c4\u1d62 = ???', fontsize=16, ha='center', fontweight='bold', color=COLORS['red'])\n",
    "ax.text(25, -4, 'Individual effects\\nare UNOBSERVED', fontsize=10, ha='center', style='italic')\n",
    "ax.set_xlabel('Unit i')\n",
    "ax.set_ylabel('Treatment Effect')\n",
    "ax.set_title('(b) Causal (CATE): Hard to Evaluate', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe fundamental problem: We can't compare individual CATE predictions to ground truth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "---\n\n## 2. Formal Treatment\n\n### 2.1 What We Want vs What We Can See\n\n**What we want to evaluate**:\n$$\\text{Error}_i = \\hat{\\tau}(X_i) - \\tau_i = \\hat{\\tau}(X_i) - [Y_i(1) - Y_i(0)]$$\n\n**What we observe**:\n$$Y_i = T_i \\cdot Y_i(1) + (1-T_i) \\cdot Y_i(0)$$\n\nWe only see **one** potential outcome \u2014 not both!\n\n### 2.2 Aggregate Estimation\n\n**The key insight**: While we can't see individual $\\tau_i$, we CAN estimate **aggregate** treatment effects for groups.\n\nIf treatment is randomized within a group, then:\n\n$$\\hat{\\tau}_{group} = \\bar{Y}_{treated} - \\bar{Y}_{control}$$\n\nThis is unbiased for the ATE in that group!\n\n**Evaluation strategy**:\n1. Use CATE model to create groups (bands)\n2. Estimate actual treatment effect within each band\n3. Good model \u2192 bands have **different** treatment effects\n4. Random model \u2192 all bands have **same** treatment effect (\u2248 overall ATE)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize aggregate vs individual estimation\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Simulate group-level estimation\n",
    "np.random.seed(42)\n",
    "n_per_group = 100\n",
    "\n",
    "# Three groups with different true CATEs\n",
    "groups = [\n",
    "    ('High Effect', 2.5, COLORS['green']),\n",
    "    ('Medium Effect', 1.0, COLORS['blue']),\n",
    "    ('Low Effect', 0.2, COLORS['red'])\n",
    "]\n",
    "\n",
    "x_pos = 0\n",
    "for name, true_tau, color in groups:\n",
    "    # Simulate randomized data in this group\n",
    "    T = np.random.binomial(1, 0.5, n_per_group)\n",
    "    Y0 = np.random.normal(10, 2, n_per_group)\n",
    "    Y1 = Y0 + true_tau + np.random.normal(0, 0.5, n_per_group)\n",
    "    Y = T * Y1 + (1-T) * Y0\n",
    "    \n",
    "    # Estimate treatment effect\n",
    "    tau_hat = Y[T==1].mean() - Y[T==0].mean()\n",
    "    \n",
    "    # Plot\n",
    "    ax.bar(x_pos, tau_hat, width=0.8, color=color, alpha=0.7, \n",
    "           label=f'{name}: \u03c4\u0302={tau_hat:.2f} (true={true_tau})')\n",
    "    ax.errorbar(x_pos, tau_hat, yerr=0.3, fmt='none', color='black', capsize=5)\n",
    "    x_pos += 1\n",
    "\n",
    "ax.set_xticks([0, 1, 2])\n",
    "ax.set_xticklabels(['High Effect', 'Medium Effect', 'Low Effect'])\n",
    "ax.set_ylabel('Estimated Treatment Effect')\n",
    "ax.set_title('Key Insight: We Can Estimate ATE Within Groups', fontweight='bold')\n",
    "ax.axhline(0, color='black', ls=':', lw=1)\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nIf a CATE model creates these groups, we can verify the groups have different effects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "---\n\n## 3. Implementation: Train Causal, Evaluate Causal\n\nOur evaluation strategy:\n1. Train models on **non-random** data (observational)\n2. Evaluate on **random** data (where ATE estimation is valid)\n\nThis is realistic: random data is expensive, so we save it for evaluation.\n\n### 3.1 Data Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Non-random: for training (observational)\n",
    "# Random: for evaluation (prices randomized)\n",
    "prices = load_facure_data('ice_cream_sales.csv')  # Non-random\n",
    "prices_rnd = load_facure_data('ice_cream_sales_rnd.csv')  # Random\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining (non-random): {len(prices):,} observations\")\n",
    "print(f\"Evaluation (random):   {len(prices_rnd):,} observations\")\n",
    "\n",
    "print(\"\\nNon-random data (price correlates with other factors):\")\n",
    "print(prices[['temp', 'price', 'sales']].corr().round(3))\n",
    "\n",
    "print(\"\\nRandom data (price is randomized):\")\n",
    "print(prices_rnd[['temp', 'price', 'sales']].corr().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "### 3.2 Build Candidate Models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: CATE model (regression with interactions)\n",
    "m1 = smf.ols(\"sales ~ price*cost + price*C(weekday) + price*temp\", data=prices).fit()\n",
    "\n",
    "# Model 2: Predictive model (ML, no causal structure)\n",
    "X = [\"temp\", \"weekday\", \"cost\", \"price\"]\n",
    "y = \"sales\"\n",
    "m2 = GradientBoostingRegressor(max_depth=5, n_estimators=100, random_state=42)\n",
    "m2.fit(prices[X], prices[y])\n",
    "\n",
    "# Model 3: Random model (baseline)\n",
    "# Predictions are random numbers\n",
    "\n",
    "print(\"Models trained:\")\n",
    "print(\"  M1: CATE model (regression with interactions)\")\n",
    "print(\"  M2: Predictive model (GradientBoosting)\")\n",
    "print(\"  M3: Random model (baseline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on random (evaluation) data\n",
    "def predict_sensitivity(model, df, h=0.01):\n",
    "    \"\"\"Numerical derivative approximation for sensitivity.\"\"\"\n",
    "    df_plus = df.copy()\n",
    "    df_plus['price'] = df['price'] + h\n",
    "    return (model.predict(df_plus) - model.predict(df)) / h\n",
    "\n",
    "prices_rnd_pred = prices_rnd.copy()\n",
    "prices_rnd_pred['sensitivity_m_pred'] = predict_sensitivity(m1, prices_rnd)\n",
    "prices_rnd_pred['pred_m_pred'] = m2.predict(prices_rnd[X])\n",
    "prices_rnd_pred['rand_m_pred'] = np.random.uniform(size=len(prices_rnd))\n",
    "\n",
    "print(\"Predictions added to evaluation data:\")\n",
    "print(prices_rnd_pred[['temp', 'price', 'sales', \n",
    "                       'sensitivity_m_pred', 'pred_m_pred', 'rand_m_pred']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "---\n\n## 4. Numeric Demonstration\n\nLet's verify we have sensible predictions before moving to evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of predictions\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col, name in [('sensitivity_m_pred', 'CATE Model'), \n",
    "                   ('pred_m_pred', 'Predictive Model'),\n",
    "                   ('rand_m_pred', 'Random Model')]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean: {prices_rnd_pred[col].mean():.3f}\")\n",
    "    print(f\"  Std:  {prices_rnd_pred[col].std():.3f}\")\n",
    "    print(f\"  Min:  {prices_rnd_pred[col].min():.3f}\")\n",
    "    print(f\"  Max:  {prices_rnd_pred[col].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predictions\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Panel 1: CATE predictions\n",
    "ax = axes[0]\n",
    "ax.hist(prices_rnd_pred['sensitivity_m_pred'], bins=40, color=COLORS['green'], \n",
    "        alpha=0.7, edgecolor='white')\n",
    "ax.axvline(0, color='black', ls=':', lw=1.5)\n",
    "ax.set_xlabel('Predicted Sensitivity')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('(a) CATE Model: Sensitivity Predictions', fontweight='bold')\n",
    "\n",
    "# Panel 2: Predictive model\n",
    "ax = axes[1]\n",
    "ax.hist(prices_rnd_pred['pred_m_pred'], bins=40, color=COLORS['blue'], \n",
    "        alpha=0.7, edgecolor='white')\n",
    "ax.axvline(prices_rnd_pred['sales'].mean(), color='black', ls=':', lw=1.5,\n",
    "          label=f'Mean sales: {prices_rnd_pred[\"sales\"].mean():.0f}')\n",
    "ax.set_xlabel('Predicted Sales')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('(b) Predictive Model: Sales Predictions', fontweight='bold')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 3: Random model\n",
    "ax = axes[2]\n",
    "ax.hist(prices_rnd_pred['rand_m_pred'], bins=40, color=TUFTE_PALETTE['primary'], \n",
    "        alpha=0.7, edgecolor='white')\n",
    "ax.set_xlabel('Random Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('(c) Random Model: Uniform Noise', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key question: Which model creates the best segments?\n",
    "print(\"=\" * 60)\n",
    "print(\"THE EVALUATION QUESTION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "We have three models with different predictions.\n",
    "\n",
    "The question: Which one best segments units by ACTUAL treatment effect?\n",
    "\n",
    "To answer this, we need to:\n",
    "1. Create bands (segments) from each model's predictions\n",
    "2. Estimate actual treatment effect in each band\n",
    "3. Better model = bands with MORE DIFFERENT treatment effects\n",
    "\n",
    "This is what we'll cover in the next notebooks:\n",
    "- 02: Sensitivity by band (visual evaluation)\n",
    "- 03: Cumulative gain curves (quantitative evaluation)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "```\nKey Takeaways \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n1. Individual CATE is unobservable\n   - Can't compute \u03c4\u0302(x) - \u03c4\u1d62 directly\n   - No simple ground truth for evaluation\n\n2. But aggregate ATE IS estimable\n   - With randomized data: ATE = \u0232(1) - \u0232(0)\n   - Can estimate within any subgroup!\n\n3. Evaluation strategy\n   - Create bands from model predictions\n   - Estimate actual ATE within each band\n   - Good model \u2192 bands have different ATEs\n   \n4. Setup\n   - Train on observational data (abundant)\n   - Evaluate on randomized data (precious)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "---\n\n## 5. Interview Appendix\n\n### Practice Questions\n\n**Q1 (Meta E5, DS)**: *\"How do you evaluate a CATE model when you can't observe individual treatment effects?\"*\n\n<details>\n<summary>Solution</summary>\n\n**The challenge**: Individual $\\tau_i = Y_i(1) - Y_i(0)$ is never observed.\n\n**The solution**: Evaluate at the **group level**.\n\n1. **Create bands** from CATE predictions:\n   - Sort units by predicted $\\hat{\\tau}(x)$\n   - Split into quantiles (e.g., high/medium/low)\n\n2. **Estimate actual ATE** in each band:\n   - With randomized data: $\\hat{\\tau}_{band} = \\bar{Y}_{T=1} - \\bar{Y}_{T=0}$\n   - This is unbiased for the true ATE in that band\n\n3. **Compare bands**:\n   - Good model: Bands have different treatment effects\n   - Random model: All bands have same ATE (\u2248 overall ATE)\n\n**Key requirement**: Need randomized data for evaluation (or valid causal identification).\n\n**Metrics**:\n- Sensitivity by band plots\n- Cumulative gain curves\n- AUUC (Area Under Uplift Curve)\n\n</details>\n\n---\n\n**Q2 (Amazon L6, Econ)**: *\"Why do we need randomized data to evaluate CATE models?\"*\n\n<details>\n<summary>Solution</summary>\n\n**The core issue**: Evaluating CATE requires estimating actual treatment effects in subgroups.\n\n**With randomized data**:\n- Treatment is independent of potential outcomes: $(Y(0), Y(1)) \\perp T$\n- Simple difference is unbiased: $E[Y|T=1] - E[Y|T=0] = E[Y(1) - Y(0)]$\n- Works within any subgroup defined by X\n\n**Without randomization**:\n- Confounding: $E[Y|T=1] - E[Y|T=0] \\neq E[Y(1) - Y(0)]$\n- Need to control for confounders\n- But: controlling for confounders in evaluation is tricky\n- Risk of overfitting the evaluation itself\n\n**Practical setup**:\n- Observational data is abundant \u2192 use for training\n- Randomized data is expensive \u2192 save for evaluation\n- Even a small randomized holdout is valuable\n\n**Alternative**: If no randomized data, use synthetic data or A/B tests.\n\n</details>\n\n---\n\n**Q3 (Google L5, Quant)**: *\"What's wrong with using prediction accuracy (R\u00b2, RMSE) to evaluate CATE models?\"*\n\n<details>\n<summary>Solution</summary>\n\n**The problem**: Prediction accuracy \u2260 treatment effect accuracy.\n\n1. **Different targets**:\n   - R\u00b2 measures how well you predict Y\n   - CATE measures how Y changes with T\n   - A model can predict Y well but miss the causal effect\n\n2. **Example**:\n   - Model A: R\u00b2 = 0.95, but ignores treatment entirely\n   - Model B: R\u00b2 = 0.80, but captures treatment heterogeneity\n   - Model A has better R\u00b2 but is useless for treatment decisions!\n\n3. **What we need to evaluate**:\n   - Not: \"Does the model predict Y well?\"\n   - But: \"Does the model separate units by treatment effect?\"\n\n4. **Correct evaluation**:\n   - Create bands from model predictions\n   - Check if bands have different actual treatment effects\n   - Use cumulative gain curves, not R\u00b2\n\n**Analogy**: Evaluating a weather model by how well it predicts temperature doesn't tell you if it captures how temperature affects umbrella sales.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "---\n\n## 6. References\n\n[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 19: \"Evaluating Causal Models.\"\n\n[^2]: Gutierrez, P. and G\u00e9rardy, J. Y. (2017). Causal Inference and Uplift Modeling: A Review of the Literature. *JMLR Workshop and Conference Proceedings*, 67, 1-13.\n\n[^3]: Athey, S. and Imbens, G. W. (2016). Recursive partitioning for heterogeneous causal effects. *PNAS*, 113(27), 7353-7360.\n\n[^4]: Radcliffe, N. J. (2007). Using Control Groups to Target on Predicted Lift. *Direct Market Journal*, 3, 14-21.\n\n---\n\n**Next**: [02. Sensitivity by Band](./02_sensitivity_by_band.ipynb) \u2014 Visual evaluation of CATE models"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}