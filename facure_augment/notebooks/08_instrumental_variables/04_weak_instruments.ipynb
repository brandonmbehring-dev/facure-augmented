{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08.4 Weak Instruments\n",
    "\n",
    "**Chapter**: 8 - Instrumental Variables  \n",
    "**Section**: 4 - Weakness of Instruments  \n",
    "**Facure Source**: 08-Instrumental-Variables.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [Why Weak Instruments Matter](#11-why-weak-instruments-matter)\n",
    "   - 1.2 [Bias Toward OLS](#12-bias-toward-ols)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [Finite-Sample Bias of 2SLS](#21-finite-sample-bias-of-2sls)\n",
    "   - 2.2 [Stock-Yogo Critical Values](#22-stock-yogo-critical-values)\n",
    "   - 2.3 [LIML as Alternative](#23-liml-as-alternative)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [Simulation: Varying Instrument Strength](#31-simulation-varying-instrument-strength)\n",
    "   - 3.2 [Variance Explosion with Weak Instruments](#32-variance-explosion-with-weak-instruments)\n",
    "   - 3.3 [2SLS vs LIML Comparison](#33-2sls-vs-liml-comparison)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    ols_summary_table,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from linearmodels.iv import IV2SLS, IVLIML\n",
    "from scipy import stats\n",
    "\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: Weak instrument detection is a critical skill. The \"F > 10\" rule of thumb is ubiquitous, but understanding *why* and *when* it fails separates experts from practitioners.\n",
    "\n",
    "### 1.1 Why Weak Instruments Matter\n",
    "\n",
    "As Facure explains: *\"When dealing with IV, we need to remember we are estimating the ATE indirectly. Our estimates depend on both the first stage and the second stage. If the impact of the treatment on the outcome is indeed strong, the second stage will also be strong. However, it doesn't matter how strong the second stage is if we have a weak first stage.\"*\n",
    "\n",
    "A **weak first stage** means the instrument has only a very small correlation with the treatment. Therefore, we can't learn much about the treatment from the instrument.\n",
    "\n",
    "### 1.2 Bias Toward OLS\n",
    "\n",
    "**Key insight**: 2SLS is **biased toward OLS** in finite samples!\n",
    "\n",
    "$$\\text{2SLS bias} \\approx \\frac{\\text{OLS bias}}{1 + F}$$\n",
    "\n",
    "where $F$ is the first-stage F-statistic.\n",
    "\n",
    "**Implications:**\n",
    "- If OLS has positive bias, 2SLS will also have positive bias\n",
    "- If OLS has negative bias, 2SLS will also have negative bias\n",
    "- As $F \\to \\infty$, 2SLS bias \u2192 0 (consistency)\n",
    "- As $F \\to 0$, 2SLS bias \u2192 OLS bias (worthless!)\n",
    "\n",
    "\u2605 Insight \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "- 2SLS is **consistent** but **biased** in finite samples\n",
    "- Bias increases with more instruments (more paths to OLS)\n",
    "- Just-identified IV (one instrument) is **median-unbiased**\n",
    "- Rule of thumb: F > 10 for \"safe\" zone (Stock, Wright, Yogo 2002)\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 Finite-Sample Bias of 2SLS\n",
    "\n",
    "Consider the structural model:\n",
    "\n",
    "$$y_i = \\rho s_i + u_i \\quad \\text{with } \\text{Cov}(s_i, u_i) = \\sigma_{su}$$\n",
    "\n",
    "and first stage:\n",
    "\n",
    "$$s_i = \\pi' z_i + \\xi_i$$\n",
    "\n",
    "**Approximate bias formula** (Bound, Jaeger, Baker 1995):\n",
    "\n",
    "$$E[\\hat{\\rho}_{2SLS}] - \\rho \\approx \\frac{\\sigma_{u\\xi}/\\sigma^2_{\\xi}}{\\mu^2/\\sigma^2_{\\pi}} \\cdot (K/n)$$\n",
    "\n",
    "where $K$ = number of instruments, $n$ = sample size, $\\mu^2/\\sigma^2_{\\pi}$ is the concentration parameter.\n",
    "\n",
    "**Simplified form**: The relative bias of 2SLS to OLS is approximately:\n",
    "\n",
    "$$\\frac{\\text{Bias}_{2SLS}}{\\text{Bias}_{OLS}} \\approx \\frac{K}{F}$$\n",
    "\n",
    "where $F$ is the first-stage F-statistic on excluded instruments.\n",
    "\n",
    "### 2.2 Stock-Yogo Critical Values\n",
    "\n",
    "Stock and Yogo (2005) provide critical values for the F-statistic based on:\n",
    "\n",
    "| Acceptable Relative Bias | Required F (one instrument) |\n",
    "|--------------------------|----------------------------|\n",
    "| 10% of OLS bias | F > 16.38 |\n",
    "| 15% of OLS bias | F > 8.96 |\n",
    "| 20% of OLS bias | F > 6.66 |\n",
    "| 25% of OLS bias | F > 5.53 |\n",
    "\n",
    "The common \"F > 10\" rule is a convenient approximation for 10-15% relative bias.\n",
    "\n",
    "### 2.3 LIML as Alternative\n",
    "\n",
    "**Limited Information Maximum Likelihood (LIML)** is an alternative to 2SLS that is:\n",
    "\n",
    "1. **Approximately median-unbiased** for over-identified models\n",
    "2. **Same asymptotic distribution** as 2SLS (so equally efficient asymptotically)\n",
    "3. **Less biased** than 2SLS with weak instruments\n",
    "4. **More variable** (higher variance) than 2SLS\n",
    "\n",
    "The LIML estimator minimizes:\n",
    "\n",
    "$$\\hat{\\rho}_{LIML} = \\arg\\min_{\\rho} \\frac{(y - s\\rho)'M_X(y - s\\rho)}{(y - s\\rho)'M_Z(y - s\\rho)}$$\n",
    "\n",
    "where $M_X$ and $M_Z$ are residual-maker matrices.\n",
    "\n",
    "**Practical advice** (Angrist & Pischke):\n",
    "1. Report first-stage F-statistic (bigger is better)\n",
    "2. Compare 2SLS with LIML \u2014 if similar, be happy; if different, worry\n",
    "3. Use just-identified IV with strongest single instrument\n",
    "4. Look at reduced form: if you can't see it there, it's probably not there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 Simulation: Varying Instrument Strength\n",
    "\n",
    "This simulation replicates Facure's demonstration of how IV estimates deteriorate with weaker instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facure's simulation setup\n",
    "np.random.seed(42)\n",
    "n = 10000\n",
    "\n",
    "# Generate base data\n",
    "X = np.random.normal(0, 2, n)  # Observable control\n",
    "U = np.random.normal(0, 2, n)  # Unobservable confounder\n",
    "T = np.random.normal(1 + 0.5 * U, 5, n)  # Endogenous treatment\n",
    "Y = np.random.normal(2 + X - 0.5 * U + 2 * T, 5, n)  # Outcome (TRUE EFFECT = 2.0)\n",
    "\n",
    "TRUE_EFFECT = 2.0\n",
    "\n",
    "# Generate instruments with varying strength\n",
    "# Higher std deviation = weaker correlation with T\n",
    "stddevs = np.linspace(0.1, 100, 50)\n",
    "instruments = {f\"Z_{i}\": np.random.normal(T, s, n) for i, s in enumerate(stddevs)}\n",
    "\n",
    "sim_data = pd.DataFrame({'X': X, 'U': U, 'T': T, 'Y': Y}).assign(**instruments)\n",
    "\n",
    "print(f\"Sample size: n = {n:,}\")\n",
    "print(f\"True causal effect: {TRUE_EFFECT}\")\n",
    "print(f\"Number of instrument variants: {len(instruments)}\")\n",
    "\n",
    "# Check correlation structure\n",
    "correlations = sim_data[[f'Z_{i}' for i in range(5)]].corrwith(sim_data['T'])\n",
    "print(f\"\\nCorrelation with T (first 5 instruments):\")\n",
    "print(correlations.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run IV with each instrument and collect results\n",
    "results = []\n",
    "\n",
    "for i, s in enumerate(stddevs):\n",
    "    z_col = f'Z_{i}'\n",
    "    \n",
    "    # Compute correlation\n",
    "    corr_tz = sim_data['T'].corr(sim_data[z_col])\n",
    "    \n",
    "    # Run first stage to get F-statistic\n",
    "    first_stage = smf.ols(f'T ~ X + {z_col}', data=sim_data).fit()\n",
    "    f_stat = first_stage.fvalue  # Joint F, but we want F on excluded instrument\n",
    "    \n",
    "    # Approximate F on excluded instrument\n",
    "    t_stat_z = first_stage.tvalues[z_col]\n",
    "    f_on_z = t_stat_z ** 2\n",
    "    \n",
    "    # Run 2SLS\n",
    "    formula = f'Y ~ 1 + X + [T ~ {z_col}]'\n",
    "    try:\n",
    "        iv_result = IV2SLS.from_formula(formula, sim_data).fit()\n",
    "        ate = iv_result.params['T']\n",
    "        se = iv_result.std_errors['T']\n",
    "    except Exception:\n",
    "        ate = np.nan\n",
    "        se = np.nan\n",
    "    \n",
    "    results.append({\n",
    "        'z_std': s,\n",
    "        'corr': corr_tz,\n",
    "        'f_stat': f_on_z,\n",
    "        'ate': ate,\n",
    "        'se': se,\n",
    "        'bias': ate - TRUE_EFFECT\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: SE by instrument strength\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: SE by correlation\n",
    "ax = axes[0]\n",
    "ax.scatter(results_df['corr'], results_df['se'], \n",
    "           c=TUFTE_PALETTE['treatment'], alpha=0.7, s=30)\n",
    "ax.set_xlabel('Corr(Z, T)')\n",
    "ax.set_ylabel('IV Standard Error')\n",
    "ax.set_title('(a) Variance Explosion with Weak Instruments')\n",
    "ax.axhline(0.1, color=TUFTE_PALETTE['control'], linestyle='--', \n",
    "           label='SE with strong IV', alpha=0.5)\n",
    "\n",
    "# Panel 2: ATE estimate with CI by correlation\n",
    "ax = axes[1]\n",
    "ax.scatter(results_df['corr'], results_df['ate'], \n",
    "           c=TUFTE_PALETTE['treatment'], alpha=0.7, s=30)\n",
    "ax.fill_between(results_df.sort_values('corr')['corr'],\n",
    "                results_df.sort_values('corr')['ate'] - 1.96 * results_df.sort_values('corr')['se'],\n",
    "                results_df.sort_values('corr')['ate'] + 1.96 * results_df.sort_values('corr')['se'],\n",
    "                alpha=0.2, color=TUFTE_PALETTE['treatment'])\n",
    "ax.axhline(TRUE_EFFECT, color='black', linestyle='--', linewidth=2, label='True effect')\n",
    "ax.set_xlabel('Corr(Z, T)')\n",
    "ax.set_ylabel('IV Estimate')\n",
    "ax.set_title('(b) Estimates Become Useless with Weak Instruments')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### 3.2 Variance Explosion with Weak Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize by F-statistic (more intuitive)\nfig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n\n# Filter to reasonable range (exclude extreme values)\nplot_df = results_df[(results_df['f_stat'] < 500) & (results_df['se'] < 100)].copy()\n\n# Panel 1: SE by F-statistic\nax = axes[0]\nax.scatter(plot_df['f_stat'], plot_df['se'], \n           c=TUFTE_PALETTE['treatment'], alpha=0.7, s=30)\nax.axvline(10, color='red', linestyle='--', linewidth=2, label='F = 10 threshold')\nax.set_xlabel('First-Stage F-statistic')\nax.set_ylabel('IV Standard Error')\nax.set_title('(a) SE by Instrument Strength')\nax.legend(frameon=False)\nax.set_xlim(0, 100)\n\n# Panel 2: Bias by F-statistic\nax = axes[1]\nax.scatter(plot_df['f_stat'], plot_df['bias'], \n           c=TUFTE_PALETTE['treatment'], alpha=0.7, s=30)\nax.axhline(0, color='black', linestyle='-', linewidth=1)\nax.axvline(10, color='red', linestyle='--', linewidth=2, label='F = 10 threshold')\nax.set_xlabel('First-Stage F-statistic')\nax.set_ylabel('Bias (Estimate - True)')\nax.set_title('(b) Bias by Instrument Strength')\nax.legend(frameon=False)\nax.set_xlim(0, 100)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics (filter out extreme values)\nstrong = results_df[results_df['f_stat'] > 10]\nweak = results_df[(results_df['f_stat'] <= 10) & (results_df['se'] < 100)]\n\nprint(\"\\n\u2605 Summary:\")\nprint(f\"  Strong instruments (F > 10): {len(strong)} cases\")\nif len(strong) > 0:\n    print(f\"    - Mean SE: {strong['se'].mean():.4f}\")\n    print(f\"    - Mean absolute bias: {strong['bias'].abs().mean():.4f}\")\nprint(f\"  Weak instruments (F \u2264 10): {len(weak)} cases (extreme values filtered)\")\nif len(weak) > 0:\n    print(f\"    - Mean SE: {weak['se'].mean():.4f}\")\n    print(f\"    - Mean absolute bias: {weak['bias'].abs().mean():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 3.3 2SLS vs LIML Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 2SLS vs LIML with weak instruments\n",
    "np.random.seed(123)\n",
    "n = 5000\n",
    "\n",
    "# Generate data with moderately weak instrument\n",
    "U = np.random.normal(0, 1, n)\n",
    "Z = np.random.binomial(1, 0.5, n)\n",
    "T = 0.5 + 0.15 * Z + 0.8 * U + np.random.normal(0, 1, n)  # Weak first stage (coef = 0.15)\n",
    "Y = 1.0 + TRUE_EFFECT * T + 1.2 * U + np.random.normal(0, 1, n)\n",
    "\n",
    "weak_data = pd.DataFrame({'Y': Y, 'T': T, 'Z': Z, 'U': U})\n",
    "\n",
    "# First stage F-statistic\n",
    "first_stage = smf.ols('T ~ Z', data=weak_data).fit()\n",
    "f_stat = first_stage.tvalues['Z'] ** 2\n",
    "\n",
    "print(f\"WEAK INSTRUMENT SCENARIO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True effect: {TRUE_EFFECT}\")\n",
    "print(f\"First-stage coefficient: {first_stage.params['Z']:.4f}\")\n",
    "print(f\"First-stage F-statistic: {f_stat:.2f}\")\n",
    "print(f\"Weak instrument? {f_stat < 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS (biased)\n",
    "ols_result = smf.ols('Y ~ T', data=weak_data).fit()\n",
    "\n",
    "# 2SLS\n",
    "iv2sls_result = IV2SLS.from_formula('Y ~ 1 + [T ~ Z]', weak_data).fit()\n",
    "\n",
    "# LIML\n",
    "ivliml_result = IVLIML.from_formula('Y ~ 1 + [T ~ Z]', weak_data).fit()\n",
    "\n",
    "# Oracle (with U)\n",
    "oracle_result = smf.ols('Y ~ T + U', data=weak_data).fit()\n",
    "\n",
    "print(\"\\nCOMPARISON: OLS vs 2SLS vs LIML\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Method':<15} {'Estimate':>12} {'SE':>12} {'Bias':>12} {'RMSE':>12}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'True':<15} {TRUE_EFFECT:>12.4f} {'-':>12} {0:>12.4f} {'-':>12}\")\n",
    "print(f\"{'OLS':<15} {ols_result.params['T']:>12.4f} {ols_result.bse['T']:>12.4f} \"\n",
    "      f\"{ols_result.params['T'] - TRUE_EFFECT:>12.4f} {'-':>12}\")\n",
    "print(f\"{'2SLS':<15} {iv2sls_result.params['T']:>12.4f} {iv2sls_result.std_errors['T']:>12.4f} \"\n",
    "      f\"{iv2sls_result.params['T'] - TRUE_EFFECT:>12.4f} {'-':>12}\")\n",
    "print(f\"{'LIML':<15} {ivliml_result.params['T']:>12.4f} {ivliml_result.std_errors['T']:>12.4f} \"\n",
    "      f\"{ivliml_result.params['T'] - TRUE_EFFECT:>12.4f} {'-':>12}\")\n",
    "print(f\"{'Oracle':<15} {oracle_result.params['T']:>12.4f} {oracle_result.bse['T']:>12.4f} \"\n",
    "      f\"{oracle_result.params['T'] - TRUE_EFFECT:>12.4f} {'-':>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo: Compare 2SLS vs LIML bias distribution\n",
    "n_sims = 500\n",
    "n_obs = 1000\n",
    "\n",
    "results_2sls = []\n",
    "results_liml = []\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(n_sims):\n",
    "    # Generate data\n",
    "    U = np.random.normal(0, 1, n_obs)\n",
    "    Z = np.random.binomial(1, 0.5, n_obs)\n",
    "    T = 0.5 + 0.15 * Z + 0.8 * U + np.random.normal(0, 1, n_obs)\n",
    "    Y = 1.0 + TRUE_EFFECT * T + 1.2 * U + np.random.normal(0, 1, n_obs)\n",
    "    \n",
    "    mc_data = pd.DataFrame({'Y': Y, 'T': T, 'Z': Z})\n",
    "    \n",
    "    try:\n",
    "        # 2SLS\n",
    "        iv2sls = IV2SLS.from_formula('Y ~ 1 + [T ~ Z]', mc_data).fit()\n",
    "        results_2sls.append(iv2sls.params['T'])\n",
    "        \n",
    "        # LIML\n",
    "        ivliml = IVLIML.from_formula('Y ~ 1 + [T ~ Z]', mc_data).fit()\n",
    "        results_liml.append(ivliml.params['T'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "results_2sls = np.array(results_2sls)\n",
    "results_liml = np.array(results_liml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Monte Carlo results\n",
    "fig, ax = create_tufte_figure(figsize=(10, 5))\n",
    "\n",
    "# Histogram of estimates\n",
    "bins = np.linspace(-5, 10, 50)\n",
    "ax.hist(results_2sls, bins=bins, alpha=0.5, label='2SLS', \n",
    "        color=TUFTE_PALETTE['control'], density=True)\n",
    "ax.hist(results_liml, bins=bins, alpha=0.5, label='LIML', \n",
    "        color=TUFTE_PALETTE['treatment'], density=True)\n",
    "ax.axvline(TRUE_EFFECT, color='black', linestyle='--', linewidth=2, label='True effect')\n",
    "ax.axvline(np.mean(results_2sls), color=TUFTE_PALETTE['control'], linestyle='-', linewidth=2)\n",
    "ax.axvline(np.median(results_liml), color=TUFTE_PALETTE['treatment'], linestyle='-', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Estimated Effect')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Monte Carlo: 2SLS vs LIML with Weak Instrument')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nMONTE CARLO RESULTS (500 simulations)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True effect: {TRUE_EFFECT}\")\n",
    "print(f\"\")\n",
    "print(f\"2SLS:\")\n",
    "print(f\"  Mean:   {np.mean(results_2sls):.4f} (bias: {np.mean(results_2sls) - TRUE_EFFECT:.4f})\")\n",
    "print(f\"  Median: {np.median(results_2sls):.4f}\")\n",
    "print(f\"  SD:     {np.std(results_2sls):.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"LIML:\")\n",
    "print(f\"  Mean:   {np.mean(results_liml):.4f} (bias: {np.mean(results_liml) - TRUE_EFFECT:.4f})\")\n",
    "print(f\"  Median: {np.median(results_liml):.4f}\")\n",
    "print(f\"  SD:     {np.std(results_liml):.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"\u2605 LIML is approximately median-unbiased ({np.median(results_liml):.2f} vs true {TRUE_EFFECT})\")\n",
    "print(f\"\u2605 But LIML has higher variance ({np.std(results_liml):.2f} vs {np.std(results_2sls):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "**Key Findings:**\n",
    "\n",
    "1. **2SLS is biased** toward OLS even with consistent instruments\n",
    "2. **LIML is approximately median-unbiased** \u2014 the median is close to the true value\n",
    "3. **LIML has higher variance** \u2014 the distribution is more spread out\n",
    "4. **Trade-off**: Less bias (LIML) vs. lower variance (2SLS)\n",
    "5. **Practical advice**: Compare both; if they agree, be confident; if they disagree, worry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "Weak instrument diagnostics in `causal_inference_mastery` at `src/causal_inference/iv/`:\n",
    "\n",
    "```python\n",
    "from causal_inference.iv.two_stage import TwoStageLeastSquares, LIML\n",
    "from causal_inference.iv.diagnostics import (\n",
    "    first_stage_f_test,\n",
    "    weak_instrument_test,\n",
    "    stock_yogo_critical_values\n",
    ")\n",
    "\n",
    "# Fit 2SLS\n",
    "model_2sls = TwoStageLeastSquares(\n",
    "    formula='log_wage ~ [years_of_schooling ~ q4] + year_of_birth + state_of_birth'\n",
    ")\n",
    "result_2sls = model_2sls.fit(data)\n",
    "\n",
    "# Weak instrument test\n",
    "f_test = first_stage_f_test(result_2sls)\n",
    "print(f\"First-stage F: {f_test.statistic:.2f}\")\n",
    "print(f\"Stock-Yogo 10% critical value: {stock_yogo_critical_values(n_instruments=1)['10%']}\")\n",
    "print(f\"Weak instrument? {f_test.statistic < 10}\")\n",
    "\n",
    "# Compare with LIML\n",
    "model_liml = LIML(\n",
    "    formula='log_wage ~ [years_of_schooling ~ q4] + year_of_birth + state_of_birth'\n",
    ")\n",
    "result_liml = model_liml.fit(data)\n",
    "\n",
    "print(f\"2SLS estimate: {result_2sls.params['years_of_schooling']:.4f}\")\n",
    "print(f\"LIML estimate: {result_liml.params['years_of_schooling']:.4f}\")\n",
    "print(f\"Difference: {abs(result_2sls.params['years_of_schooling'] - result_liml.params['years_of_schooling']):.4f}\")\n",
    "```\n",
    "\n",
    "The production code also includes:\n",
    "- Cragg-Donald weak instrument test for multiple instruments\n",
    "- Anderson-Rubin confidence intervals (robust to weak instruments)\n",
    "- Conditional likelihood ratio tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "---\n\n## Additional Content: Weak Instrument Robust Inference\n\n### Anderson-Rubin Confidence Sets\n\nWhen instruments are weak, standard 2SLS confidence intervals have wrong coverage. **Anderson-Rubin (AR) confidence sets** are valid regardless of instrument strength.\n\n**The AR test**:\n$$AR(\\theta_0) = \\frac{(Y - T\\theta_0)'P_Z(Y - T\\theta_0) / K}{(Y - T\\theta_0)'M_Z(Y - T\\theta_0) / (n - K)}$$\n\nwhere $P_Z = Z(Z'Z)^{-1}Z'$ is the projection onto instruments.\n\n**Key property**: Under $H_0: \\theta = \\theta_0$, $AR(\\theta_0) \\sim F(K, n-K)$ regardless of instrument strength.\n\n**AR confidence set**: Invert the test\u2014include all $\\theta_0$ values not rejected:\n$$\\text{CI}_{AR} = \\{\\theta_0 : AR(\\theta_0) < F_{1-\\alpha}(K, n-K)\\}$$\n\n**Practical implications**:\n- AR sets can be unbounded or even empty with very weak instruments\n- Unbounded CI = \"data is uninformative about $\\theta$\"\n- Use `ivreg::ivmodel()` in R or `linearmodels` in Python\n\n### The tF > 10 Rule and Its Limitations\n\n**Stock-Yogo (2005)** provides critical values, but the \"F > 10\" rule has limitations:\n\n1. **Derived under homoskedasticity**: With heteroskedastic errors, use robust F-statistics\n2. **Single endogenous variable**: Multiple endogenous variables require Cragg-Donald or Kleibergen-Paap statistics\n3. **Over-identification matters**: With many instruments, need higher F for same bias reduction\n4. **Asymptotic result**: In small samples, may need even higher F\n\n**Modern recommendations**:\n- **Robust F**: Use heteroskedasticity-robust first-stage F (Olea & Pflueger, 2013)\n- **Effective F**: Accounts for many instruments better than standard F\n- **Report AR**: Always report AR confidence sets alongside 2SLS when F < 20\n\n\u2605 Insight \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n- F > 10 is a **rule of thumb**, not a theorem\n- Anderson-Rubin CIs are **always valid** regardless of instrument strength\n- When 2SLS CI and AR CI disagree \u2192 trust AR\n- Unbounded AR CI means \"we don't know\"\u2014that's honest\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Interview Appendix\n\n### Practice Questions\n\n**Q1 (Google L5, Quant)**: *\"Your first-stage F-statistic is 8. What are the implications for your IV estimates, and what should you do?\"*\n\n<details>\n<summary>Solution</summary>\n\n**Key insight**: F = 8 is below the Stock-Yogo threshold of 10, indicating a **weak instrument problem**.\n\n**Problems with weak instruments:**\n\n1. **Bias toward OLS**: 2SLS bias \u2248 (OLS bias) \u00d7 (K/F) where K = number of instruments\n   - With F = 8, substantial bias remains toward the endogeneity-contaminated OLS estimate\n\n2. **Invalid inference**: Standard errors are wrong\n   - t-tests reject too often under the null\n   - Confidence intervals have incorrect coverage\n\n3. **Non-normal sampling distribution**:\n   - IV estimator isn't approximately normal in finite samples\n   - Standard asymptotic inference breaks down\n\n**What to do:**\n\n1. **Report LIML alongside 2SLS**: If estimates differ substantially, weak instruments are affecting results\n2. **Use Anderson-Rubin confidence sets**: Valid regardless of instrument strength\n3. **Consider the effective F (tF)**: More accurate for heteroskedastic errors\n4. **Look for stronger instruments**: But beware the many-weak-instruments problem\n5. **Be transparent**: Report the F-statistic and acknowledge the limitation\n\n</details>\n\n---\n\n**Q2 (Meta E5, DS)**: *\"Explain the bias formula for 2SLS with weak instruments. Why is 2SLS biased toward OLS?\"*\n\n<details>\n<summary>Solution</summary>\n\n**The key formula**:\n\n$$\\text{Bias}_{2SLS} \\approx \\frac{\\text{Bias}_{OLS}}{1 + F}$$\n\n**Or equivalently**, relative bias:\n\n$$\\frac{\\text{Bias}_{2SLS}}{\\text{Bias}_{OLS}} \\approx \\frac{K}{F}$$\n\nwhere K = number of instruments, F = first-stage F-statistic.\n\n**Why 2SLS is biased toward OLS**:\n\n1. **Finite sample problem**: 2SLS uses predicted values $\\hat{T} = \\hat{\\pi}'Z$\n2. **With weak instruments**: $\\hat{\\pi}$ has high variance \u2192 $\\hat{T}$ contains mostly noise\n3. **In the limit of zero relevance**: $\\hat{T} \\to \\bar{T}$ (constant) \u2192 2SLS coefficient \u2192 OLS coefficient\n4. **Intuition**: When instruments don't move T, 2SLS can't distinguish good from bad variation\n\n**The bias comes from correlation between first-stage errors and second-stage errors**:\n- When instruments are weak, first-stage residuals dominate $\\hat{T}$\n- These residuals are correlated with the structural error (that's the endogeneity!)\n- So 2SLS inherits OLS's endogeneity bias\n\n</details>\n\n---\n\n**Q3 (Jane Street, Quant)**: *\"What are Anderson-Rubin confidence sets and why are they 'weak-instrument robust'?\"*\n\n<details>\n<summary>Solution</summary>\n\n**Anderson-Rubin (AR) Test**:\n\nTests $H_0: \\kappa = \\kappa_0$ using:\n\n$$AR(\\kappa_0) = \\frac{(Y - T\\kappa_0)'P_Z(Y - T\\kappa_0) / K}{(Y - T\\kappa_0)'M_Z(Y - T\\kappa_0) / (n - K)} \\sim F(K, n-K)$$\n\nwhere $P_Z = Z(Z'Z)^{-1}Z'$ projects onto instruments.\n\n**The AR Confidence Set**: Invert the test\u2014include all $\\kappa_0$ not rejected:\n\n$$\\text{CI}_{AR} = \\{\\kappa_0 : AR(\\kappa_0) < F_{1-\\alpha}(K, n-K)\\}$$\n\n**Why weak-instrument robust**:\n\n1. **No reliance on first-stage strength**: The F-distribution is exact under $H_0$ regardless of $\\pi$\n2. **The key insight**: Under the null $\\kappa = \\kappa_0$, $(Y - T\\kappa_0)$ equals the structural error $v$\n3. **Exclusion restriction** implies $\\text{Cov}(Z, v) = 0$ \u2192 numerator is small\n4. **This works regardless of how weak the first stage is**\n\n**Trade-off**:\n- AR sets can be unbounded (one-sided or both) with very weak instruments\n- Unbounded CI honestly says: \"data is uninformative about $\\kappa$\"\n- More conservative than standard IV CIs (correct coverage vs. over-rejection)\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Stock, J. H., Wright, J. H., & Yogo, M. (2002). \"A Survey of Weak Instruments and Weak Identification in Generalized Method of Moments.\" *Journal of Business & Economic Statistics*.\n",
    "\n",
    "[^2]: Stock, J. H. & Yogo, M. (2005). \"Testing for Weak Instruments in Linear IV Regression.\" In *Identification and Inference for Econometric Models*. [research_kb: `a2e2d729-3730-4120-b155-28dfe5a7d0a4`]\n",
    "\n",
    "[^3]: Angrist, J. D. & Pischke, J.-S. (2009). *Mostly Harmless Econometrics*, Section 4.6. Princeton University Press. [research_kb: `93737674-d68d-4952-957f-00e26f085088`]\n",
    "\n",
    "[^4]: Bound, J., Jaeger, D. A., & Baker, R. M. (1995). \"Problems with Instrumental Variables Estimation When the Correlation Between the Instruments and the Endogenous Explanatory Variable is Weak.\" *JASA*.\n",
    "\n",
    "[^5]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 8: \"Instrumental Variables.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Precision Improvement:**\n",
    "- You said: \"Build the weak instruments notebook\"\n",
    "- Concise: \"Build 04_weak_instruments\"\n",
    "- Precise: `/facure_augment 08.4 --weak-iv --stock-yogo --liml`\n",
    "- Pattern: [build] [target] [content-flags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}