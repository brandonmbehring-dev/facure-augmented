{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Prediction Might Help: Monotonic Elasticity Cases\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Appendix A4 | Notebook 3 of 3**\n",
    "\n",
    "Despite the failures shown in previous notebooks, prediction-based targeting\n",
    "CAN work under specific conditions. This notebook explores when and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from facure_augment.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### The Key Question\n",
    "\n",
    "We've established that prediction models generally fail for causal optimization.\n",
    "But data scientists often counter: \"surely ML must help *sometimes*?\"\n",
    "\n",
    "**Answer**: Yes, under specific conditions.\n",
    "\n",
    "### Monotonic Elasticity\n",
    "\n",
    "The critical condition is **monotonic elasticity** - when the relationship between\n",
    "$Y$ and $\\frac{\\partial Y}{\\partial T}$ doesn't reverse direction.\n",
    "\n",
    "**Two Cases**:\n",
    "\n",
    "| Case | Y vs Elasticity | Prediction Helps? |\n",
    "|------|-----------------|-------------------|\n",
    "| Coupon/Profitability | Reverses (+ then -) | **NO** |\n",
    "| Wait Time/Satisfaction | Always negative | **YES** |\n",
    "\n",
    "### Visual Intuition\n",
    "\n",
    "When prediction works:\n",
    "- Y-axis slices keep units with similar elasticities together\n",
    "- Higher Y → consistently higher (or lower) elasticity\n",
    "- No \"mixing\" of positive and negative elasticity regions\n",
    "\n",
    "When prediction fails:\n",
    "- Y-axis slices mix units from different elasticity regions\n",
    "- Same Y can mean very different elasticities\n",
    "- Optimization signal gets scrambled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Correlation Between Y and Elasticity\n",
    "\n",
    "Let the response curve be $Y = f(T, X)$.\n",
    "\n",
    "The elasticity is:\n",
    "$$\\tau(T, X) = \\frac{\\partial f(T, X)}{\\partial T}$$\n",
    "\n",
    "**Prediction helps when**:\n",
    "$$\\text{sign}\\left(\\text{Corr}(Y, \\tau)\\right) \\text{ is constant across } T$$\n",
    "\n",
    "### 2.2 Quadratic Example (Fails)\n",
    "\n",
    "$$Y = \\alpha + \\beta T - \\gamma T^2$$\n",
    "\n",
    "Elasticity: $\\tau(T) = \\beta - 2\\gamma T$\n",
    "\n",
    "- For $T < T^* = \\frac{\\beta}{2\\gamma}$: As Y ↑, τ ↓ (negative correlation)\n",
    "- For $T > T^*$: As Y ↓, τ ↓ (positive correlation)\n",
    "\n",
    "**Correlation reverses** → prediction fails.\n",
    "\n",
    "### 2.3 Saturating Example (Works)\n",
    "\n",
    "$$Y = \\alpha - \\beta \\cdot \\log(1 + T)$$\n",
    "\n",
    "Elasticity: $\\tau(T) = \\frac{-\\beta}{1 + T}$\n",
    "\n",
    "- As T ↑: Y ↓ and τ ↑ (toward 0)\n",
    "- Always: As Y ↑, τ ↓ (negative correlation throughout)\n",
    "\n",
    "**Correlation consistent** → prediction can help.\n",
    "\n",
    "### 2.4 Random Treatment Case\n",
    "\n",
    "When $T \\perp X$ (random assignment):\n",
    "- No confounding: $X$ has no information about $T$\n",
    "- Model $M(X)$ cannot learn $T$ through features\n",
    "- Conditioning on $M(X)$ doesn't block $T \\to Y$ path\n",
    "\n",
    "**Result**: Prediction actually **helps** by reducing variance from $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Case 1: Saturating Response (Prediction Helps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saturating_response(n=5000, seed=42):\n",
    "    \"\"\"Generate data with saturating (log) response to treatment.\n",
    "    \n",
    "    DGP: Y = 100 - 20*log(1+T) + 5*X + noise\n",
    "    \n",
    "    This creates monotonic elasticity:\n",
    "    - Elasticity = -20/(1+T) is always negative\n",
    "    - As Y increases, elasticity magnitude increases (more negative)\n",
    "    - Correlation between Y and elasticity is consistent\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Feature (confounder)\n",
    "    X = np.random.normal(5, 2, n)\n",
    "    \n",
    "    # Treatment: assigned based on X (confounded)\n",
    "    T = np.clip(X + np.random.normal(0, 1, n), 0, 15)\n",
    "    \n",
    "    # Saturating response\n",
    "    Y = 100 - 20 * np.log(1 + T) + 5 * X + np.random.normal(0, 5, n)\n",
    "    \n",
    "    return pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n",
    "\n",
    "sat_data = generate_saturating_response()\n",
    "print(f\"Data shape: {sat_data.shape}\")\n",
    "sat_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the saturating response\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Response curve\n",
    "ax = axes[0]\n",
    "T_grid = np.linspace(0, 15, 100)\n",
    "Y_curve = 100 - 20 * np.log(1 + T_grid) + 5 * 5  # At X=5\n",
    "ax.plot(T_grid, Y_curve, 'b-', linewidth=2, label='Response curve')\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Outcome Y')\n",
    "ax.set_title('Saturating Response: Y = 100 - 20·log(1+T)')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Elasticity curve\n",
    "ax = axes[1]\n",
    "elasticity = -20 / (1 + T_grid)\n",
    "ax.plot(T_grid, elasticity, 'r-', linewidth=2)\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Elasticity ∂Y/∂T')\n",
    "ax.set_title('Elasticity: Always Negative (Monotonic)')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Y vs Elasticity correlation\n",
    "ax = axes[2]\n",
    "ax.plot(Y_curve, elasticity, 'g-', linewidth=2)\n",
    "ax.set_xlabel('Outcome Y')\n",
    "ax.set_ylabel('Elasticity ∂Y/∂T')\n",
    "ax.set_title('Y vs Elasticity: Consistent Negative Correlation')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation\n",
    "corr = np.corrcoef(Y_curve, elasticity)[0, 1]\n",
    "print(f\"Correlation between Y and elasticity: {corr:.4f}\")\n",
    "print(\"→ Always negative correlation: prediction CAN help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Quadratic Response (Prediction Fails)\n",
    "\n",
    "Compare with the quadratic case from notebook 02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic response: correlation reverses\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Response curve\n",
    "ax = axes[0]\n",
    "T_grid = np.linspace(0, 15, 100)\n",
    "Y_quad = 100 + 20 * T_grid - 1.5 * T_grid**2  # Peak at T ≈ 6.67\n",
    "ax.plot(T_grid, Y_quad, 'b-', linewidth=2, label='Response curve')\n",
    "ax.axvline(20/(2*1.5), color='green', linestyle='--', alpha=0.7, label=f'Optimal T*={20/(2*1.5):.1f}')\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Outcome Y')\n",
    "ax.set_title('Quadratic Response: Y = 100 + 20T - 1.5T²')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Elasticity curve\n",
    "ax = axes[1]\n",
    "elasticity_quad = 20 - 3 * T_grid\n",
    "ax.plot(T_grid, elasticity_quad, 'r-', linewidth=2)\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(20/(2*1.5), color='green', linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Elasticity ∂Y/∂T')\n",
    "ax.set_title('Elasticity: Crosses Zero (Non-Monotonic)')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Y vs Elasticity - show the reversal\n",
    "ax = axes[2]\n",
    "# Color by region: before vs after peak\n",
    "peak_idx = np.argmax(Y_quad)\n",
    "ax.plot(Y_quad[:peak_idx], elasticity_quad[:peak_idx], 'b-', linewidth=2, label='Before peak')\n",
    "ax.plot(Y_quad[peak_idx:], elasticity_quad[peak_idx:], 'r-', linewidth=2, label='After peak')\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Outcome Y')\n",
    "ax.set_ylabel('Elasticity ∂Y/∂T')\n",
    "ax.set_title('Y vs Elasticity: Correlation REVERSES')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Before peak: As Y↑, elasticity↓ (negative correlation)\")\n",
    "print(\"After peak: As Y↓, elasticity↓ (positive correlation)\")\n",
    "print(\"→ Correlation reverses: prediction FAILS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction-Based Targeting on Saturating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train prediction model\n",
    "model = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "model.fit(sat_data[['X']], sat_data['Y'])\n",
    "\n",
    "# Create prediction bands\n",
    "sat_data['Y_pred'] = model.predict(sat_data[['X']])\n",
    "sat_data['band'] = pd.qcut(sat_data['Y_pred'], q=5, labels=False) + 1\n",
    "\n",
    "print(\"Band distribution:\")\n",
    "print(sat_data.groupby('band').agg({'Y': 'mean', 'T': 'mean'}).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute elasticity by band - for saturating, should show consistent pattern\n",
    "def compute_band_elasticity(df, band_col='band', T_col='T', Y_col='Y'):\n",
    "    \"\"\"Compute elasticity within each band.\"\"\"\n",
    "    results = []\n",
    "    for band in sorted(df[band_col].unique()):\n",
    "        band_data = df[df[band_col] == band]\n",
    "        # Compute via finite differences\n",
    "        if len(band_data) > 10:\n",
    "            # Sort by T and compute gradient\n",
    "            sorted_data = band_data.sort_values(T_col)\n",
    "            dY = np.gradient(sorted_data[Y_col].values)\n",
    "            dT = np.gradient(sorted_data[T_col].values)\n",
    "            # Avoid division by zero\n",
    "            valid = np.abs(dT) > 0.01\n",
    "            if valid.sum() > 5:\n",
    "                elasticity = np.median(dY[valid] / dT[valid])\n",
    "                results.append({\n",
    "                    'band': band,\n",
    "                    'mean_Y': band_data[Y_col].mean(),\n",
    "                    'mean_T': band_data[T_col].mean(),\n",
    "                    'elasticity': elasticity,\n",
    "                    'n': len(band_data)\n",
    "                })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "sat_elasticity = compute_band_elasticity(sat_data)\n",
    "print(\"Elasticity by prediction band (saturating response):\")\n",
    "print(sat_elasticity.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: saturating vs quadratic\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Saturating: monotonic elasticity across bands\n",
    "ax = axes[0]\n",
    "ax.bar(sat_elasticity['band'], sat_elasticity['elasticity'], color=COLORS['blue'], alpha=0.7)\n",
    "ax.axhline(0, color='black', linestyle='-')\n",
    "ax.set_xlabel('Prediction Band')\n",
    "ax.set_ylabel('Within-Band Elasticity')\n",
    "ax.set_title('Saturating Response: Elasticity Preserved\\n(Prediction CAN help)')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Add text annotation\n",
    "ax.text(0.5, 0.9, 'All bands show\\nnegative elasticity', transform=ax.transAxes,\n",
    "        fontsize=10, verticalalignment='top', \n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "# For comparison: show what quadratic would look like\n",
    "ax = axes[1]\n",
    "# Simulated: some bands positive, some negative\n",
    "quad_bands = pd.DataFrame({\n",
    "    'band': [1, 2, 3, 4, 5],\n",
    "    'elasticity': [5.2, 2.1, -0.3, -0.2, -1.1]  # Example mixed signs\n",
    "})\n",
    "colors = [COLORS['green'] if e > 0 else COLORS['red'] for e in quad_bands['elasticity']]\n",
    "ax.bar(quad_bands['band'], quad_bands['elasticity'], color=colors, alpha=0.7)\n",
    "ax.axhline(0, color='black', linestyle='-')\n",
    "ax.set_xlabel('Prediction Band')\n",
    "ax.set_ylabel('Within-Band Elasticity')\n",
    "ax.set_title('Quadratic Response: Elasticity Scrambled\\n(Prediction FAILS)')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "ax.text(0.5, 0.9, 'Mixed +/- elasticity\\nwithin similar Y bands', transform=ax.transAxes,\n",
    "        fontsize=10, verticalalignment='top', \n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "Prediction-based targeting works when:\n",
    "\n",
    "1. **Monotonic elasticity**: Y and ∂Y/∂T move in consistent direction\n",
    "2. **No reversal**: Correlation doesn't flip across treatment range\n",
    "3. **Y-slices preserve structure**: Similar Y → similar elasticity\n",
    "\n",
    "Examples where prediction helps:\n",
    "- Saturating response (diminishing returns)\n",
    "- Exponential decay\n",
    "- Log-linear relationships\n",
    "\n",
    "Examples where prediction fails:\n",
    "- Quadratic (inverted-U) response\n",
    "- Any curve with interior optimum\n",
    "- Price-revenue relationships with elastic/inelastic regions\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### When Random Treatment Makes Everything Better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_treatment(n=5000, seed=42):\n",
    "    \"\"\"Generate data with RANDOM treatment assignment.\n",
    "    \n",
    "    When T ⊥ X, prediction models help because:\n",
    "    1. Model cannot learn T through X\n",
    "    2. Conditioning on M(X) doesn't block T → Y\n",
    "    3. Variance reduction from controlling X\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Feature (NOT a confounder since T is random)\n",
    "    X = np.random.normal(5, 2, n)\n",
    "    \n",
    "    # Treatment: RANDOM (independent of X)\n",
    "    T = np.random.uniform(0, 15, n)\n",
    "    \n",
    "    # Quadratic response - usually fails, but with random T...\n",
    "    Y = 100 + 20 * T - 1.5 * T**2 + 5 * X + np.random.normal(0, 10, n)\n",
    "    \n",
    "    return pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n",
    "\n",
    "random_data = generate_random_treatment()\n",
    "print(\"Treatment-feature correlation:\", np.corrcoef(random_data['T'], random_data['X'])[0,1].round(4))\n",
    "print(\"→ Near zero: T is independent of X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With random treatment, prediction bands PRESERVE elasticity structure\n",
    "model_rand = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "model_rand.fit(random_data[['X']], random_data['Y'])\n",
    "\n",
    "random_data['Y_pred'] = model_rand.predict(random_data[['X']])\n",
    "random_data['band'] = pd.qcut(random_data['Y_pred'], q=5, labels=False) + 1\n",
    "\n",
    "# Check elasticity by band\n",
    "rand_elasticity = compute_band_elasticity(random_data)\n",
    "print(\"\\nElasticity by band (RANDOM treatment):\")\n",
    "print(rand_elasticity.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: Confounded vs Random treatment\n",
    "def generate_confounded_quadratic(n=5000, seed=42):\n",
    "    \"\"\"Same DGP but with confounded treatment.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.normal(5, 2, n)\n",
    "    T = np.clip(X + np.random.normal(0, 1, n), 0, 15)  # Confounded!\n",
    "    Y = 100 + 20 * T - 1.5 * T**2 + 5 * X + np.random.normal(0, 10, n)\n",
    "    return pd.DataFrame({'X': X, 'T': T, 'Y': Y})\n",
    "\n",
    "conf_data = generate_confounded_quadratic()\n",
    "\n",
    "# Fit model and create bands\n",
    "model_conf = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "model_conf.fit(conf_data[['X']], conf_data['Y'])\n",
    "conf_data['Y_pred'] = model_conf.predict(conf_data[['X']])\n",
    "conf_data['band'] = pd.qcut(conf_data['Y_pred'], q=5, labels=False) + 1\n",
    "\n",
    "conf_elasticity = compute_band_elasticity(conf_data)\n",
    "\n",
    "print(\"Confounded treatment (T correlated with X):\")\n",
    "print(f\"  Corr(T, X) = {np.corrcoef(conf_data['T'], conf_data['X'])[0,1]:.3f}\")\n",
    "print(\"\\nRandom treatment (T independent of X):\")\n",
    "print(f\"  Corr(T, X) = {np.corrcoef(random_data['T'], random_data['X'])[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Confounded\n",
    "ax = axes[0]\n",
    "for band in sorted(conf_data['band'].unique()):\n",
    "    band_data = conf_data[conf_data['band'] == band]\n",
    "    ax.scatter(band_data['T'], band_data['Y'], alpha=0.3, s=10, label=f'Band {band}')\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Outcome Y')\n",
    "ax.set_title('Confounded Treatment\\n(Prediction fails for quadratic)')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Random\n",
    "ax = axes[1]\n",
    "for band in sorted(random_data['band'].unique()):\n",
    "    band_data = random_data[random_data['band'] == band]\n",
    "    ax.scatter(band_data['T'], band_data['Y'], alpha=0.3, s=10, label=f'Band {band}')\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Outcome Y')\n",
    "ax.set_title('Random Treatment\\n(Prediction helps even for quadratic)')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation:\")\n",
    "print(\"With RANDOM treatment, bands span the full treatment range,\")\n",
    "print(\"allowing us to see the causal relationship within each band.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = pd.DataFrame({\n",
    "    'Question': [\n",
    "        '1. Is treatment randomized?',\n",
    "        '2. Is response monotonic (no interior optimum)?',\n",
    "        '3. Does Y-elasticity correlation stay consistent?',\n",
    "        'ALL YES → Prediction may help',\n",
    "        'ANY NO → Use causal methods instead'\n",
    "    ],\n",
    "    'If YES': [\n",
    "        '→ Prediction helps (variance reduction)',\n",
    "        '→ Prediction might help',\n",
    "        '→ Prediction might help',\n",
    "        '✓',\n",
    "        ''\n",
    "    ],\n",
    "    'If NO': [\n",
    "        '→ Check Q2-3 carefully',\n",
    "        '→ Prediction FAILS (quadratic, inverted-U)',\n",
    "        '→ Prediction FAILS (correlation reverses)',\n",
    "        '',\n",
    "        '✗'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Decision Framework: When Does Prediction Help?\")\n",
    "print(\"=\"*60)\n",
    "print(decision_tree.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Key Takeaway ────────────────────────────────────────────────\n",
    "Prediction-based targeting is NOT universally bad.\n",
    "\n",
    "It CAN work when:\n",
    "1. Treatment is random (RCT context)\n",
    "2. Response has monotonic elasticity\n",
    "3. Y-elasticity correlation is consistent\n",
    "\n",
    "It FAILS when:\n",
    "1. Treatment is confounded AND\n",
    "2. Response has interior optimum (quadratic)\n",
    "3. Y-elasticity correlation reverses\n",
    "\n",
    "**Default assumption**: Use causal methods unless you can\n",
    "verify the special conditions where prediction helps.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: When can prediction-based targeting actually work for causal optimization?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Three conditions** where prediction helps:\n",
    "\n",
    "1. **Random treatment assignment**:\n",
    "   - T ⊥ X eliminates confounding\n",
    "   - Model cannot learn T through features\n",
    "   - Conditioning on M(X) provides variance reduction without blocking causal path\n",
    "\n",
    "2. **Monotonic elasticity**:\n",
    "   - Relationship between Y and ∂Y/∂T is consistent\n",
    "   - Higher Y always means higher (or lower) elasticity\n",
    "   - Examples: Saturating response, exponential decay\n",
    "\n",
    "3. **No interior optimum**:\n",
    "   - Response curve doesn't have a peak within treatment range\n",
    "   - Derivative doesn't cross zero\n",
    "   - Quadratic/inverted-U responses violate this\n",
    "\n",
    "**Key insight**: The fundamental issue is whether Y-axis slices keep units with similar elasticities together.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Explain why random treatment assignment fixes the prediction problem.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**DAG perspective**:\n",
    "\n",
    "With confounding: X → T → Y, X → Y\n",
    "- Model M(X) can learn T indirectly through X\n",
    "- Conditioning on M(X) blocks some of T → Y path\n",
    "\n",
    "With random T: X → Y, T → Y (no X → T)\n",
    "- Model M(X) cannot learn anything about T\n",
    "- Conditioning on M(X) only removes X → Y variation\n",
    "- Full T → Y effect remains visible\n",
    "\n",
    "**Variance reduction bonus**:\n",
    "- Residual variance after controlling for X is smaller\n",
    "- Treatment effect estimation becomes more precise\n",
    "- This is why RCTs with covariate adjustment are powerful\n",
    "\n",
    "**Practical implication**: In A/B tests (randomized), prediction-based segmentation for heterogeneity analysis is valid.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: Give an example where high predictive accuracy actively harms causal optimization.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Pricing example**:\n",
    "\n",
    "Goal: Find optimal price to maximize revenue.\n",
    "\n",
    "True DGP: Revenue = α + βP - γP² (quadratic in price)\n",
    "- Optimal price P* = β/(2γ)\n",
    "- Elasticity reverses at P*\n",
    "\n",
    "**High R² prediction model**:\n",
    "- Groups customers by predicted revenue\n",
    "- Within each band, revenue has low variance\n",
    "- Can't see how revenue changes with price\n",
    "\n",
    "**The paradox**:\n",
    "- Better prediction → flatter within-band curves\n",
    "- Flatter curves → less visible elasticity\n",
    "- Less visible elasticity → worse price optimization\n",
    "\n",
    "**Conclusion**: A prediction model with R² = 0.95 might perform WORSE for optimization than a simple average-based policy.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: How would you diagnose whether prediction-based targeting is appropriate for a given business problem?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Diagnostic checklist**:\n",
    "\n",
    "1. **Check treatment assignment mechanism**:\n",
    "   - Is it randomized? → Prediction likely helps\n",
    "   - Is it based on features? → Risk of confounding\n",
    "\n",
    "2. **Plot response curve**:\n",
    "   - Is there an interior maximum/minimum?\n",
    "   - Does the curve cross horizontal?\n",
    "   - Quadratic shape → prediction fails\n",
    "\n",
    "3. **Examine elasticity pattern**:\n",
    "   - Compute ∂Y/∂T at different treatment levels\n",
    "   - Does it change sign?\n",
    "   - Sign change → correlation reverses → prediction fails\n",
    "\n",
    "4. **Test within prediction bands**:\n",
    "   - Do bands show meaningful variation in Y vs T?\n",
    "   - Are elasticities consistent across bands?\n",
    "   - Flat within-band curves → prediction is flattening\n",
    "\n",
    "**Safe default**: Use causal methods (DML, CATE estimators) unless you have strong evidence that conditions for prediction success are met.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: A colleague argues: \"Our model has high R², so it must be good for targeting.\" How do you respond?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**The counterintuitive truth**:\n",
    "\n",
    "High R² can be *harmful* for causal optimization because:\n",
    "\n",
    "1. **Good prediction = low residual variance**:\n",
    "   - R² measures: 1 - Var(Y - Ŷ)/Var(Y)\n",
    "   - High R² means Y is well-explained by X\n",
    "\n",
    "2. **Low residual variance = flat within-band curves**:\n",
    "   - Prediction bands group units with similar Y\n",
    "   - Little variation left for T to explain\n",
    "\n",
    "3. **Flat curves = invisible elasticity**:\n",
    "   - Can't estimate ∂Y/∂T if Y doesn't vary\n",
    "   - Optimization becomes impossible\n",
    "\n",
    "**The key distinction**:\n",
    "- Prediction task: Estimate E[Y|X]\n",
    "- Optimization task: Estimate ∂Y/∂T\n",
    "\n",
    "These are **fundamentally different objectives**. A model that excels at one may fail catastrophically at the other.\n",
    "\n",
    "**Response to colleague**: \"High R² measures prediction quality, not optimization quality. These are orthogonal goals. In fact, a high R² model might actively hurt our ability to find the optimal treatment by flattening the response curves we need to optimize.\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Athey, S., & Imbens, G. (2016). Recursive Partitioning for Heterogeneous Causal Effects.\n",
    "      *Proceedings of the National Academy of Sciences*, 113(27), 7353-7360.\n",
    "\n",
    "[^2]: Chernozhukov, V., et al. (2018). Generic Machine Learning Inference on Heterogeneous\n",
    "      Treatment Effects in Randomized Experiments. *NBER Working Paper*.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, Appendix:\n",
    "      When Prediction Fails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
