{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.2 Synthetic Control Optimization\n",
    "\n",
    "**Chapter**: 15 - Synthetic Control  \n",
    "**Section**: 2 - Finding Optimal Weights  \n",
    "**Facure Source**: 15-Synthetic-Control.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-15\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [Why Not Just Use OLS?](#11-why-not-just-use-ols)\n",
    "   - 1.2 [Constrained Optimization](#12-constrained-optimization)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [The Optimization Problem](#21-the-optimization-problem)\n",
    "   - 2.2 [Convex Combination Constraints](#22-convex-combination-constraints)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [OLS Weights (Overfitting)](#31-ols-weights-overfitting)\n",
    "   - 3.2 [Constrained Optimization](#32-constrained-optimization)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, smf,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import fmin_slsqp\n",
    "from functools import partial\n",
    "\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: The constrained optimization is what makes synthetic control work. Understanding why we need constraints—and what happens without them—is crucial.\n",
    "\n",
    "### 1.1 Why Not Just Use OLS?\n",
    "\n",
    "Synthetic control can be seen as a regression where:\n",
    "- **Y**: The treated unit's outcomes (California's cigarette sales)\n",
    "- **X**: The control units' outcomes (other states' sales)\n",
    "- **β**: Weights for each control unit\n",
    "\n",
    "**Problem with OLS**:\n",
    "\n",
    "With 38 states and only 18 pre-treatment years, we have **more parameters than observations**.\n",
    "\n",
    "- OLS overfits: Perfect fit pre-treatment, garbage post-treatment\n",
    "- Weights can be negative (\"subtract Nevada\"?)\n",
    "- Weights can be huge (\"multiply by 5\")\n",
    "\n",
    "### 1.2 Constrained Optimization\n",
    "\n",
    "**Solution**: Constrain weights to be a **convex combination**:\n",
    "\n",
    "1. $w_j \\geq 0$ for all $j$ (non-negative)\n",
    "2. $\\sum_j w_j = 1$ (sum to one)\n",
    "\n",
    "This ensures:\n",
    "- Interpolation, not extrapolation\n",
    "- Interpretable weights (\"30% Nevada, 25% Utah...\")\n",
    "- Sparse solutions (most weights are zero)\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- Convex combination = interpolation within the \"hull\" of controls\n",
    "- OLS extrapolates beyond observed data → overfitting\n",
    "- Constraints regularize the solution, like Lasso\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 The Optimization Problem\n",
    "\n",
    "**Objective**: Minimize pre-treatment prediction error.\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{W}} \\left\\| \\mathbf{X}_1 - \\mathbf{X}_0 \\mathbf{W} \\right\\| = \\left( \\sum_{h=1}^{K} v_h \\left( X_{h1} - \\sum_{j=2}^{J+1} w_j X_{hj} \\right)^2 \\right)^{1/2}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X}_1$: Treated unit's pre-treatment characteristics (including lagged outcomes)\n",
    "- $\\mathbf{X}_0$: Control units' pre-treatment characteristics\n",
    "- $v_h$: Importance weight for variable $h$ (often equal)\n",
    "\n",
    "**Subject to constraints**:\n",
    "\n",
    "$$\n",
    "w_j \\geq 0 \\quad \\forall j, \\qquad \\sum_{j=2}^{J+1} w_j = 1\n",
    "$$\n",
    "\n",
    "### 2.2 Convex Combination Constraints\n",
    "\n",
    "**Why these constraints?**\n",
    "\n",
    "| Constraint | Purpose | Effect |\n",
    "|------------|---------|--------|\n",
    "| $w_j \\geq 0$ | No \"subtraction\" | Interpretable, stable |\n",
    "| $\\sum w_j = 1$ | Convex combination | Interpolation only |\n",
    "\n",
    "**Geometric interpretation**:\n",
    "\n",
    "The constraints force the synthetic control to lie inside the **convex hull** of control units.\n",
    "\n",
    "- If the treated is inside the hull → can match perfectly\n",
    "- If the treated is outside the hull → some mismatch is unavoidable\n",
    "- Being outside = treated unit is more extreme than any combination of controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 OLS Weights (Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "cigar = load_facure_data('smoking.csv')\n",
    "cigar = cigar.drop(columns=['lnincome', 'beer', 'age15to24'], errors='ignore')\n",
    "\n",
    "# Prepare data for synthetic control\n",
    "features = ['cigsale', 'retprice']\n",
    "\n",
    "# Pivot: rows = (feature, year), columns = states\n",
    "inverted = (cigar.query('~after_treatment')  # Pre-treatment only\n",
    "            .pivot(index='state', columns='year')[features]\n",
    "            .T)\n",
    "\n",
    "# California is state 3\n",
    "y = inverted[3].values  # Treated\n",
    "X = inverted.drop(columns=3).values  # Donor pool\n",
    "\n",
    "print(f\"Pre-treatment data shape:\")\n",
    "print(f\"  y (California): {y.shape}\")\n",
    "print(f\"  X (38 control states): {X.shape}\")\n",
    "print(f\"\\n⚠️ More control units ({X.shape[1]}) than time points ({X.shape[0]}) → OLS will overfit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS (no constraints)\n",
    "weights_ols = LinearRegression(fit_intercept=False).fit(X, y).coef_\n",
    "\n",
    "print(\"OLS WEIGHTS (UNCONSTRAINED):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Min weight: {weights_ols.min():.3f}\")\n",
    "print(f\"Max weight: {weights_ols.max():.3f}\")\n",
    "print(f\"Sum of weights: {weights_ols.sum():.3f}\")\n",
    "print(f\"Number negative: {(weights_ols < 0).sum()}\")\n",
    "print(f\"\\n⚠️ Weights are all over the place!\")\n",
    "print(f\"   Negative weights = extrapolation\")\n",
    "print(f\"   Sum ≠ 1 = scaling up/down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build synthetic California with OLS weights\n",
    "calif_synth_ols = (cigar.query('~california')\n",
    "                   .pivot(index='year', columns='state')['cigsale']\n",
    "                   .values.dot(weights_ols))\n",
    "\n",
    "calif_actual = cigar.query('california').sort_values('year')['cigsale'].values\n",
    "years = cigar.query('california').sort_values('year')['year'].values\n",
    "\n",
    "# Plot\n",
    "fig, ax = create_tufte_figure(figsize=(10, 6))\n",
    "\n",
    "ax.plot(years, calif_actual, 'o-', color=TUFTE_PALETTE['treatment'], \n",
    "        linewidth=2, markersize=6, label='California (Actual)')\n",
    "ax.plot(years, calif_synth_ols, 's-', color=TUFTE_PALETTE['control'], \n",
    "        linewidth=2, markersize=6, label='Synthetic (OLS)')\n",
    "ax.axvline(1988, color=TUFTE_PALETTE['effect'], linestyle=':', linewidth=2, label='Proposition 99')\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Cigarette Sales (Packs per Capita)')\n",
    "ax.set_title('OLS Synthetic Control: Overfitting Problem')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n⚠️ OLS fits pre-treatment perfectly but post-treatment is erratic!\")\n",
    "print(\"   This is classic overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Constrained Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def loss_w(W, X, y):\n",
    "    \"\"\"Root mean squared error between synthetic and treated.\"\"\"\n",
    "    return np.sqrt(np.mean((y - X.dot(W))**2))\n",
    "\n",
    "# Constrained optimization\n",
    "def get_weights(X, y):\n",
    "    \"\"\"Find weights minimizing pre-treatment RMSE with convex constraints.\"\"\"\n",
    "    n_units = X.shape[1]\n",
    "    w_start = [1/n_units] * n_units  # Start with equal weights\n",
    "    \n",
    "    weights = fmin_slsqp(\n",
    "        partial(loss_w, X=X, y=y),\n",
    "        np.array(w_start),\n",
    "        f_eqcons=lambda x: np.sum(x) - 1,  # Sum to 1\n",
    "        bounds=[(0.0, 1.0)] * n_units,      # Non-negative\n",
    "        disp=False\n",
    "    )\n",
    "    return weights\n",
    "\n",
    "# Get constrained weights\n",
    "weights_constrained = get_weights(X, y)\n",
    "\n",
    "print(\"CONSTRAINED WEIGHTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sum of weights: {weights_constrained.sum():.6f}\")\n",
    "print(f\"Min weight: {weights_constrained.min():.6f}\")\n",
    "print(f\"Max weight: {weights_constrained.max():.6f}\")\n",
    "print(f\"Non-zero weights: {(weights_constrained > 0.01).sum()}\")\n",
    "print(f\"\\n✓ Weights are sparse and interpretable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which states contribute\n",
    "state_ids = cigar.query('~california')['state'].unique()\n",
    "weight_df = pd.DataFrame({'state': state_ids, 'weight': weights_constrained})\n",
    "weight_df = weight_df[weight_df['weight'] > 0.01].sort_values('weight', ascending=False)\n",
    "\n",
    "print(\"\\nSTATES CONTRIBUTING TO SYNTHETIC CALIFORNIA:\")\n",
    "print(\"-\" * 40)\n",
    "for _, row in weight_df.iterrows():\n",
    "    print(f\"  State {int(row['state']):2d}: {row['weight']:.1%}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Total: {weight_df['weight'].sum():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build synthetic California with constrained weights\n",
    "calif_synth_constrained = (cigar.query('~california')\n",
    "                           .pivot(index='year', columns='state')['cigsale']\n",
    "                           .values.dot(weights_constrained))\n",
    "\n",
    "# Compare OLS vs Constrained\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: OLS\n",
    "ax = axes[0]\n",
    "ax.plot(years, calif_actual, 'o-', color=TUFTE_PALETTE['treatment'], \n",
    "        linewidth=2, markersize=5, label='California')\n",
    "ax.plot(years, calif_synth_ols, 's--', color=TUFTE_PALETTE['secondary'], \n",
    "        linewidth=2, markersize=5, alpha=0.7, label='Synthetic (OLS)')\n",
    "ax.axvline(1988, color=TUFTE_PALETTE['effect'], linestyle=':', linewidth=2)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Cigarette Sales')\n",
    "ax.set_title('(a) OLS: Overfitting')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: Constrained\n",
    "ax = axes[1]\n",
    "ax.plot(years, calif_actual, 'o-', color=TUFTE_PALETTE['treatment'], \n",
    "        linewidth=2, markersize=5, label='California')\n",
    "ax.plot(years, calif_synth_constrained, 's-', color=TUFTE_PALETTE['effect'], \n",
    "        linewidth=2, markersize=5, label='Synthetic (Constrained)')\n",
    "ax.axvline(1988, color=TUFTE_PALETTE['effect'], linestyle=':', linewidth=2)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Cigarette Sales')\n",
    "ax.set_title('(b) Constrained: Smooth Projection')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify the difference\n",
    "pre_mask = years < 1988\n",
    "post_mask = years >= 1988\n",
    "\n",
    "rmse_ols_pre = np.sqrt(np.mean((calif_actual[pre_mask] - calif_synth_ols[pre_mask])**2))\n",
    "rmse_ols_post = np.sqrt(np.mean((calif_actual[post_mask] - calif_synth_ols[post_mask])**2))\n",
    "rmse_constr_pre = np.sqrt(np.mean((calif_actual[pre_mask] - calif_synth_constrained[pre_mask])**2))\n",
    "rmse_constr_post = np.sqrt(np.mean((calif_actual[post_mask] - calif_synth_constrained[post_mask])**2))\n",
    "\n",
    "print(\"\\nFIT QUALITY COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Method':<20} {'Pre-RMSE':>12} {'Post-RMSE':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'OLS (unconstrained)':<20} {rmse_ols_pre:>12.2f} {rmse_ols_post:>12.2f}\")\n",
    "print(f\"{'Constrained':<20} {rmse_constr_pre:>12.2f} {rmse_constr_post:>12.2f}\")\n",
    "print(\"\\nNote: OLS has perfect pre-fit but erratic post-fit\")\n",
    "print(\"      Constrained trades some pre-fit for stable projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "★ Insight ─────────────────────────────────────\n",
    "**Constrained optimization:**\n",
    "\n",
    "- Prevents overfitting by limiting to convex combinations\n",
    "- Produces sparse, interpretable weights\n",
    "- Small pre-treatment error traded for stable post-treatment projection\n",
    "- This is the core of the synthetic control method\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "**Production code**:\n",
    "\n",
    "```python\n",
    "from scipy.optimize import fmin_slsqp\n",
    "from functools import partial\n",
    "\n",
    "def loss_w(W, X, y):\n",
    "    \"\"\"RMSE loss for synthetic control.\"\"\"\n",
    "    return np.sqrt(np.mean((y - X.dot(W))**2))\n",
    "\n",
    "def get_synthetic_weights(X, y):\n",
    "    \"\"\"\n",
    "    Find optimal synthetic control weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array (T_pre, J)\n",
    "        Pre-treatment outcomes for J control units\n",
    "    y : array (T_pre,)\n",
    "        Pre-treatment outcomes for treated unit\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    weights : array (J,)\n",
    "        Non-negative weights summing to 1\n",
    "    \"\"\"\n",
    "    J = X.shape[1]\n",
    "    w_init = np.ones(J) / J  # Equal weights start\n",
    "    \n",
    "    weights = fmin_slsqp(\n",
    "        partial(loss_w, X=X, y=y),\n",
    "        w_init,\n",
    "        f_eqcons=lambda w: np.sum(w) - 1,\n",
    "        bounds=[(0, 1)] * J,\n",
    "        disp=False\n",
    "    )\n",
    "    return weights\n",
    "```\n",
    "\n",
    "**Alternative: CVXPY** for more complex constraints:\n",
    "\n",
    "```python\n",
    "import cvxpy as cp\n",
    "\n",
    "W = cp.Variable(J, nonneg=True)\n",
    "objective = cp.Minimize(cp.sum_squares(y - X @ W))\n",
    "constraints = [cp.sum(W) == 1]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "prob.solve()\n",
    "weights = W.value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Meta E5, DS)**: *\"Why do we use constrained optimization instead of just OLS?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**OLS problems in synthetic control**:\n",
    "\n",
    "1. **Overfitting**: With J controls and T<J time periods, OLS has more parameters than observations\n",
    "   - Perfect pre-treatment fit\n",
    "   - Erratic, unreliable post-treatment projection\n",
    "\n",
    "2. **Extrapolation**: OLS allows negative and unbounded weights\n",
    "   - \"Subtract 50% of Texas\" is meaningless\n",
    "   - Extrapolates beyond observed data\n",
    "\n",
    "3. **Interpretability**: OLS weights are hard to explain\n",
    "   - What does w = -1.2 mean?\n",
    "\n",
    "**Constrained optimization fixes these**:\n",
    "\n",
    "1. **Regularization**: Constraints limit degrees of freedom → less overfitting\n",
    "2. **Interpolation**: Convex combination stays within the data range\n",
    "3. **Interpretability**: \"30% Nevada + 25% Utah\" is meaningful\n",
    "\n",
    "**Key insight**: The constraints act like regularization (similar to Lasso), encouraging sparse, stable solutions.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Google L5, Quant)**: *\"How do you choose the importance weights $v_h$ for different features?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Three approaches**:\n",
    "\n",
    "1. **Equal weights** (simple):\n",
    "   - $v_h = 1$ for all features\n",
    "   - Requires feature standardization first\n",
    "   - Most common in practice\n",
    "\n",
    "2. **Inverse variance** (automatic):\n",
    "   - $v_h = 1/\\text{Var}(X_h)$\n",
    "   - Features with high variance get less weight\n",
    "   - Similar to standardization\n",
    "\n",
    "3. **Cross-validated** (optimal):\n",
    "   - Choose $V$ to minimize out-of-sample prediction error\n",
    "   - Nest optimization: outer loop over $V$, inner loop over $W$\n",
    "   - Computationally expensive but more principled\n",
    "\n",
    "**Practical advice**:\n",
    "- Start with equal weights on standardized features\n",
    "- If outcome prediction is key, give lagged outcomes higher weight\n",
    "- Cross-validation is gold standard but often not worth the complexity\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Amazon L6, Econ)**: *\"What happens if the treated unit is outside the convex hull of control units?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Outside the hull = can't be perfectly matched**.\n",
    "\n",
    "**Example**: California has the lowest cigarette sales AND highest prices. No positive combination of other states can match both.\n",
    "\n",
    "**Consequences**:\n",
    "\n",
    "1. **Non-zero pre-treatment error**: Some mismatch is unavoidable\n",
    "2. **Extrapolation risk**: The counterfactual is partly \"made up\"\n",
    "3. **Bias potential**: Treatment effect may be biased\n",
    "\n",
    "**What to do**:\n",
    "\n",
    "1. **Report pre-treatment fit**: Be transparent about the mismatch\n",
    "2. **Sensitivity analysis**: Show results under different weight specifications\n",
    "3. **Covariate adjustment**: Augmented SCM adds regression adjustment\n",
    "4. **Consider**: Maybe SCM isn't appropriate for this case\n",
    "\n",
    "**Key interview point**: Being outside the hull is informative—it tells us the treated unit is \"extreme\" in ways the controls can't capture.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 15: \"Synthetic Control.\"\n",
    "\n",
    "[^2]: Abadie, A., Diamond, A., and Hainmueller, J. (2010). Synthetic Control Methods for Comparative Case Studies. *JASA*, 105(490), 493-505.\n",
    "\n",
    "[^3]: Abadie, A. and Gardeazabal, J. (2003). The Economic Costs of Conflict: A Case Study of the Basque Country. *American Economic Review*, 93(1), 113-132.\n",
    "\n",
    "[^4]: Doudchenko, N. and Imbens, G. W. (2016). Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis. NBER Working Paper No. 22791.\n",
    "\n",
    "---\n",
    "\n",
    "**Precision Improvement:**\n",
    "- You said: \"Build optimization notebook\"\n",
    "- Concise: \"Build 02_optimization.ipynb\"\n",
    "- Precise: `/augmented 15.2 --fmin-slsqp --convex-constraints`\n",
    "- Pattern: [build] [target] [content-flags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
