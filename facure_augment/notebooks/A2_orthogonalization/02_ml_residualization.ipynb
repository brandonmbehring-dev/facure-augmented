{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing with Orthogonalization: ML Residualization\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Appendix A2 | Notebook 2 of 3**\n",
    "\n",
    "Extending FWL to machine learning: using flexible models for\n",
    "residualization while maintaining causal identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from facure_augment.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### From Linear to Nonparametric\n",
    "\n",
    "FWL works because:\n",
    "- Residualization removes confounding variation\n",
    "- The \"as-if-random\" treatment variation remains\n",
    "\n",
    "**Key insight from Chernozhukov et al. (2018)**:\n",
    "> The residualization step can use ANY consistent estimator.\n",
    "\n",
    "This means we can replace linear regression with:\n",
    "- Random Forests\n",
    "- Gradient Boosting\n",
    "- Neural Networks\n",
    "- Any ML model\n",
    "\n",
    "### Why Use ML?\n",
    "\n",
    "| Linear Regression | ML Models |\n",
    "|-------------------|----------|\n",
    "| Assumes linearity | Captures nonlinearities |\n",
    "| Manual interactions | Automatic interactions |\n",
    "| Low-dimensional | High-dimensional |\n",
    "| Needs specification | Data-driven |\n",
    "\n",
    "### The Catch\n",
    "\n",
    "ML models can overfit, which creates problems:\n",
    "- If $M_T(X)$ overfits → $T^* \\approx 0$ (no variation)\n",
    "- If $M_Y(X)$ overfits → $Y^* \\approx 0$ (no signal)\n",
    "\n",
    "**Solution**: Cross-fitting (covered in notebook 03)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 The Partial Linear Model\n",
    "\n",
    "Assume the true relationship is:\n",
    "$$Y = \\tau \\cdot T + g(X) + \\epsilon$$\n",
    "$$T = m(X) + \\nu$$\n",
    "\n",
    "where:\n",
    "- $\\tau$ is the treatment effect (constant)\n",
    "- $g(X)$ and $m(X)$ are **unknown** functions\n",
    "- $\\epsilon, \\nu$ are mean-zero errors\n",
    "\n",
    "### 2.2 Robinson (1988) Transformation\n",
    "\n",
    "Define:\n",
    "$$\\tilde{Y} = Y - E[Y|X] = Y - \\ell(X)$$\n",
    "$$\\tilde{T} = T - E[T|X] = T - m(X)$$\n",
    "\n",
    "where $\\ell(X) = \\tau \\cdot m(X) + g(X)$ is the reduced form.\n",
    "\n",
    "**Key result**:\n",
    "$$\\tilde{Y} = \\tau \\cdot \\tilde{T} + \\tilde{\\epsilon}$$\n",
    "\n",
    "This is a simple regression with **known** slope $\\tau$!\n",
    "\n",
    "### 2.3 ML Estimation\n",
    "\n",
    "Replace unknown expectations with ML estimates:\n",
    "$$\\hat{\\tilde{Y}} = Y - \\hat{\\ell}(X)$$\n",
    "$$\\hat{\\tilde{T}} = T - \\hat{m}(X)$$\n",
    "\n",
    "Estimate $\\tau$ via:\n",
    "$$\\hat{\\tau} = \\frac{\\sum_i \\hat{\\tilde{Y}}_i \\hat{\\tilde{T}}_i}{\\sum_i \\hat{\\tilde{T}}_i^2}$$\n",
    "\n",
    "### 2.4 Why This Works\n",
    "\n",
    "Under regularity conditions:\n",
    "1. ML models converge to true $E[Y|X]$ and $E[T|X]$\n",
    "2. Residuals converge to \"true\" residuals\n",
    "3. Final regression recovers $\\tau$\n",
    "\n",
    "**Neyman orthogonality** ensures ML estimation errors don't bias $\\hat{\\tau}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Simulated Data with Nonlinear Confounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonlinear_confounding(n=5000, seed=42):\n",
    "    \"\"\"Generate data with nonlinear confounding that defeats linear regression.\n",
    "    \n",
    "    DGP:\n",
    "    - True treatment effect: τ = -2.0\n",
    "    - Confounding: X affects both T and Y nonlinearly\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Confounders\n",
    "    X1 = np.random.uniform(0, 10, n)\n",
    "    X2 = np.random.uniform(0, 5, n)\n",
    "    \n",
    "    # Nonlinear treatment assignment\n",
    "    m_X = 5 + 2*np.sin(X1) + 0.5*X2**2  # True E[T|X]\n",
    "    T = m_X + np.random.normal(0, 1, n)\n",
    "    \n",
    "    # Nonlinear outcome\n",
    "    TAU = -2.0  # True treatment effect\n",
    "    g_X = 10 + 3*np.cos(X1) + X1*X2  # True g(X)\n",
    "    Y = TAU * T + g_X + np.random.normal(0, 2, n)\n",
    "    \n",
    "    return pd.DataFrame({'X1': X1, 'X2': X2, 'T': T, 'Y': Y}), TAU\n",
    "\n",
    "data, TRUE_TAU = generate_nonlinear_confounding()\n",
    "print(f\"True treatment effect: τ = {TRUE_TAU}\")\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression (misspecified)\n",
    "linear_model = smf.ols(\"Y ~ T + X1 + X2\", data=data).fit()\n",
    "tau_linear = linear_model.params['T']\n",
    "\n",
    "print(\"Linear Regression Result:\")\n",
    "print(f\"  Estimated τ: {tau_linear:.4f}\")\n",
    "print(f\"  True τ:      {TRUE_TAU:.4f}\")\n",
    "print(f\"  Bias:        {tau_linear - TRUE_TAU:.4f}\")\n",
    "print(\"\\n⚠️  Linear model is BIASED due to nonlinear confounding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Residualization (Naive - Without Cross-Fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ML models on SAME data (problematic!)\n",
    "X_cols = ['X1', 'X2']\n",
    "\n",
    "# Model for E[T|X]\n",
    "m_t = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "m_t.fit(data[X_cols], data['T'])\n",
    "T_pred = m_t.predict(data[X_cols])\n",
    "\n",
    "# Model for E[Y|X]  \n",
    "m_y = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "m_y.fit(data[X_cols], data['Y'])\n",
    "Y_pred = m_y.predict(data[X_cols])\n",
    "\n",
    "# Residuals (SAME sample - naive approach)\n",
    "T_star_naive = data['T'] - T_pred\n",
    "Y_star_naive = data['Y'] - Y_pred\n",
    "\n",
    "# Estimate tau from residuals\n",
    "tau_naive_ml = np.sum(T_star_naive * Y_star_naive) / np.sum(T_star_naive**2)\n",
    "\n",
    "print(\"Naive ML Residualization (same-sample):\")\n",
    "print(f\"  Estimated τ: {tau_naive_ml:.4f}\")\n",
    "print(f\"  True τ:      {TRUE_TAU:.4f}\")\n",
    "print(f\"  Bias:        {tau_naive_ml - TRUE_TAU:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check residual variance\n",
    "print(f\"\\nResidual variances (indicative of overfitting):\")\n",
    "print(f\"  Var(T*): {np.var(T_star_naive):.4f} (should be ~1.0 from noise)\")\n",
    "print(f\"  Var(Y*): {np.var(Y_star_naive):.4f} (should be ~4.0 from noise)\")\n",
    "\n",
    "# Compare to linear residuals\n",
    "m_t_lin = smf.ols(\"T ~ X1 + X2\", data=data).fit()\n",
    "m_y_lin = smf.ols(\"Y ~ X1 + X2\", data=data).fit()\n",
    "print(f\"\\nLinear residual variances (reference):\")\n",
    "print(f\"  Var(T* linear): {np.var(m_t_lin.resid):.4f}\")\n",
    "print(f\"  Var(Y* linear): {np.var(m_y_lin.resid):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original data\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(data['T'], data['Y'], c=data['X1'], cmap='viridis', alpha=0.3, s=10)\n",
    "plt.colorbar(scatter, ax=ax, label='X1')\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Outcome Y')\n",
    "ax.set_title('Original Data (confounded by X1)')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# ML residualized data\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(T_star_naive, Y_star_naive, c=data['X1'], cmap='viridis', alpha=0.3, s=10)\n",
    "plt.colorbar(scatter, ax=ax, label='X1')\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add regression line\n",
    "x_line = np.linspace(T_star_naive.min(), T_star_naive.max(), 100)\n",
    "ax.plot(x_line, tau_naive_ml * x_line, 'r-', linewidth=2, label=f'Slope = {tau_naive_ml:.2f}')\n",
    "ax.set_xlabel('T* (residualized treatment)')\n",
    "ax.set_ylabel('Y* (residualized outcome)')\n",
    "ax.set_title('ML Residualized Data\\n(confounding removed)')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "ML residualization visibly removes confounding:\n",
    "- Original data: X1 creates positive correlation\n",
    "- Residualized data: Clear negative slope (true effect)\n",
    "\n",
    "Note: The naive (same-sample) approach shown here has\n",
    "overfitting issues. Cross-fitting is needed for valid inference.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Comparing ML Models for Residualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_residualize(data, X_cols, T_col, Y_col, model_class, **model_kwargs):\n",
    "    \"\"\"Residualize Y and T using ML model (naive, same-sample).\n",
    "    \n",
    "    WARNING: This is for demonstration only. \n",
    "    Use cross-fitting for valid inference.\n",
    "    \"\"\"\n",
    "    X = data[X_cols]\n",
    "    T = data[T_col]\n",
    "    Y = data[Y_col]\n",
    "    \n",
    "    # Fit models\n",
    "    m_t = model_class(**model_kwargs)\n",
    "    m_t.fit(X, T)\n",
    "    T_star = T - m_t.predict(X)\n",
    "    \n",
    "    m_y = model_class(**model_kwargs)\n",
    "    m_y.fit(X, Y)\n",
    "    Y_star = Y - m_y.predict(X)\n",
    "    \n",
    "    # Estimate tau\n",
    "    tau = np.sum(T_star * Y_star) / np.sum(T_star**2)\n",
    "    \n",
    "    return tau, T_star, Y_star\n",
    "\n",
    "# Compare different ML models\n",
    "results = {\n",
    "    'Linear': tau_linear,\n",
    "}\n",
    "\n",
    "# Random Forest\n",
    "tau_rf, _, _ = ml_residualize(data, X_cols, 'T', 'Y', \n",
    "                              RandomForestRegressor, n_estimators=100, max_depth=5, random_state=42)\n",
    "results['Random Forest'] = tau_rf\n",
    "\n",
    "# Gradient Boosting\n",
    "tau_gb, _, _ = ml_residualize(data, X_cols, 'T', 'Y',\n",
    "                              GradientBoostingRegressor, n_estimators=100, max_depth=3, random_state=42)\n",
    "results['Gradient Boosting'] = tau_gb\n",
    "\n",
    "# Display results\n",
    "print(f\"True τ = {TRUE_TAU}\")\n",
    "print(\"\\nEstimated τ by method:\")\n",
    "for method, tau in results.items():\n",
    "    bias = tau - TRUE_TAU\n",
    "    print(f\"  {method:20s}: τ = {tau:7.4f}, bias = {bias:+7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data: Ice Cream Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data\n",
    "prices = load_facure_data(\"ice_cream_sales.csv\")\n",
    "\n",
    "# Linear residualization\n",
    "m_t_lin = smf.ols(\"price ~ cost + temp + C(weekday)\", data=prices).fit()\n",
    "m_y_lin = smf.ols(\"sales ~ cost + temp + C(weekday)\", data=prices).fit()\n",
    "T_star_lin = prices['price'] - m_t_lin.predict(prices)\n",
    "Y_star_lin = prices['sales'] - m_y_lin.predict(prices)\n",
    "tau_lin = np.sum(T_star_lin * Y_star_lin) / np.sum(T_star_lin**2)\n",
    "\n",
    "# ML residualization (Random Forest)\n",
    "X_cols = ['cost', 'temp', 'weekday']\n",
    "m_t_rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "m_t_rf.fit(prices[X_cols], prices['price'])\n",
    "T_star_rf = prices['price'] - m_t_rf.predict(prices[X_cols])\n",
    "\n",
    "m_y_rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "m_y_rf.fit(prices[X_cols], prices['sales'])\n",
    "Y_star_rf = prices['sales'] - m_y_rf.predict(prices[X_cols])\n",
    "\n",
    "tau_rf = np.sum(T_star_rf * Y_star_rf) / np.sum(T_star_rf**2)\n",
    "\n",
    "print(\"Ice Cream Price Elasticity (naive, same-sample):\")\n",
    "print(f\"  Linear residualization: τ = {tau_lin:.4f}\")\n",
    "print(f\"  RF residualization:     τ = {tau_rf:.4f}\")\n",
    "print(\"\\nBoth show NEGATIVE elasticity (makes economic sense!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear\n",
    "ax = axes[0]\n",
    "sample_idx = np.random.choice(len(prices), 1000, replace=False)\n",
    "ax.scatter(T_star_lin.iloc[sample_idx], Y_star_lin.iloc[sample_idx], alpha=0.3, s=10, c=COLORS['blue'])\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "x_line = np.linspace(-3, 3, 100)\n",
    "ax.plot(x_line, tau_lin * x_line, 'r-', linewidth=2, label=f'τ = {tau_lin:.2f}')\n",
    "ax.set_xlabel('Price* (linear residual)')\n",
    "ax.set_ylabel('Sales* (linear residual)')\n",
    "ax.set_title('Linear Residualization')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Random Forest\n",
    "ax = axes[1]\n",
    "ax.scatter(T_star_rf.iloc[sample_idx], Y_star_rf.iloc[sample_idx], alpha=0.3, s=10, c=COLORS['orange'])\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.plot(x_line, tau_rf * x_line, 'r-', linewidth=2, label=f'τ = {tau_rf:.2f}')\n",
    "ax.set_xlabel('Price* (RF residual)')\n",
    "ax.set_ylabel('Sales* (RF residual)')\n",
    "ax.set_title('Random Forest Residualization')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Key Takeaway ────────────────────────────────────────────────\n",
    "ML residualization extends FWL to nonparametric settings:\n",
    "\n",
    "1. **Replace linear models with ML** for E[T|X] and E[Y|X]\n",
    "2. **Captures nonlinearities** and interactions automatically\n",
    "3. **Final regression still simple** (Y* on T*)\n",
    "4. **Critical issue**: Must use cross-fitting to avoid overfitting\n",
    "\n",
    "Next notebook: Why cross-fitting is essential.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: What is the partial linear model and why is it useful?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Model**:\n",
    "$$Y = \\tau T + g(X) + \\epsilon$$\n",
    "$$T = m(X) + \\nu$$\n",
    "\n",
    "**Features**:\n",
    "- Treatment effect $\\tau$ is parametric (single coefficient)\n",
    "- Confounding $g(X)$ and $m(X)$ are nonparametric (unknown functions)\n",
    "- \"Partial\" because only part is parametric\n",
    "\n",
    "**Why useful**:\n",
    "1. Allows flexible confounding control without specifying functional form\n",
    "2. Treatment effect remains interpretable\n",
    "3. Can use ML for nuisance functions\n",
    "4. Foundation for Double/Debiased ML\n",
    "\n",
    "**Estimation via Robinson transform**:\n",
    "$$\\tilde{Y} = \\tau \\tilde{T} + \\tilde{\\epsilon}$$\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Why can we use ML models for residualization?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**FWL requires only** that residuals are orthogonal to controls:\n",
    "- $E[\\tilde{T} | X] = 0$ (treatment residual uncorrelated with X)\n",
    "- $E[\\tilde{Y} | X] = 0$ (outcome residual uncorrelated with X)\n",
    "\n",
    "**Any consistent estimator** of $E[T|X]$ and $E[Y|X]$ achieves this:\n",
    "- Linear regression: consistent if linear specification correct\n",
    "- ML models: consistent under regularity conditions\n",
    "- Both produce residuals orthogonal to X\n",
    "\n",
    "**ML advantages**:\n",
    "- Captures nonlinear relationships\n",
    "- Handles high-dimensional X\n",
    "- Data-driven (no manual specification)\n",
    "\n",
    "**Caveat**: Need cross-fitting to avoid overfitting bias.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: What happens if the treatment model overfits?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Overfitting** means $\\hat{m}(X) \\approx T$ on training data.\n",
    "\n",
    "**Consequences**:\n",
    "1. $\\hat{T}^* = T - \\hat{m}(X) \\approx 0$\n",
    "2. Very little treatment variation remains\n",
    "3. Denominator $\\sum \\hat{T}^{*2} \\approx 0$\n",
    "4. $\\hat{\\tau}$ becomes unstable or undefined\n",
    "\n",
    "**Intuition**: If we perfectly predict T from X, there's no \"as-if-random\" variation left.\n",
    "\n",
    "**Solution**: Cross-fitting\n",
    "- Fit model on fold A\n",
    "- Predict on fold B\n",
    "- Prevents overfitting because model hasn't seen prediction data\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: Why must the outcome model be regression, not classification?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**For binary T or Y**, it's tempting to use classification.\n",
    "\n",
    "**Why regression is required**:\n",
    "\n",
    "1. **We need E[T|X], not P(T=1|X)... actually, these are the same for binary T!**\n",
    "   - For binary T: $E[T|X] = P(T=1|X)$\n",
    "   - Classification probability IS the conditional expectation\n",
    "\n",
    "2. **The real issue**: Classification outputs discrete classes, not probabilities\n",
    "   - If using `predict()` instead of `predict_proba()`, residuals are discrete\n",
    "   - Discrete residuals break FWL assumptions\n",
    "\n",
    "3. **For continuous Y**: Classification doesn't apply at all\n",
    "   - Must use regression to get $E[Y|X]$\n",
    "\n",
    "**Best practice**: Always use regression (or classification probabilities for binary).\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: How do you choose which ML model to use for residualization?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Guiding principle**: Good prediction → good residualization.\n",
    "\n",
    "**Model selection criteria**:\n",
    "1. **Predictive performance** (cross-validated R²)\n",
    "2. **Computational cost** (especially for bootstrapping)\n",
    "3. **Interpretability** (if you want to understand nuisance functions)\n",
    "\n",
    "**Common choices**:\n",
    "- Random Forest: Robust, handles interactions, parallelizable\n",
    "- Gradient Boosting: Often best performance, slower\n",
    "- Lasso: Good for sparse, high-dimensional X\n",
    "- Neural Networks: For very large datasets\n",
    "\n",
    "**Robustness approach**: Use multiple models, check if results are similar.\n",
    "\n",
    "**Double robustness**: Combine PS and outcome regression (DR estimator).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Robinson, P. M. (1988). Root-N-Consistent Semiparametric Regression.\n",
    "      *Econometrica*, 56(4), 931-954.\n",
    "\n",
    "[^2]: Chernozhukov, V., et al. (2018). Double/Debiased Machine Learning for\n",
    "      Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, Appendix:\n",
    "      Debiasing with Orthogonalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
