{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20.3 F-Learner Limitations\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Implementation](#implementation)\n",
    "4. [Numeric Demonstration](#numeric)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand why F-learner has high variance\n",
    "- Recognize the nonlinear treatment effect problem\n",
    "- Apply linearization strategies for continuous treatments\n",
    "- Know when to use F-learner vs. other meta-learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(123)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition <a name=\"intuition\"></a>\n",
    "\n",
    "### The Price of Simplicity\n",
    "\n",
    "F-learner's target transformation is elegantly simple:\n",
    "$$Y^* = Y \\cdot \\frac{T - e(X)}{e(X)(1-e(X))}$$\n",
    "\n",
    "But this simplicity comes with costs:\n",
    "\n",
    "### Limitation 1: High Variance\n",
    "\n",
    "The transformed target $Y^*$ is a **noisy individual estimate** of CATE:\n",
    "- Each unit contributes one data point\n",
    "- That single point must carry information about the treatment effect\n",
    "- Variance is amplified by propensity weighting\n",
    "\n",
    "**Consequence**: Overfitting, especially with small samples or extreme propensities.\n",
    "\n",
    "### Limitation 2: Nonlinear Treatment Effects\n",
    "\n",
    "For continuous treatment, we assume local linearity:\n",
    "$$Y = \\alpha + \\beta T + e$$\n",
    "\n",
    "But treatment effects often **saturate**:\n",
    "- First price increase → large sales drop\n",
    "- Later increases → diminishing impact\n",
    "\n",
    "**Problem**: Elasticity changes with treatment level, not just covariates.\n",
    "\n",
    "### Limitation 3: Binary Treatment Only (Standard Form)\n",
    "\n",
    "The original F-learner is defined for binary T ∈ {0, 1}.\n",
    "Continuous extension exists but lacks solid theoretical foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment <a name=\"formal\"></a>\n",
    "\n",
    "### Variance Analysis\n",
    "\n",
    "For binary treatment F-learner:\n",
    "$$Y^* = Y \\cdot \\frac{T - e(X)}{e(X)(1-e(X))}$$\n",
    "\n",
    "**Variance decomposition**:\n",
    "$$Var(Y^* | X) = \\frac{Var(Y(1)|X)}{e(X)} + \\frac{Var(Y(0)|X)}{1-e(X)}$$\n",
    "\n",
    "**Key insight**: As $e(X) \\to 0$ or $e(X) \\to 1$, variance explodes.\n",
    "\n",
    "This is the **positivity problem** - extreme propensities lead to unstable estimates.\n",
    "\n",
    "### Nonlinear Treatment Effects\n",
    "\n",
    "Consider demand with diminishing sensitivity:\n",
    "$$D = \\frac{1}{P^\\alpha}$$\n",
    "\n",
    "where $\\alpha$ is the price sensitivity parameter.\n",
    "\n",
    "**Problem**: The slope $\\frac{\\partial D}{\\partial P}$ varies with $P$:\n",
    "$$\\frac{\\partial D}{\\partial P} = -\\alpha P^{-(\\alpha+1)}$$\n",
    "\n",
    "Two units at different price points can have different slopes **even with identical covariates**.\n",
    "\n",
    "### Linearization Strategy\n",
    "\n",
    "Taking logs:\n",
    "$$\\log(D) = -\\alpha \\log(P)$$\n",
    "\n",
    "Now the relationship is linear with constant elasticity $-\\alpha$.\n",
    "\n",
    "**General principle**: Transform $Y$ and/or $T$ to achieve approximately linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementation <a name=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_learner_transform(Y, T, ps):\n",
    "    \"\"\"F-learner target transformation for binary treatment.\"\"\"\n",
    "    return Y * (T - ps) / (ps * (1 - ps))\n",
    "\n",
    "def continuous_f_learner_transform(Y, T, scale=False):\n",
    "    \"\"\"F-learner target transformation for continuous treatment.\"\"\"\n",
    "    Y_centered = Y - Y.mean()\n",
    "    T_centered = T - T.mean()\n",
    "    if scale:\n",
    "        return Y_centered * T_centered / T.var()\n",
    "    return Y_centered * T_centered\n",
    "\n",
    "def sensitivity(data, y, t):\n",
    "    \"\"\"Estimate treatment effect via OLS coefficient.\"\"\"\n",
    "    t_bar = data[t].mean()\n",
    "    y_bar = data[y].mean()\n",
    "    cov = np.sum((data[t] - t_bar) * (data[y] - y_bar))\n",
    "    var = np.sum((data[t] - t_bar) ** 2)\n",
    "    return cov / var if var > 0 else 0\n",
    "\n",
    "def cumulative_gain(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    \"\"\"Compute cumulative gain curve.\"\"\"\n",
    "    size = dataset.shape[0]\n",
    "    ordered_df = dataset.sort_values(prediction, ascending=False).reset_index(drop=True)\n",
    "    n_rows = list(range(min_periods, size, size // steps)) + [size]\n",
    "    return np.array([sensitivity(ordered_df.head(rows), y, t) * (rows/size) \n",
    "                     for rows in n_rows])\n",
    "\n",
    "print(\"Utility functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Numeric Demonstration <a name=\"numeric\"></a>\n",
    "\n",
    "### Demo 1: Variance Problem with Extreme Propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate RCT with varying propensities\n",
    "np.random.seed(42)\n",
    "n = 5000\n",
    "\n",
    "# True CATE varies with X\n",
    "X = np.random.randn(n, 2)\n",
    "true_cate = 2 + X[:, 0]  # CATE = 2 + X1\n",
    "\n",
    "# Simulate outcomes\n",
    "Y0 = X[:, 0] + 0.5 * X[:, 1] + np.random.randn(n)\n",
    "Y1 = Y0 + true_cate\n",
    "\n",
    "def run_f_learner_simulation(ps_value, n_sims=100):\n",
    "    \"\"\"Run F-learner with fixed propensity, return CATE estimates.\"\"\"\n",
    "    cate_estimates = []\n",
    "    \n",
    "    for _ in range(n_sims):\n",
    "        # Random treatment assignment\n",
    "        T = np.random.binomial(1, ps_value, n)\n",
    "        Y = T * Y1 + (1 - T) * Y0\n",
    "        \n",
    "        # F-learner transformation\n",
    "        Y_star = f_learner_transform(Y, T, ps_value)\n",
    "        \n",
    "        # Fit model\n",
    "        model = LGBMRegressor(max_depth=2, n_estimators=50, verbose=-1)\n",
    "        model.fit(X, Y_star)\n",
    "        \n",
    "        # Average CATE estimate\n",
    "        cate_estimates.append(model.predict(X).mean())\n",
    "    \n",
    "    return np.array(cate_estimates)\n",
    "\n",
    "# Compare propensities\n",
    "propensities = [0.5, 0.3, 0.1, 0.05]\n",
    "results = {}\n",
    "\n",
    "print(\"Running simulations (this may take a moment)...\")\n",
    "for ps in propensities:\n",
    "    estimates = run_f_learner_simulation(ps, n_sims=50)\n",
    "    results[ps] = {\n",
    "        'mean': estimates.mean(),\n",
    "        'std': estimates.std(),\n",
    "        'estimates': estimates\n",
    "    }\n",
    "    print(f\"  ps={ps}: Mean={estimates.mean():.3f}, Std={estimates.std():.3f}\")\n",
    "\n",
    "true_ate = true_cate.mean()\n",
    "print(f\"\\nTrue ATE: {true_ate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize variance explosion\nfig, axes = create_tufte_figure(ncols=2, figsize=(14, 5))\n\n# Left: Distribution of estimates\ncolors_list = [COLORS['blue'], COLORS['gray'], COLORS['red'], COLORS['orange']]\nfor ps, color in zip(propensities, colors_list):\n    axes[0].hist(results[ps]['estimates'], bins=15, alpha=0.5, \n                 label=f'ps={ps} (σ={results[ps][\"std\"]:.2f})', color=color)\n\naxes[0].axvline(x=true_ate, color='black', linestyle='--', linewidth=2, label=f'True ATE={true_ate:.2f}')\naxes[0].set_xlabel('Estimated ATE')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('F-Learner: Variance vs Propensity')\naxes[0].legend(loc='upper left')\n\n# Right: Variance by propensity\nps_range = np.linspace(0.05, 0.5, 20)\n# Theoretical variance scaling: 1/(ps*(1-ps))\ntheoretical_var = 1 / (ps_range * (1 - ps_range))\ntheoretical_var = theoretical_var / theoretical_var.min()  # Normalize\n\naxes[1].plot(ps_range, theoretical_var, 'b-', linewidth=2, label='Theoretical: 1/(e(1-e))')\naxes[1].scatter([0.5, 0.3, 0.1, 0.05], \n                [results[p]['std']**2 / results[0.5]['std']**2 for p in [0.5, 0.3, 0.1, 0.05]],\n                color='red', s=100, zorder=5, label='Observed')\naxes[1].set_xlabel('Propensity Score e(X)')\naxes[1].set_ylabel('Relative Variance')\naxes[1].set_title('Variance Explosion at Extreme Propensities')\naxes[1].legend()\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n★ Insight: As propensity moves away from 0.5, variance increases dramatically.\")\nprint(\"  This is why positivity (e bounded away from 0,1) is crucial.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: Nonlinear Treatment Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate demand with diminishing sensitivity\n",
    "np.random.seed(123)\n",
    "n = 3000\n",
    "\n",
    "# Covariates: temperature (affects base demand)\n",
    "temp = np.random.uniform(10, 35, n)\n",
    "\n",
    "# Randomized prices\n",
    "price = np.random.uniform(2, 10, n)\n",
    "\n",
    "# True DGP: Demand = BaseLevel / Price^alpha\n",
    "# alpha varies with temperature (hot days = less elastic)\n",
    "alpha = 1.5 - 0.02 * temp  # Lower alpha when hot\n",
    "base_demand = 500 + 10 * temp  # Higher base when hot\n",
    "demand = base_demand / (price ** alpha) + np.random.randn(n) * 20\n",
    "demand = np.maximum(demand, 10)  # Floor at 10\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'temp': temp,\n",
    "    'price': price,\n",
    "    'demand': demand,\n",
    "    'alpha': alpha\n",
    "})\n",
    "\n",
    "print(\"Simulated demand data:\")\n",
    "print(f\"  Demand range: [{demand.min():.0f}, {demand.max():.0f}]\")\n",
    "print(f\"  Price range: [{price.min():.1f}, {price.max():.1f}]\")\n",
    "print(f\"  Elasticity (α) range: [{alpha.min():.2f}, {alpha.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the nonlinearity problem\nfig, axes = create_tufte_figure(ncols=2, figsize=(14, 5))\n\n# Left: Demand vs Price (nonlinear)\ncold_mask = temp < 20\nhot_mask = temp > 30\n\naxes[0].scatter(price[cold_mask], demand[cold_mask], alpha=0.3, s=20, \n                color=COLORS['blue'], label='Cold days (T<20°)')\naxes[0].scatter(price[hot_mask], demand[hot_mask], alpha=0.3, s=20, \n                color=COLORS['red'], label='Hot days (T>30°)')\n\n# Fit curves for visualization\np_range = np.linspace(2, 10, 100)\ncold_curve = (500 + 10*15) / (p_range ** (1.5 - 0.02*15))\nhot_curve = (500 + 10*32) / (p_range ** (1.5 - 0.02*32))\n\naxes[0].plot(p_range, cold_curve, color=COLORS['blue'], linewidth=2, linestyle='--')\naxes[0].plot(p_range, hot_curve, color=COLORS['red'], linewidth=2, linestyle='--')\n\naxes[0].set_xlabel('Price')\naxes[0].set_ylabel('Demand')\naxes[0].set_title('Nonlinear Demand Curves')\naxes[0].legend()\n\n# Right: The problem - slope varies with price\n# dD/dP = -α * BaseLevel * P^(-α-1)\ncold_slope = -1.2 * 650 * p_range**(-2.2)\nhot_slope = -0.86 * 820 * p_range**(-1.86)\n\naxes[1].plot(p_range, cold_slope, color=COLORS['blue'], linewidth=2, label='Cold days')\naxes[1].plot(p_range, hot_slope, color=COLORS['red'], linewidth=2, label='Hot days')\naxes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n\naxes[1].set_xlabel('Price')\naxes[1].set_ylabel('Marginal Effect (dD/dP)')\naxes[1].set_title('Elasticity Changes with Price Level')\naxes[1].legend()\n\n# Add annotation\naxes[1].annotate('At same price,\\ncold & hot can\\nhave same slope!', \n                 xy=(6, -40), fontsize=10, ha='center')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n★ Problem: At different price points, hot and cold days can have identical slopes.\")\nprint(\"  This confuses the F-learner - it can't distinguish temperature effect from price effect.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 3: Linearization Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add log transforms\n",
    "df['log_price'] = np.log(df['price'])\n",
    "df['log_demand'] = np.log(df['demand'])\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=123)\n",
    "\n",
    "# Approach 1: Raw F-learner (linear assumption on raw data)\n",
    "Y_star_raw = continuous_f_learner_transform(\n",
    "    train['demand'].values, \n",
    "    train['price'].values\n",
    ")\n",
    "\n",
    "model_raw = LGBMRegressor(max_depth=3, min_child_samples=200, verbose=-1)\n",
    "model_raw.fit(train[['temp']].values, Y_star_raw)\n",
    "\n",
    "# Approach 2: Log-log F-learner (linearized)\n",
    "Y_star_log = continuous_f_learner_transform(\n",
    "    train['log_demand'].values, \n",
    "    train['log_price'].values\n",
    ")\n",
    "\n",
    "model_log = LGBMRegressor(max_depth=3, min_child_samples=200, verbose=-1)\n",
    "model_log.fit(train[['temp']].values, Y_star_log)\n",
    "\n",
    "# Predict\n",
    "test_eval = test.copy()\n",
    "test_eval['cate_raw'] = model_raw.predict(test[['temp']].values)\n",
    "test_eval['cate_log'] = model_log.predict(test[['temp']].values)\n",
    "\n",
    "print(\"Model predictions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate both approaches\nfig, axes = create_tufte_figure(ncols=2, figsize=(14, 5))\n\n# Left: Raw approach - cumulative gain\ngain_raw = cumulative_gain(test_eval, 'cate_raw', y='demand', t='price')\nbaseline_raw = sensitivity(test, 'demand', 'price')\n\nx_axis = np.linspace(0, 100, len(gain_raw))\naxes[0].plot(x_axis, gain_raw, color=COLORS['blue'], linewidth=2, label='Raw F-Learner')\naxes[0].plot([0, 100], [0, baseline_raw], linestyle='--', color='black', \n             linewidth=1.5, label='Random')\naxes[0].set_xlabel('% of Population')\naxes[0].set_ylabel('Cumulative Gain')\naxes[0].set_title('Raw F-Learner (Assumes Linear)')\naxes[0].legend()\n\n# Right: Log-log approach - cumulative gain\ngain_log = cumulative_gain(test_eval, 'cate_log', y='log_demand', t='log_price')\nbaseline_log = sensitivity(test, 'log_demand', 'log_price')\n\naxes[1].plot(x_axis, gain_log, color=COLORS['gray'], linewidth=2, label='Log-Log F-Learner')\naxes[1].plot([0, 100], [0, baseline_log], linestyle='--', color='black', \n             linewidth=1.5, label='Random')\naxes[1].set_xlabel('% of Population')\naxes[1].set_ylabel('Cumulative Gain')\naxes[1].set_title('Log-Log F-Learner (Linearized)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n★ Insight: Log-log linearization enables the F-learner to capture true heterogeneity.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predicted vs actual elasticity by temperature\n",
    "test_eval['temp_bin'] = pd.cut(test_eval['temp'], bins=5)\n",
    "\n",
    "comparison = test_eval.groupby('temp_bin', observed=True).agg({\n",
    "    'cate_raw': 'mean',\n",
    "    'cate_log': 'mean',\n",
    "    'alpha': 'mean',  # True elasticity\n",
    "    'temp': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Actual elasticity by band (log-log)\n",
    "actual_elast = test_eval.groupby('temp_bin', observed=True).apply(\n",
    "    lambda df: sensitivity(df, 'log_demand', 'log_price'),\n",
    "    include_groups=False\n",
    ").rename('actual_log_elast')\n",
    "\n",
    "comparison = pd.concat([comparison, actual_elast], axis=1)\n",
    "print(\"Predicted vs Actual Elasticity by Temperature:\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **Log-log elasticity** is approximately constant (linear relationship achieved)\n",
    "2. **Predicted CATE (log)** correlates well with actual elasticity\n",
    "3. **True α decreases** with temperature (hot = less elastic), and model captures this\n",
    "\n",
    "### When to Use F-Learner\n",
    "\n",
    "| Situation | F-Learner Appropriate? |\n",
    "|-----------|------------------------|\n",
    "| Large sample (>10K) | ✅ Yes |\n",
    "| Balanced propensities (0.3 < e < 0.7) | ✅ Yes |\n",
    "| Binary treatment | ✅ Yes |\n",
    "| Need exact CATE magnitude | ⚠️ Consider alternatives |\n",
    "| Small sample | ❌ High variance |\n",
    "| Extreme propensities | ❌ Unstable |\n",
    "| Nonlinear dose-response | ❌ Without linearization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix <a name=\"interview\"></a>\n",
    "\n",
    "### Q1: What are the main limitations of the F-learner?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Three main limitations**:\n",
    "\n",
    "1. **High variance**: Transformed target $Y^*$ is noisy individual CATE estimate\n",
    "   - Variance ∝ $1/(e(1-e))$ - explodes at extreme propensities\n",
    "   - Leads to overfitting, especially with small samples\n",
    "\n",
    "2. **Binary treatment only** (standard form)\n",
    "   - Continuous extension lacks theoretical foundation\n",
    "   - Requires local linearity assumption\n",
    "\n",
    "3. **Nonlinear treatment effects**\n",
    "   - Assumes constant slope within covariate regions\n",
    "   - Fails when effect depends on treatment level itself\n",
    "   - Requires linearization (log transforms, etc.)\n",
    "\n",
    "**When to avoid F-learner**:\n",
    "- Small samples (<1K)\n",
    "- Extreme propensities (<0.1 or >0.9)\n",
    "- Known nonlinear dose-response\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: How do you handle nonlinear treatment effects?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Linearization strategies**:\n",
    "\n",
    "1. **Log-log transformation** (for constant elasticity):\n",
    "   $$\\log(Y) = \\alpha + \\beta \\log(T)$$\n",
    "   - Interpretation: $\\beta$ = % change in Y per % change in T\n",
    "\n",
    "2. **Log-level transformation** (for diminishing returns):\n",
    "   $$\\log(Y) = \\alpha + \\beta T$$\n",
    "   - Interpretation: $\\beta$ = % change in Y per unit T\n",
    "\n",
    "3. **Polynomial/spline terms** (for complex patterns):\n",
    "   - Include $T^2$, $T^3$ as features\n",
    "   - Use basis splines for flexible fit\n",
    "\n",
    "**Alternative approaches**:\n",
    "- Dose-response modeling (generalized propensity score)\n",
    "- Causal forests with treatment as continuous variable\n",
    "- Neural network CATE estimators\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: Compare F-learner to other meta-learners.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Meta-Learner | Pros | Cons |\n",
    "|--------------|------|------|\n",
    "| **F-Learner** | Simplest, any model | High variance, binary only |\n",
    "| **T-Learner** | Intuitive (μ₁ - μ₀) | Regularization bias, needs similar n₁, n₀ |\n",
    "| **S-Learner** | Simple single model | Underestimates heterogeneity |\n",
    "| **X-Learner** | Good for imbalanced | Complex, two stages |\n",
    "| **R-Learner** | Neyman orthogonal | Requires cross-fitting |\n",
    "| **Causal Forest** | Flexible, uncertainty | Computationally heavy |\n",
    "\n",
    "**Selection guidance**:\n",
    "- Large sample, balanced: F-Learner or S-Learner\n",
    "- Imbalanced treatment: X-Learner\n",
    "- Need valid inference: R-Learner or Causal Forest\n",
    "- High-dimensional: R-Learner with DML\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References <a name=\"references\"></a>\n",
    "\n",
    "1. **Künzel, S.R. et al.** (2019). *Metalearners for Estimating Heterogeneous Treatment Effects using Machine Learning*. PNAS.\n",
    "\n",
    "2. **Nie, X. & Wager, S.** (2021). *Quasi-oracle estimation of heterogeneous treatment effects*. Biometrika.\n",
    "\n",
    "3. **Athey, S. & Imbens, G.W.** (2016). *Machine Learning Methods for Estimating Heterogeneous Causal Effects*.\n",
    "\n",
    "4. **Facure, M.** (2022). *Causal Inference for the Brave and True*, Chapter 20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}