{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# KNN Matching and Bias Correction\n",
    "\n",
    "**Chapter 10, Section 3**\n",
    "\n",
    "This notebook covers K-nearest neighbors matching and the curse of dimensionality problem.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition) - KNN as matching estimator\n",
    "2. [Formal Treatment](#formal) - Matching bias and curse of dimensionality\n",
    "3. [Numeric Demonstration](#numeric) - Bias in high dimensions\n",
    "4. [Implementation](#implementation) - Bias-corrected matching\n",
    "5. [Interview Appendix](#interview) - Practice questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import *\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Set notebook style\n",
    "set_notebook_style()\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "### K-Nearest Neighbors Matching\n",
    "\n",
    "Instead of finding *one* exact match, KNN finds the *K closest* control units:\n",
    "\n",
    "$$\\hat{Y}_i(0) = \\frac{1}{K} \\sum_{j \\in \\mathcal{N}_K(i)} Y_j$$\n",
    "\n",
    "where $\\mathcal{N}_K(i)$ is the set of K nearest control units to treated unit $i$.\n",
    "\n",
    "**Why use K > 1?**\n",
    "- Reduces variance (average of K outcomes)\n",
    "- More robust to outliers\n",
    "- Trade-off: Introduces bias if matches are imperfect\n",
    "\n",
    "### The Curse of Dimensionality\n",
    "\n",
    "As the number of covariates increases, matching becomes harder:\n",
    "\n",
    "| # Features | Neighbor Distance | Match Quality |\n",
    "|------------|------------------|---------------|\n",
    "| 1 | Small | Excellent |\n",
    "| 5 | Moderate | Good |\n",
    "| 10 | Large | Poor |\n",
    "| 20 | Very large | Very poor |\n",
    "\n",
    "**Why?** In high dimensions, all points become approximately equidistant. The \"nearest\" neighbor may not be close at all.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load medicine data\n",
    "med = load_facure_data(\"medicine_impact_recovery.csv\")\n",
    "\n",
    "print(f\"Data: {len(med)} patients\")\n",
    "print(f\"Treatment: {med['medication'].mean():.1%} received medication\")\n",
    "print(f\"\\nFeatures: sex, age, severity\")\n",
    "med.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate curse of dimensionality with simulated data\n",
    "def simulate_nearest_neighbor_distance(n_samples=1000, n_features_list=[1, 2, 5, 10, 20, 50]):\n",
    "    \"\"\"Show how nearest neighbor distance grows with dimensions.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    results = []\n",
    "    \n",
    "    for n_features in n_features_list:\n",
    "        # Generate random data\n",
    "        X = np.random.randn(n_samples, n_features)\n",
    "        \n",
    "        # Find nearest neighbor distances\n",
    "        nn = NearestNeighbors(n_neighbors=2)  # 2 because first is self\n",
    "        nn.fit(X)\n",
    "        distances, _ = nn.kneighbors(X)\n",
    "        \n",
    "        # Second column is distance to nearest (non-self) neighbor\n",
    "        nn_distances = distances[:, 1]\n",
    "        \n",
    "        results.append({\n",
    "            'n_features': n_features,\n",
    "            'mean_distance': nn_distances.mean(),\n",
    "            'median_distance': np.median(nn_distances),\n",
    "            'max_distance': nn_distances.max()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "curse_results = simulate_nearest_neighbor_distance()\n",
    "\n",
    "print(\"CURSE OF DIMENSIONALITY:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Nearest neighbor distance grows with dimension count:\")\n",
    "print(curse_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the curse\n",
    "fig, ax = create_tufte_figure(figsize=(10, 6))\n",
    "\n",
    "ax.plot(curse_results['n_features'], curse_results['mean_distance'], \n",
    "        'o-', color=COLORS['blue'], linewidth=2, markersize=8, label='Mean distance')\n",
    "ax.plot(curse_results['n_features'], curse_results['median_distance'], \n",
    "        's--', color=COLORS['red'], linewidth=2, markersize=8, label='Median distance')\n",
    "\n",
    "set_tufte_title(ax, \"Curse of Dimensionality: Nearest Neighbor Distance Grows\")\n",
    "set_tufte_labels(ax, \"Number of Features\", \"Distance to Nearest Neighbor\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xticks(curse_results['n_features'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"With more features, 'nearest' neighbors aren't very near at all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Formal Treatment\n",
    "\n",
    "### Matching Bias\n",
    "\n",
    "Even with nearest neighbor matching, there's residual bias because matches aren't exact.\n",
    "\n",
    "**Definition** (Matching Bias):\n",
    "\n",
    "For treated unit $i$ matched to control unit $j(i)$:\n",
    "\n",
    "$$\\text{Bias}_i = E[Y_j(0) | X_{j(i)}] - E[Y_i(0) | X_i]$$\n",
    "\n",
    "If $X_{j(i)} \\neq X_i$, and the outcome depends on $X$, then this bias is non-zero.\n",
    "\n",
    "### Abadie-Imbens Bias Correction\n",
    "\n",
    "**Key insight**: Use regression to adjust for the covariate difference.\n",
    "\n",
    "**Step 1**: Estimate outcome model on controls:\n",
    "$$\\hat{\\mu}_0(x) = \\hat{\\alpha} + \\hat{\\beta}'x$$\n",
    "\n",
    "**Step 2**: Compute bias-corrected counterfactual:\n",
    "$$\\hat{Y}_i(0)^{BC} = Y_{j(i)} + [\\hat{\\mu}_0(X_i) - \\hat{\\mu}_0(X_{j(i)})]$$\n",
    "\n",
    "The correction term adjusts for the covariate difference:\n",
    "$$\\hat{\\mu}_0(X_i) - \\hat{\\mu}_0(X_{j(i)}) = \\hat{\\beta}'(X_i - X_{j(i)})$$\n",
    "\n",
    "### ATT with Bias Correction\n",
    "\n",
    "$$\\hat{\\tau}_{ATT}^{BC} = \\frac{1}{N_1} \\sum_{i: T_i=1} \\left[ Y_i - \\hat{Y}_i(0)^{BC} \\right]$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for matching\n",
    "features = ['sex', 'age', 'severity']\n",
    "X = med[features].values\n",
    "Y = med['recovery'].values\n",
    "T = med['medication'].values\n",
    "\n",
    "# Split by treatment\n",
    "X_treated = X[T == 1]\n",
    "X_control = X[T == 0]\n",
    "Y_treated = Y[T == 1]\n",
    "Y_control = Y[T == 0]\n",
    "\n",
    "print(f\"Treated: {len(X_treated)} patients\")\n",
    "print(f\"Control: {len(X_control)} patients\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_control_std = scaler.fit_transform(X_control)\n",
    "X_treated_std = scaler.transform(X_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_matching_att(X_treated, X_control, Y_treated, Y_control, K=1):\n",
    "    \"\"\"\n",
    "    Estimate ATT using K-nearest neighbor matching.\n",
    "    \"\"\"\n",
    "    nn = NearestNeighbors(n_neighbors=K, metric='euclidean')\n",
    "    nn.fit(X_control)\n",
    "    \n",
    "    # Find K nearest controls for each treated\n",
    "    distances, indices = nn.kneighbors(X_treated)\n",
    "    \n",
    "    # Counterfactual = average of K matched control outcomes\n",
    "    counterfactuals = Y_control[indices].mean(axis=1)\n",
    "    \n",
    "    # Individual treatment effects\n",
    "    effects = Y_treated - counterfactuals\n",
    "    att = effects.mean()\n",
    "    se = effects.std() / np.sqrt(len(effects))\n",
    "    \n",
    "    return {\n",
    "        'ATT': att,\n",
    "        'SE': se,\n",
    "        'mean_distance': distances.mean(),\n",
    "        'effects': effects,\n",
    "        'match_indices': indices\n",
    "    }\n",
    "\n",
    "# Compare different K values\n",
    "print(\"KNN MATCHING WITH DIFFERENT K:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for K in [1, 3, 5, 10]:\n",
    "    result = knn_matching_att(X_treated_std, X_control_std, Y_treated, Y_control, K=K)\n",
    "    print(f\"K={K:2d}: ATT = {result['ATT']:.2f} (SE = {result['SE']:.2f}), mean distance = {result['mean_distance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Numeric Demonstration\n",
    "\n",
    "### Bias-Corrected Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_corrected_matching(X_treated, X_control, Y_treated, Y_control, K=1):\n",
    "    \"\"\"\n",
    "    Abadie-Imbens bias-corrected matching estimator.\n",
    "    \n",
    "    Step 1: Find K nearest neighbors\n",
    "    Step 2: Estimate outcome model on control group\n",
    "    Step 3: Adjust counterfactual for covariate differences\n",
    "    \"\"\"\n",
    "    # Step 1: Find nearest neighbors\n",
    "    nn = NearestNeighbors(n_neighbors=K, metric='euclidean')\n",
    "    nn.fit(X_control)\n",
    "    distances, indices = nn.kneighbors(X_treated)\n",
    "    \n",
    "    # Step 2: Estimate outcome model on control group\n",
    "    outcome_model = LinearRegression()\n",
    "    outcome_model.fit(X_control, Y_control)\n",
    "    \n",
    "    # Step 3: Bias correction\n",
    "    corrected_effects = []\n",
    "    uncorrected_effects = []\n",
    "    \n",
    "    for i in range(len(X_treated)):\n",
    "        # Average outcome of K matched controls\n",
    "        matched_outcomes = Y_control[indices[i]].mean()\n",
    "        \n",
    "        # Uncorrected counterfactual\n",
    "        uncorrected = Y_treated[i] - matched_outcomes\n",
    "        uncorrected_effects.append(uncorrected)\n",
    "        \n",
    "        # Bias correction term\n",
    "        # For each match, compute predicted outcome difference\n",
    "        mu_treated = outcome_model.predict(X_treated[i:i+1])[0]\n",
    "        mu_matched = outcome_model.predict(X_control[indices[i]]).mean()\n",
    "        \n",
    "        # Bias-corrected counterfactual\n",
    "        correction = mu_treated - mu_matched\n",
    "        corrected_counterfactual = matched_outcomes + correction\n",
    "        corrected = Y_treated[i] - corrected_counterfactual\n",
    "        corrected_effects.append(corrected)\n",
    "    \n",
    "    uncorrected_effects = np.array(uncorrected_effects)\n",
    "    corrected_effects = np.array(corrected_effects)\n",
    "    \n",
    "    return {\n",
    "        'ATT_uncorrected': uncorrected_effects.mean(),\n",
    "        'SE_uncorrected': uncorrected_effects.std() / np.sqrt(len(uncorrected_effects)),\n",
    "        'ATT_corrected': corrected_effects.mean(),\n",
    "        'SE_corrected': corrected_effects.std() / np.sqrt(len(corrected_effects)),\n",
    "        'mean_correction': np.abs(corrected_effects - uncorrected_effects).mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare uncorrected vs bias-corrected\n",
    "print(\"BIAS CORRECTION COMPARISON:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Naive estimate\n",
    "naive = Y_treated.mean() - Y_control.mean()\n",
    "print(f\"\\nNaive (no matching): ATT = {naive:.2f}\")\n",
    "\n",
    "print(f\"\\n{'K':<5} {'Uncorrected ATT':<18} {'Corrected ATT':<18} {'Mean Correction'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for K in [1, 3, 5, 10]:\n",
    "    result = bias_corrected_matching(X_treated_std, X_control_std, Y_treated, Y_control, K=K)\n",
    "    print(f\"{K:<5} {result['ATT_uncorrected']:<18.2f} {result['ATT_corrected']:<18.2f} {result['mean_correction']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias correction\n",
    "fig, axes = create_tufte_figure(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Get detailed results for K=5\n",
    "K = 5\n",
    "nn = NearestNeighbors(n_neighbors=K, metric='euclidean')\n",
    "nn.fit(X_control_std)\n",
    "distances, indices = nn.kneighbors(X_treated_std)\n",
    "\n",
    "# For visualization, show a subset\n",
    "np.random.seed(42)\n",
    "viz_idx = np.random.choice(len(X_treated), 100, replace=False)\n",
    "\n",
    "# Left: Match distances histogram\n",
    "ax = axes[0]\n",
    "mean_match_distances = distances.mean(axis=1)\n",
    "ax.hist(mean_match_distances, bins=30, alpha=0.7, color=COLORS['blue'], edgecolor='white')\n",
    "ax.axvline(mean_match_distances.mean(), color=COLORS['red'], linestyle='--', linewidth=2, \n",
    "           label=f'Mean = {mean_match_distances.mean():.2f}')\n",
    "set_tufte_title(ax, f\"Match Quality: Distance to K={K} Nearest Neighbors\")\n",
    "set_tufte_labels(ax, \"Mean Distance to K Neighbors\", \"Count\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Right: Correction magnitude\n",
    "ax = axes[1]\n",
    "outcome_model = LinearRegression()\n",
    "outcome_model.fit(X_control_std, Y_control)\n",
    "\n",
    "corrections = []\n",
    "for i in viz_idx:\n",
    "    mu_treated = outcome_model.predict(X_treated_std[i:i+1])[0]\n",
    "    mu_matched = outcome_model.predict(X_control_std[indices[i]]).mean()\n",
    "    corrections.append(mu_treated - mu_matched)\n",
    "corrections = np.array(corrections)\n",
    "\n",
    "ax.hist(corrections, bins=30, alpha=0.7, color=COLORS['green'], edgecolor='white')\n",
    "ax.axvline(corrections.mean(), color=COLORS['red'], linestyle='--', linewidth=2,\n",
    "           label=f'Mean = {corrections.mean():.2f}')\n",
    "ax.axvline(0, color='gray', linestyle='-', linewidth=1)\n",
    "set_tufte_title(ax, \"Bias Correction Term Distribution\")\n",
    "set_tufte_labels(ax, \"Correction (predicted difference)\", \"Count\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean correction magnitude: {np.abs(corrections).mean():.3f}\")\n",
    "print(\"Positive correction = treated unit predicted higher outcome than matched controls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### sklearn KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn's KNeighborsRegressor for matching\n",
    "def sklearn_knn_matching(X_treated, X_control, Y_treated, Y_control, K=1):\n",
    "    \"\"\"\n",
    "    Use sklearn KNeighborsRegressor for matching.\n",
    "    This directly predicts counterfactual outcomes.\n",
    "    \"\"\"\n",
    "    knn = KNeighborsRegressor(n_neighbors=K, weights='uniform')\n",
    "    knn.fit(X_control, Y_control)\n",
    "    \n",
    "    # Predict counterfactuals for treated units\n",
    "    counterfactuals = knn.predict(X_treated)\n",
    "    \n",
    "    # ATT\n",
    "    effects = Y_treated - counterfactuals\n",
    "    att = effects.mean()\n",
    "    se = effects.std() / np.sqrt(len(effects))\n",
    "    \n",
    "    return {'ATT': att, 'SE': se}\n",
    "\n",
    "# Compare with our implementation\n",
    "print(\"SKLEARN VS MANUAL KNN MATCHING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for K in [1, 5, 10]:\n",
    "    sklearn_result = sklearn_knn_matching(X_treated_std, X_control_std, Y_treated, Y_control, K)\n",
    "    manual_result = knn_matching_att(X_treated_std, X_control_std, Y_treated, Y_control, K)\n",
    "    \n",
    "    print(f\"K={K:2d}: sklearn ATT = {sklearn_result['ATT']:.4f}, manual ATT = {manual_result['ATT']:.4f}\")\n",
    "\n",
    "print(\"\\nBoth implementations give identical results (as expected).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary comparing all methods\n",
    "print(\"\\nFINAL COMPARISON:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<35} {'ATT':<12} {'SE':<12} {'95% CI'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Naive\n",
    "naive_att = Y_treated.mean() - Y_control.mean()\n",
    "naive_se = np.sqrt(Y_treated.var()/len(Y_treated) + Y_control.var()/len(Y_control))\n",
    "print(f\"{'Naive (no matching)':<35} {naive_att:<12.2f} {naive_se:<12.2f} [{naive_att-1.96*naive_se:.2f}, {naive_att+1.96*naive_se:.2f}]\")\n",
    "\n",
    "# KNN K=1\n",
    "r1 = knn_matching_att(X_treated_std, X_control_std, Y_treated, Y_control, K=1)\n",
    "print(f\"{'KNN (K=1)':<35} {r1['ATT']:<12.2f} {r1['SE']:<12.2f} [{r1['ATT']-1.96*r1['SE']:.2f}, {r1['ATT']+1.96*r1['SE']:.2f}]\")\n",
    "\n",
    "# KNN K=5\n",
    "r5 = knn_matching_att(X_treated_std, X_control_std, Y_treated, Y_control, K=5)\n",
    "print(f\"{'KNN (K=5)':<35} {r5['ATT']:<12.2f} {r5['SE']:<12.2f} [{r5['ATT']-1.96*r5['SE']:.2f}, {r5['ATT']+1.96*r5['SE']:.2f}]\")\n",
    "\n",
    "# Bias-corrected K=5\n",
    "bc5 = bias_corrected_matching(X_treated_std, X_control_std, Y_treated, Y_control, K=5)\n",
    "print(f\"{'Bias-corrected KNN (K=5)':<35} {bc5['ATT_corrected']:<12.2f} {bc5['SE_corrected']:<12.2f} [{bc5['ATT_corrected']-1.96*bc5['SE_corrected']:.2f}, {bc5['ATT_corrected']+1.96*bc5['SE_corrected']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1: What is matching bias and how do you correct it?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Matching Bias** occurs because nearest-neighbor matches are imperfect:\n",
    "\n",
    "$$\\text{Bias}_i = E[Y(0) | X_{matched}] - E[Y(0) | X_{treated}]$$\n",
    "\n",
    "Even the \"nearest\" control unit has different covariate values, leading to systematic differences in expected outcomes.\n",
    "\n",
    "**Abadie-Imbens Bias Correction**:\n",
    "\n",
    "1. Estimate outcome model on controls: $\\hat{\\mu}_0(x)$\n",
    "2. Compute correction term: $\\hat{\\mu}_0(X_i) - \\hat{\\mu}_0(X_{j(i)})$\n",
    "3. Adjust counterfactual: $\\hat{Y}_i(0)^{BC} = Y_{j(i)} + \\text{correction}$\n",
    "\n",
    "**Intuition**: The correction estimates how much the outcome would differ if the matched control had the same covariates as the treated unit.\n",
    "\n",
    "**When to use**:\n",
    "- Always with approximate matching\n",
    "- Especially important with many covariates (curse of dimensionality)\n",
    "- Less necessary with very close matches\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q2: What is the curse of dimensionality in matching?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Definition**: As the number of covariates increases, the \"nearest\" neighbor becomes farther away.\n",
    "\n",
    "**Why it happens**:\n",
    "1. Volume of space grows exponentially with dimensions\n",
    "2. Data points become increasingly sparse\n",
    "3. All points become approximately equidistant\n",
    "\n",
    "**Consequences for matching**:\n",
    "- Poor match quality (large covariate differences)\n",
    "- Increased matching bias\n",
    "- Estimates become unreliable\n",
    "\n",
    "**Solutions**:\n",
    "1. **Propensity score**: Match on 1D score instead of full X\n",
    "2. **Feature selection**: Use only the most important confounders\n",
    "3. **Bias correction**: Abadie-Imbens adjustment\n",
    "4. **Coarsening**: Bin continuous variables (Coarsened Exact Matching)\n",
    "5. **Regularization**: Use ML methods that handle high dimensions\n",
    "\n",
    "**Rule of thumb**: Be cautious with >10 continuous covariates in standard matching.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q3: How do you choose K in KNN matching?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Trade-off**:\n",
    "- **K=1**: Lowest bias (closest match), highest variance\n",
    "- **Large K**: Lower variance (averaged), higher bias (worse matches)\n",
    "\n",
    "**Practical guidance**:\n",
    "\n",
    "1. **Start with K=1** for unbiased estimates if matches are close\n",
    "2. **Increase K (3-5)** if:\n",
    "   - Variance is high\n",
    "   - Many similar controls available\n",
    "   - Bias correction is used\n",
    "\n",
    "3. **Use cross-validation** to select K:\n",
    "   - Predict outcomes using different K\n",
    "   - Choose K with best predictive accuracy\n",
    "\n",
    "4. **Check sensitivity**:\n",
    "   - Run analysis with K=1, 3, 5, 10\n",
    "   - Results should be stable across K\n",
    "\n",
    "**With bias correction**: K matters less because bias is explicitly adjusted.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[^1]: Facure, M. (2022). *Causal Inference for the Brave and True*, Chapter 10.\n",
    "\n",
    "[^2]: Abadie, A., & Imbens, G. W. (2006). Large sample properties of matching estimators for average treatment effects. *Econometrica*, 74(1), 235-267.\n",
    "\n",
    "[^3]: Abadie, A., & Imbens, G. W. (2011). Bias-corrected matching estimators for average treatment effects. *Journal of Business & Economic Statistics*, 29(1), 1-11.\n",
    "\n",
    "[^4]: Cross-reference: Propensity score as solution to curse of dimensionality in `11_propensity_score/01_balancing_score.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
