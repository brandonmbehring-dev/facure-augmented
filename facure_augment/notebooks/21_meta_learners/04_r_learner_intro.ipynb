{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# 21.4 R-Learner Introduction\n\n**Chapter**: 21 - Meta-Learners  \n**Section**: 4 - R-Learner and Bridge to Double ML  \n**Facure Source**: 21-Meta-Learners.ipynb, 22-Debiased-Orthogonal-Machine-Learning.ipynb  \n**Version**: 1.0.0  \n**Last Validated**: 2026-01-16\n\n---\n\n## Table of Contents\n\n1. [Limitations of S/T/X Learners](#1-limitations-of-stx-learners)\n2. [The Robinson Transformation](#2-the-robinson-transformation)\n3. [R-Learner: Residual-on-Residual](#3-r-learner-residual-on-residual)\n4. [Connection to Double ML](#4-connection-to-double-ml)\n5. [Interview Appendix](#5-interview-appendix)\n6. [References](#6-references)\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "---\n\n## 1. Facure's Intuition: Limitations of S/T/X Learners\n\n> **Recall from previous notebooks**:\n> - S-learner: Regularization bias (shrinks toward zero)\n> - T-learner: Sample imbalance creates spurious heterogeneity\n> - X-learner: Better, but still susceptible to model misspecification\n\n**Common thread**: All learners estimate $E[Y|X,T]$ directly, which couples:\n- The outcome model (nuisance)\n- The treatment effect (target)\n\n**The R-Learner takes a different approach**: Use **residuals** to separate the causal signal from confounding."
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "---\n\n## 2. Formal Treatment: The Robinson Transformation\n\nThe R-learner is based on **Robinson's (1988) partial linear model**:\n\n$$Y = \\tau(X) \\cdot T + g(X) + \\varepsilon$$\n\nwhere:\n- $\\tau(X)$: The CATE we want to estimate\n- $g(X)$: Direct effect of confounders on outcome (nuisance)\n- $\\varepsilon$: Noise\n\nAnd treatment follows:\n$$T = m(X) + v$$\n\nwhere $m(X) = E[T|X]$ (propensity).\n\n### The Key Insight\n\nTake expectations conditional on X:\n$$E[Y|X] = \\tau(X) \\cdot E[T|X] + g(X) = \\tau(X) \\cdot m(X) + g(X)$$\n\nSubtract from the original equation:\n$$Y - E[Y|X] = \\tau(X) \\cdot (T - E[T|X]) + \\varepsilon$$\n\nOr more compactly:\n$$\\tilde{Y} = \\tau(X) \\cdot \\tilde{T} + \\varepsilon$$\n\n**The nuisance function $g(X)$ cancels out!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with known CATE function\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 2000\n",
    "X = np.random.uniform(0, 1, (n, 2))\n",
    "\n",
    "# Propensity: m(X) = P(T=1|X) depends on X\n",
    "propensity = 0.3 + 0.4 * X[:, 0]  # Linear in X0\n",
    "T = np.random.binomial(1, propensity)\n",
    "\n",
    "# True CATE: tau(X) = 1 + 2*X0 (heterogeneous!)\n",
    "true_tau = 1 + 2 * X[:, 0]\n",
    "\n",
    "# Outcome: Y = tau(X)*T + g(X) + noise\n",
    "# where g(X) = sin(2*pi*X0) + X1^2 (complex nuisance)\n",
    "g_x = np.sin(2 * np.pi * X[:, 0]) + X[:, 1]**2\n",
    "Y = true_tau * T + g_x + np.random.normal(0, 0.3, n)\n",
    "\n",
    "print(f\"Generated {n} samples\")\n",
    "print(f\"True CATE range: [{true_tau.min():.2f}, {true_tau.max():.2f}]\")\n",
    "print(f\"Treatment rate: {T.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "---\n\n## 3. Numeric Demonstration: R-Learner\n\nThe R-learner (Nie & Wager, 2021) operationalizes Robinson's insight:\n\n### Algorithm\n\n1. **Estimate nuisance functions** (using cross-fitting):\n   - $\\hat{m}(X) \\approx E[T|X]$ (propensity)\n   - $\\hat{\\ell}(X) \\approx E[Y|X]$ (reduced form outcome)\n\n2. **Compute residuals**:\n   - $\\tilde{T}_i = T_i - \\hat{m}(X_i)$\n   - $\\tilde{Y}_i = Y_i - \\hat{\\ell}(X_i)$\n\n3. **Estimate CATE by minimizing R-loss**:\n   $$\\hat{\\tau} = \\arg\\min_\\tau \\sum_i \\tilde{T}_i^2 \\left( \\frac{\\tilde{Y}_i}{\\tilde{T}_i} - \\tau(X_i) \\right)^2$$\n\nThis is weighted least squares of $\\tilde{Y}/\\tilde{T}$ on X, with weights $\\tilde{T}^2$.\n\n```\nR-Learner Architecture \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n       [Y, T, X]\n          \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502           \u2502\n    \u25bc           \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502\u2113\u0302(X)\u2502    \u2502m\u0302(X) \u2502   \u2190 Nuisance models (cross-fitted)\n  \u2514\u2500\u2500\u252c\u2500\u2500\u2518    \u2514\u2500\u2500\u252c\u2500\u2500\u2518\n     \u2502          \u2502\n     \u25bc          \u25bc\n   \u1ef8 = Y-\u2113\u0302   T\u0303 = T-m\u0302  \u2190 Residuals\n     \u2502          \u2502\n     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Minimize  \u2502\n    \u2502 R-Loss    \u2502   \u2190 Weighted regression of \u1ef8/T\u0303 on X\n    \u2502 for \u03c4(X)  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502\n          \u25bc\n       \u03c4\u0302(X)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Estimate nuisance functions with cross-fitting\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Propensity model m(X) = E[T|X]\n",
    "m_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "m_hat = cross_val_predict(m_model, X, T, cv=5, method='predict_proba')[:, 1]\n",
    "\n",
    "# Outcome model \u2113(X) = E[Y|X]\n",
    "l_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "l_hat = cross_val_predict(l_model, X, Y, cv=5)\n",
    "\n",
    "print(\"Nuisance models estimated (cross-fitted)\")\n",
    "print(f\"Propensity R\u00b2: {1 - np.var(T - m_hat) / np.var(T):.3f}\")\n",
    "print(f\"Outcome R\u00b2:    {1 - np.var(Y - l_hat) / np.var(Y):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute residuals\n",
    "T_tilde = T - m_hat  # Treatment residual\n",
    "Y_tilde = Y - l_hat  # Outcome residual\n",
    "\n",
    "print(f\"Treatment residual: mean={T_tilde.mean():.4f} (should be ~0)\")\n",
    "print(f\"Outcome residual:   mean={Y_tilde.mean():.4f} (should be ~0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Minimize R-loss\n",
    "# For simplicity, we'll use weighted regression with a flexible model\n",
    "\n",
    "# Weights = T_tilde^2\n",
    "weights = T_tilde**2\n",
    "\n",
    "# Target = Y_tilde / T_tilde (pseudo-outcome)\n",
    "# Need to handle T_tilde near zero - use regularization\n",
    "eps = 0.01\n",
    "pseudo_outcome = Y_tilde / (T_tilde + eps * np.sign(T_tilde))\n",
    "\n",
    "# Fit weighted regression\n",
    "tau_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "tau_model.fit(X, pseudo_outcome, sample_weight=weights)\n",
    "\n",
    "# Predict CATE\n",
    "cate_r = tau_model.predict(X)\n",
    "\n",
    "print(f\"R-Learner CATE: mean={cate_r.mean():.3f}\")\n",
    "print(f\"True CATE:      mean={true_tau.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare R-learner to other learners\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# S-learner\n",
    "s_model = LGBMRegressor(max_depth=5, n_estimators=100, verbose=-1)\n",
    "df = pd.DataFrame(X, columns=['X0', 'X1'])\n",
    "df['T'] = T\n",
    "s_model.fit(df, Y)\n",
    "df_t1 = df.copy()\n",
    "df_t1['T'] = 1\n",
    "df_t0 = df.copy()\n",
    "df_t0['T'] = 0\n",
    "cate_s = s_model.predict(df_t1) - s_model.predict(df_t0)\n",
    "\n",
    "# T-learner\n",
    "m0 = LGBMRegressor(max_depth=5, n_estimators=100, verbose=-1)\n",
    "m1 = LGBMRegressor(max_depth=5, n_estimators=100, verbose=-1)\n",
    "m0.fit(X[T==0], Y[T==0])\n",
    "m1.fit(X[T==1], Y[T==1])\n",
    "cate_t = m1.predict(X) - m0.predict(X)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_s = np.sqrt(np.mean((cate_s - true_tau)**2))\n",
    "rmse_t = np.sqrt(np.mean((cate_t - true_tau)**2))\n",
    "rmse_r = np.sqrt(np.mean((cate_r - true_tau)**2))\n",
    "\n",
    "print(\"RMSE Comparison:\")\n",
    "print(f\"  S-Learner: {rmse_s:.3f}\")\n",
    "print(f\"  T-Learner: {rmse_t:.3f}\")\n",
    "print(f\"  R-Learner: {rmse_r:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, (cate, name, color) in zip(axes, [\n",
    "    (cate_s, 'S-Learner', COLORS['blue']),\n",
    "    (cate_t, 'T-Learner', COLORS['green']),\n",
    "    (cate_r, 'R-Learner', COLORS['purple']),\n",
    "]):\n",
    "    ax.scatter(true_tau, cate, alpha=0.3, s=15, c=color)\n",
    "    ax.plot([1, 3], [1, 3], 'k--', lw=1.5)\n",
    "    ax.set_xlabel('True CATE')\n",
    "    ax.set_ylabel('Predicted CATE')\n",
    "    rmse = np.sqrt(np.mean((cate - true_tau)**2))\n",
    "    ax.set_title(f'{name}\\nRMSE={rmse:.3f}', fontweight='bold')\n",
    "    ax.set_xlim(0.8, 3.2)\n",
    "    ax.set_ylim(0.8, 3.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "---\n\n## 4. Implementation: Connection to Double ML\n\nThe R-learner is closely related to **Double Machine Learning (DML)**:\n\n```\nR-Learner \u2194 DML Connection \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nDML for ATE:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1. Cross-fit nuisance: \u2113\u0302(X), m\u0302(X)\n2. Compute residuals: \u1ef8 = Y - \u2113\u0302(X), T\u0303 = T - m\u0302(X)\n3. Regress \u1ef8 on T\u0303 to get \u03c4\u0302 (a single number)\n\nR-Learner for CATE:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1. Cross-fit nuisance: \u2113\u0302(X), m\u0302(X)  [Same!]\n2. Compute residuals: \u1ef8, T\u0303           [Same!]\n3. Learn \u03c4(X) to minimize R-loss     [CATE instead of ATE]\n\nKey insight:\n- DML estimates CONSTANT effect \u03c4\n- R-learner estimates FUNCTION \u03c4(X)\n- Same orthogonalization principle!\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```\n\n### Why Orthogonalization Helps\n\nThe residualization step creates **Neyman orthogonality**:\n- First-order errors in nuisance estimation have zero first-order effect on $\\hat{\\tau}$\n- Enables use of ML methods (which are biased) for nuisance\n- See Chapter 22 (Debiased ML) and DML Bridge notebooks for full treatment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DML estimate of ATE (for comparison)\n",
    "# This is the constant-effect version of what R-learner does for heterogeneous effects\n",
    "\n",
    "tau_dml = np.sum(Y_tilde * T_tilde) / np.sum(T_tilde**2)\n",
    "\n",
    "print(f\"DML ATE estimate: {tau_dml:.3f}\")\n",
    "print(f\"True mean CATE:   {true_tau.mean():.3f}\")\n",
    "print(f\"\\nR-learner estimates CATE(x), DML estimates mean(CATE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "### R-Learner Advantages\n\n| Aspect | S/T/X Learners | R-Learner |\n|--------|----------------|------------|\n| Handles continuous T | S only | Yes |\n| Robust to nuisance error | No | Yes (orthogonal) |\n| Sample splitting | Not required | Cross-fitting recommended |\n| Complexity | Lower | Higher |\n| When to use | Simple cases | Complex confounding |\n\n```\nMeta-Learner Evolution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nS-Learner \u2192 T-Learner \u2192 X-Learner \u2192 R-Learner\n                                         \u2502\n                                         \u2193\n                              Double Machine Learning\n                              (Chapter 22)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "---\n\n## 5. Interview Appendix\n\n### Practice Questions\n\n**Q1 (Two Sigma, Quant)**: *\"What is the R-learner and how does it relate to Double ML?\"*\n\n<details>\n<summary>Solution</summary>\n\n**R-Learner**:\n\nBased on Robinson (1988) partial linear model: $Y = \\tau(X) \\cdot T + g(X) + \\varepsilon$\n\n**Algorithm**:\n1. Estimate nuisance: $\\hat{\\ell}(X) \\approx E[Y|X]$, $\\hat{m}(X) \\approx E[T|X]$\n2. Compute residuals: $\\tilde{Y} = Y - \\hat{\\ell}(X)$, $\\tilde{T} = T - \\hat{m}(X)$\n3. Minimize R-loss: $\\sum \\tilde{T}^2 (\\tilde{Y}/\\tilde{T} - \\tau(X))^2$\n\n**Relation to DML**:\n\n- DML: Same residualization, but estimates constant $\\tau$ (ATE)\n- R-learner: Same principle, but learns $\\tau(X)$ (CATE function)\n- Both use **Neyman orthogonality**: Nuisance errors have second-order effect\n\n**Key equation**: $\\tilde{Y} = \\tau(X) \\cdot \\tilde{T} + \\varepsilon$\n\nThe nuisance $g(X)$ cancels when we subtract conditional expectations!\n\n</details>\n\n---\n\n**Q2 (Meta IC6)**: *\"Why does the R-learner handle continuous treatments while T/X learners don't?\"*\n\n<details>\n<summary>Solution</summary>\n\n**T/X learners require discrete T** because they:\n- Split data by treatment level: $D_0 = \\{i: T_i = 0\\}$, $D_1 = \\{i: T_i = 1\\}$\n- Fit separate models: $\\hat{\\mu}_0$, $\\hat{\\mu}_1$\n- With continuous T, there's no natural split!\n\n**R-learner works with continuous T** because:\n- No splitting by T value\n- Uses residualization: $\\tilde{T} = T - E[T|X]$\n- The partial linear model $Y = \\tau(X) \\cdot T + g(X) + \\varepsilon$ works for any T\n- R-loss minimization handles continuous $\\tilde{T}$ naturally\n\n**S-learner also handles continuous T** because it doesn't split by T either \u2014 it includes T as a feature.\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "---\n\n## 6. References\n\n[^1]: Nie, X. and Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. *Biometrika*, 108(2), 299-319.\n\n[^2]: Robinson, P. M. (1988). Root-N-Consistent Semiparametric Regression. *Econometrica*, 56(4), 931-954.\n\n[^3]: Chernozhukov, V. et al. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*.\n\n[^4]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapters 21-22.\n\n---\n\n**Next**: [Chapter 22: Debiased ML](../22_debiased_ml/01_dml_intuition.ipynb) \u2014 Full DML theory and implementation"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}