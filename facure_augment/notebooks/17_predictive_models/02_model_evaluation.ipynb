{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models 101: Model Evaluation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Chapter 17 | Notebook 2 of 3**\n",
    "\n",
    "This notebook covers cross-validation, model complexity, and evaluation metrics\n",
    "using gradient boosting for customer profitability prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from facure_augment.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### The Cross-Validation Imperative\n",
    "\n",
    "ML models can be made arbitrarily powerful. A sufficiently complex model will\n",
    "**perfectly fit** the training data. But this is dangerous:\n",
    "\n",
    "1. Perfect fit on training ≠ good predictions on new data\n",
    "2. Complex models learn **noise** in addition to **signal**\n",
    "3. This is called **overfitting**\n",
    "\n",
    "**Solution**: Cross-validation - hide part of the data during training,\n",
    "then evaluate on the hidden portion.\n",
    "\n",
    "> \"In God we trust; all others must bring data.\" - W. Edwards Deming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity Spectrum\n",
    "\n",
    "| Model | Complexity | Risk |\n",
    "|-------|-----------|------|\n",
    "| Mean | None | High bias (underfitting) |\n",
    "| Linear | Low | Moderate bias |\n",
    "| Shallow tree | Medium | Balanced |\n",
    "| Deep forest | High | High variance (overfitting) |\n",
    "\n",
    "The goal is to find the **sweet spot** - complex enough to capture patterns,\n",
    "simple enough to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Cross-Validation\n",
    "\n",
    "**Train-Test Split**:\n",
    "1. Partition data: $D = D_{train} \\cup D_{test}$\n",
    "2. Train model on $D_{train}$: $\\hat{f} = \\text{Train}(D_{train})$\n",
    "3. Evaluate on $D_{test}$: $\\text{Error} = \\text{Loss}(D_{test}, \\hat{f})$\n",
    "\n",
    "**K-Fold Cross-Validation**:\n",
    "1. Split data into K folds: $D = D_1 \\cup D_2 \\cup ... \\cup D_K$\n",
    "2. For each fold $k$:\n",
    "   - Train on $D \\setminus D_k$\n",
    "   - Evaluate on $D_k$\n",
    "3. Average errors across folds\n",
    "\n",
    "K-fold provides more robust error estimates than single train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation Metrics for Regression\n",
    "\n",
    "**R-squared** ($R^2$):\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2} = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "Interpretation:\n",
    "- $R^2 = 1$: Perfect prediction\n",
    "- $R^2 = 0$: Predicting the mean\n",
    "- $R^2 < 0$: Worse than predicting the mean\n",
    "\n",
    "**Mean Squared Error** (MSE):\n",
    "$$MSE = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Root MSE** (RMSE): $\\sqrt{MSE}$ - same units as outcome\n",
    "\n",
    "**Mean Absolute Error** (MAE):\n",
    "$$MAE = \\frac{1}{n} \\sum_i |y_i - \\hat{y}_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient Boosting\n",
    "\n",
    "Gradient boosting builds an **ensemble** of weak learners (usually trees) sequentially:\n",
    "\n",
    "$$\\hat{f}(x) = \\sum_{m=1}^{M} \\gamma_m h_m(x)$$\n",
    "\n",
    "At each step $m$:\n",
    "1. Compute residuals: $r_i = y_i - \\hat{f}_{m-1}(x_i)$\n",
    "2. Fit a new tree $h_m$ to residuals\n",
    "3. Update: $\\hat{f}_m = \\hat{f}_{m-1} + \\gamma_m h_m$\n",
    "\n",
    "**Key hyperparameters**:\n",
    "- `n_estimators`: Number of trees (M)\n",
    "- `max_depth`: Tree depth (complexity)\n",
    "- `learning_rate`: Step size ($\\gamma$)\n",
    "- `min_samples_split`: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "transactions = load_facure_data(\"customer_transactions.csv\")\n",
    "profitable = transactions[['customer_id']].assign(\n",
    "    net_value=transactions.drop(columns='customer_id').sum(axis=1)\n",
    ")\n",
    "customer_features = load_facure_data(\"customer_features.csv\").merge(\n",
    "    profitable, on='customer_id'\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(customer_features, test_size=0.3, random_state=13)\n",
    "\n",
    "print(f\"Training: {len(train)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: encode region with target mean\n",
    "region_means = train.groupby('region')['net_value'].mean().to_dict()\n",
    "\n",
    "def encode_region(df, region_means):\n",
    "    \"\"\"Replace region with its average net_value from training set.\"\"\"\n",
    "    return df.assign(region_encoded=df['region'].map(region_means).fillna(0))\n",
    "\n",
    "train_enc = encode_region(train, region_means)\n",
    "test_enc = encode_region(test, region_means)\n",
    "\n",
    "features = ['region_encoded', 'income', 'age']\n",
    "target = 'net_value'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model complexities\n",
    "depth_range = [1, 2, 4, 8, 12, 16]\n",
    "results = []\n",
    "\n",
    "for depth in depth_range:\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=depth,\n",
    "        learning_rate=0.1,\n",
    "        min_samples_split=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(train_enc[features], train_enc[target])\n",
    "    \n",
    "    train_r2 = r2_score(train_enc[target], model.predict(train_enc[features]))\n",
    "    test_r2 = r2_score(test_enc[target], model.predict(test_enc[features]))\n",
    "    \n",
    "    results.append({\n",
    "        'max_depth': depth,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'gap': train_r2 - test_r2\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: R² by depth\n",
    "ax = axes[0]\n",
    "ax.plot(results_df['max_depth'], results_df['train_r2'], 'o-', \n",
    "        color=COLORS['blue'], label='Train R²', linewidth=2)\n",
    "ax.plot(results_df['max_depth'], results_df['test_r2'], 's-', \n",
    "        color=COLORS['red'], label='Test R²', linewidth=2)\n",
    "ax.fill_between(results_df['max_depth'], results_df['test_r2'], results_df['train_r2'],\n",
    "                alpha=0.2, color='gray', label='Overfitting gap')\n",
    "ax.set_xlabel('Max Depth')\n",
    "ax.set_ylabel('R²')\n",
    "ax.set_title('Model Complexity vs Performance')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Right: Overfitting gap\n",
    "ax = axes[1]\n",
    "ax.bar(results_df['max_depth'].astype(str), results_df['gap'], \n",
    "       color=COLORS['orange'], alpha=0.7)\n",
    "ax.set_xlabel('Max Depth')\n",
    "ax.set_ylabel('Train R² - Test R²')\n",
    "ax.set_title('Overfitting Gap by Complexity')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best depth\n",
    "best_idx = results_df['test_r2'].idxmax()\n",
    "print(f\"\\nBest max_depth: {results_df.loc[best_idx, 'max_depth']}\")\n",
    "print(f\"Test R²: {results_df.loc[best_idx, 'test_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with tuned parameters\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=4,\n",
    "    min_samples_split=10,\n",
    "    learning_rate=0.01,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "model.fit(train_enc[features], train_enc[target])\n",
    "\n",
    "# Evaluate\n",
    "train_pred = model.predict(train_enc[features])\n",
    "test_pred = model.predict(test_enc[features])\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"  Train R²: {r2_score(train_enc[target], train_pred):.4f}\")\n",
    "print(f\"  Test R²:  {r2_score(test_enc[target], test_pred):.4f}\")\n",
    "print(f\"  Train RMSE: {np.sqrt(mean_squared_error(train_enc[target], train_pred)):.2f}\")\n",
    "print(f\"  Test RMSE:  {np.sqrt(mean_squared_error(test_enc[target], test_pred)):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "ax = axes[0]\n",
    "ax.scatter(test_enc[target], test_pred, alpha=0.3, s=10, color=COLORS['blue'])\n",
    "ax.plot([-200, 300], [-200, 300], 'r--', label='Perfect prediction')\n",
    "ax.set_xlabel('Actual Net Value')\n",
    "ax.set_ylabel('Predicted Net Value')\n",
    "ax.set_title('Prediction vs Actual (Test Set)')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Residual distribution\n",
    "ax = axes[1]\n",
    "residuals = test_enc[target].values - test_pred\n",
    "ax.hist(residuals, bins=50, color=COLORS['green'], alpha=0.7, edgecolor='black')\n",
    "ax.axvline(0, color='red', linestyle='--')\n",
    "ax.set_xlabel('Residual (Actual - Predicted)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Residual Distribution (MAE={np.mean(np.abs(residuals)):.2f})')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.barh(importance['feature'], importance['importance'], color=COLORS['blue'], alpha=0.7)\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Gradient Boosting Feature Importance')\n",
    "apply_tufte_style(ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation on training set\n",
    "cv_scores = cross_val_score(\n",
    "    GradientBoostingRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=4,\n",
    "        min_samples_split=10,\n",
    "        learning_rate=0.01,\n",
    "        random_state=123\n",
    "    ),\n",
    "    train_enc[features],\n",
    "    train_enc[target],\n",
    "    cv=5,\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "print(f\"5-Fold Cross-Validation R²:\")\n",
    "print(f\"  Scores: {cv_scores}\")\n",
    "print(f\"  Mean: {cv_scores.mean():.4f}\")\n",
    "print(f\"  Std: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "Cross-validation R² is typically lower than test set R² because:\n",
    "1. Each fold uses less training data\n",
    "2. It provides more robust error estimates\n",
    "3. Multiple runs reduce luck factor\n",
    "\n",
    "Use CV for model selection, final test set for reporting.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: What is overfitting and how do you detect it?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Overfitting**: Model learns noise in training data, performs poorly on new data.\n",
    "\n",
    "**Detection**:\n",
    "- Train performance >> Test performance (large gap)\n",
    "- Performance degrades as model complexity increases\n",
    "- Very high training accuracy (approaching 100%)\n",
    "\n",
    "**Prevention**:\n",
    "- Regularization (L1/L2 penalties, dropout)\n",
    "- Early stopping\n",
    "- Reduce model complexity\n",
    "- More training data\n",
    "- Cross-validation for hyperparameter tuning\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Explain R² and when it can be negative.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**R² formula**: $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "**Interpretation**:\n",
    "- $R^2 = 1$: Perfect prediction (residuals = 0)\n",
    "- $R^2 = 0$: Model predicts the mean\n",
    "- $R^2 < 0$: Model worse than predicting the mean\n",
    "\n",
    "**When R² is negative**:\n",
    "- Model is very wrong (predictions far from actual)\n",
    "- Usually indicates severe overfitting or wrong model\n",
    "- Often seen when evaluating on test set a model that overfit training\n",
    "\n",
    "**Note**: R² computed on training set is always ≥ 0 (intercept term).\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: How does gradient boosting differ from random forest?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "| Aspect | Gradient Boosting | Random Forest |\n",
    "|--------|------------------|---------------|\n",
    "| Training | Sequential (trees on residuals) | Parallel (independent trees) |\n",
    "| Bias-Variance | Reduces bias | Reduces variance |\n",
    "| Overfitting | More prone | More robust |\n",
    "| Speed | Slower (sequential) | Faster (parallelizable) |\n",
    "| Tuning | More sensitive | Easier to tune |\n",
    "| Performance | Often higher ceiling | More reliable baseline |\n",
    "\n",
    "**When to use which**:\n",
    "- GB: Maximum performance, willing to tune carefully\n",
    "- RF: Quick baseline, robust out-of-box, parallel compute\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Friedman, J. H. (2001). Greedy Function Approximation: A Gradient Boosting Machine.\n",
    "      *Annals of Statistics*, 29(5), 1189-1232.\n",
    "\n",
    "[^2]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, Chapter 17."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
