{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Prediction Metrics Fail for Causal Models\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Appendix A5 | Notebook 1 of 2**\n",
    "\n",
    "This notebook demonstrates why standard prediction metrics like R²\n",
    "are dangerous for evaluating causal models, even on randomized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from augmented.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### The Misconception\n",
    "\n",
    "**Common belief**: \"With randomized data, we can evaluate a causal model\n",
    "by its predictive performance (R²) on the test set.\"\n",
    "\n",
    "**Reality**: This is **wrong** and can lead to terrible causal model selection!\n",
    "\n",
    "### Why R² Can Mislead\n",
    "\n",
    "**DGP decomposition**:\n",
    "$$Y = g(X) + f(T, X) + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $g(X)$ = effect of covariates on outcome (no treatment)\n",
    "- $f(T, X)$ = treatment effect (may vary with X)\n",
    "\n",
    "**Problem**: If $|g(X)| >> |f(T, X)|$, a model can achieve high R² \n",
    "by predicting $g(X)$ while completely ignoring $f(T, X)$!\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "| Model | Predicts | R² | Causal Quality |\n",
    "|-------|----------|----|-----------------|\n",
    "| M1 | g(X) well, f(T,X) poorly | 0.92 | ❌ Bad |\n",
    "| M2 | g(X) poorly, f(T,X) well | 0.08 | ✅ Good |\n",
    "\n",
    "**Paradox**: The better predictor (M1) is a worse causal model!\n",
    "\n",
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "Prediction answers: \"What is Y?\"\n",
    "Causal inference answers: \"How does Y change with T?\"\n",
    "\n",
    "These are fundamentally different questions.\n",
    "High R² means nothing for the second question.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Additive Outcome Model\n",
    "\n",
    "General decomposition:\n",
    "$$Y_i = g(X_i) + f(T_i, W_i) + \\epsilon_i$$\n",
    "\n",
    "where:\n",
    "- $X$ = covariates affecting $Y$ directly (not through $T$)\n",
    "- $W$ = covariates affecting treatment heterogeneity\n",
    "- $T$ = treatment (randomized)\n",
    "- $g(\\cdot)$ = \"nuisance\" function\n",
    "- $f(\\cdot, \\cdot)$ = causal function of interest\n",
    "\n",
    "### 2.2 Prediction Variance Decomposition\n",
    "\n",
    "Total outcome variance:\n",
    "$$\\text{Var}(Y) = \\text{Var}(g(X)) + \\text{Var}(f(T, W)) + \\text{Var}(\\epsilon) + \\text{Cross-terms}$$\n",
    "\n",
    "If $\\text{Var}(g(X)) >> \\text{Var}(f(T, W))$, then:\n",
    "- High R² ≈ predicting $g(X)$ well\n",
    "- Causal component $f(T, W)$ contributes little to R²\n",
    "\n",
    "### 2.3 R² for Causal Models\n",
    "\n",
    "**Problem formalization**:\n",
    "\n",
    "Let $\\hat{M}$ be an estimator that outputs $\\hat{Y} = \\hat{M}(T, X, W)$.\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_i (Y_i - \\hat{M}(T_i, X_i, W_i))^2}{\\sum_i (Y_i - \\bar{Y})^2}$$\n",
    "\n",
    "**Key insight**: R² rewards predicting $Y$ accurately, but:\n",
    "- A model predicting $\\hat{Y} = \\hat{g}(X_i)$ can have high R²\n",
    "- This model has $\\hat{f}(T, W) = 0$ (no treatment effect!)\n",
    "\n",
    "### 2.4 CATE Evaluation\n",
    "\n",
    "For CATE, we need to evaluate $\\hat{\\tau}(X) = \\hat{E}[Y(1) - Y(0) | X]$.\n",
    "\n",
    "**Problem**: We never observe $Y(1) - Y(0)$ for any individual.\n",
    "\n",
    "**Fundamental challenge**: Cannot directly compute prediction error for CATE!\n",
    "\n",
    "$$\\text{MSE}_{CATE} = E[(\\hat{\\tau}(X) - \\tau(X))^2]$$\n",
    "\n",
    "This is **unobservable** because $\\tau(X) = E[Y(1) - Y(0)|X]$ requires both potential outcomes.\n",
    "\n",
    "```\n",
    "★ Key Result ──────────────────────────────────────────────────\n",
    "R² evaluates: Var(Ŷ - Y) / Var(Y)\n",
    "\n",
    "We want to evaluate: Var(τ̂(X) - τ(X))\n",
    "\n",
    "These are NOT equivalent because:\n",
    "- Y = g(X) + τ(X)·T + ε\n",
    "- Predicting Y well ≠ Estimating τ(X) well\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Simulated DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_causal_data(n=100000, n_nuisance=20, n_heter=10, seed=42):\n",
    "    \"\"\"\n",
    "    Generate data where nuisance features dominate prediction\n",
    "    but don't affect treatment heterogeneity.\n",
    "    \n",
    "    DGP: Y = g(X) + f(T, W) + ε\n",
    "    \n",
    "    - X: Features that predict Y strongly but DON'T interact with T\n",
    "    - W: Features that ONLY affect Y through interaction with T\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Sample size\n",
    "    n_nuisance : int\n",
    "        Number of nuisance features (X)\n",
    "    n_heter : int\n",
    "        Number of heterogeneity features (W)\n",
    "    seed : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Data with columns X_0...X_{n_nuisance-1}, W_0...W_{n_heter-1}, T, Y\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Nuisance features X (high predictive power, no treatment interaction)\n",
    "    X = np.random.normal(1, 10, (n, n_nuisance))\n",
    "    nuisance_coefs = np.random.uniform(-1, 1, (n_nuisance, 1))\n",
    "    \n",
    "    # Heterogeneity features W (only affect Y through T)\n",
    "    W = np.random.normal(1, 10, (n, n_heter))\n",
    "    heter_coefs = np.random.uniform(-1, 1, (n_heter, 1))\n",
    "    \n",
    "    # Treatment (randomized)\n",
    "    T = np.random.normal(10, 2, (n, 1))\n",
    "    \n",
    "    # Outcome\n",
    "    # g(X) = 20 * X @ nuisance_coefs (LARGE effect)\n",
    "    # f(T, W) = T + T * W @ heter_coefs (smaller effect)\n",
    "    g_X = 20 * X @ nuisance_coefs\n",
    "    f_T_W = T + T * (W @ heter_coefs)\n",
    "    \n",
    "    Y = np.random.normal(g_X + f_T_W, 0.1)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    X_cols = [f'X_{i}' for i in range(n_nuisance)]\n",
    "    W_cols = [f'W_{i}' for i in range(n_heter)]\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=X_cols)\n",
    "    df = pd.concat([df, pd.DataFrame(W, columns=W_cols)], axis=1)\n",
    "    df['T'] = T.flatten()\n",
    "    df['Y'] = Y.flatten()\n",
    "    \n",
    "    return df, X_cols, W_cols\n",
    "\n",
    "# Generate data\n",
    "df, X_cols, W_cols = generate_causal_data(n=100000, n_nuisance=20, n_heter=10)\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Nuisance features (X): {len(X_cols)}\")\n",
    "print(f\"Heterogeneity features (W): {len(W_cols)}\")\n",
    "print(f\"\\nVariance decomposition:\")\n",
    "print(f\"  Var(Y): {df['Y'].var():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train, test = train_test_split(df, test_size=0.5, random_state=42)\n",
    "print(f\"Train: {len(train)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Uses nuisance features X (high R², but wrong for CATE)\n",
    "formula_m1 = \"Y ~ T * (\" + \" + \".join(X_cols) + \")\"\n",
    "m1 = smf.ols(formula_m1, data=train).fit()\n",
    "\n",
    "# Model 2: Uses heterogeneity features W (low R², but correct for CATE)\n",
    "formula_m2 = \"Y ~ T * (\" + \" + \".join(W_cols) + \")\"\n",
    "m2 = smf.ols(formula_m2, data=train).fit()\n",
    "\n",
    "print(\"Model 1 (uses X, nuisance features):\")\n",
    "print(f\"  Formula: Y ~ T * (X_0 + ... + X_19)\")\n",
    "print(f\"  Train R²: {m1.rsquared:.4f}\")\n",
    "\n",
    "print(\"\\nModel 2 (uses W, heterogeneity features):\")\n",
    "print(f\"  Formula: Y ~ T * (W_0 + ... + W_9)\")\n",
    "print(f\"  Train R²: {m2.rsquared:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with R² (MISLEADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set R²\n",
    "r2_m1 = r2_score(test['Y'], m1.predict(test))\n",
    "r2_m2 = r2_score(test['Y'], m2.predict(test))\n",
    "\n",
    "print(\"Test Set R² Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Model 1 (nuisance features X): R² = {r2_m1:.4f}\")\n",
    "print(f\"Model 2 (heterogeneity features W): R² = {r2_m2:.4f}\")\n",
    "print(\"\\n⚠️  Based on R², Model 1 appears MUCH better!\")\n",
    "print(\"   But this is MISLEADING for causal inference...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the paradox\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models = ['Model 1\\n(Nuisance X)', 'Model 2\\n(Heterogeneity W)']\n",
    "r2_values = [r2_m1, r2_m2]\n",
    "\n",
    "colors = [COLORS['red'], COLORS['green']]\n",
    "bars = ax.bar(models, r2_values, color=colors, alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, r2_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "ax.set_ylabel('Test R²')\n",
    "ax.set_title('R² Comparison: The Misleading Metric\\n'\n",
    "             '(Higher R² ≠ Better Causal Model!)')\n",
    "ax.set_ylim(0, 1.1)\n",
    "apply_tufte_style(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Compute CATE Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cate_predictions(model, data, t_col='T', delta=1):\n",
    "    \"\"\"\n",
    "    Compute CATE predictions from a linear model.\n",
    "    \n",
    "    For linear models: CATE ≈ M(X, W, T) - M(X, W, T - delta)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : statsmodels fitted model\n",
    "        Fitted OLS model\n",
    "    data : pd.DataFrame\n",
    "        Data to predict on\n",
    "    t_col : str\n",
    "        Treatment column name\n",
    "    delta : float\n",
    "        Treatment change for CATE computation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        CATE predictions\n",
    "    \"\"\"\n",
    "    # Prediction at current T\n",
    "    pred_t = model.predict(data)\n",
    "    \n",
    "    # Prediction at T - delta\n",
    "    data_lower = data.copy()\n",
    "    data_lower[t_col] = data_lower[t_col] - delta\n",
    "    pred_t_lower = model.predict(data_lower)\n",
    "    \n",
    "    return pred_t - pred_t_lower\n",
    "\n",
    "# Get CATE predictions\n",
    "test_pred = test.copy()\n",
    "test_pred['cate_m1'] = get_cate_predictions(m1, test)\n",
    "test_pred['cate_m2'] = get_cate_predictions(m2, test)\n",
    "\n",
    "print(\"CATE Predictions:\")\n",
    "print(f\"  M1 mean CATE: {test_pred['cate_m1'].mean():.3f}\")\n",
    "print(f\"  M2 mean CATE: {test_pred['cate_m2'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Elasticity Curve (Proper Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_elasticity(data, y_col, t_col):\n",
    "    \"\"\"\n",
    "    Compute treatment elasticity (slope coefficient).\n",
    "    \n",
    "    Elasticity = Cov(Y, T) / Var(T)\n",
    "    \"\"\"\n",
    "    y = data[y_col].values\n",
    "    t = data[t_col].values\n",
    "    \n",
    "    cov = np.sum((t - t.mean()) * (y - y.mean()))\n",
    "    var = np.sum((t - t.mean())**2)\n",
    "    \n",
    "    return cov / var\n",
    "\n",
    "\n",
    "def cumulative_gain_curve(data, cate_col, y_col, t_col, min_periods=30, steps=100):\n",
    "    \"\"\"\n",
    "    Compute cumulative elasticity curve.\n",
    "    \n",
    "    Orders units by predicted CATE (descending) and computes\n",
    "    cumulative elasticity as more units are included.\n",
    "    \n",
    "    A good CATE model should have steeper curve (high CATE units first).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Data with CATE predictions\n",
    "    cate_col : str\n",
    "        Column with CATE predictions\n",
    "    y_col : str\n",
    "        Outcome column\n",
    "    t_col : str\n",
    "        Treatment column\n",
    "    min_periods : int\n",
    "        Minimum observations to start computing\n",
    "    steps : int\n",
    "        Number of points on curve\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Cumulative elasticity values\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    \n",
    "    # Sort by predicted CATE (descending)\n",
    "    ordered = data.sort_values(cate_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Points to evaluate\n",
    "    n_rows = list(range(min_periods, n, n // steps)) + [n]\n",
    "    \n",
    "    # Compute cumulative elasticity at each point\n",
    "    cum_elast = []\n",
    "    for rows in n_rows:\n",
    "        subset = ordered.head(rows)\n",
    "        elast = compute_elasticity(subset, y_col, t_col)\n",
    "        cum_elast.append(elast * (rows / n))  # Scale by proportion\n",
    "    \n",
    "    return np.array(cum_elast), np.array(n_rows) / n\n",
    "\n",
    "# Compute curves\n",
    "curve_m1, pct_m1 = cumulative_gain_curve(test_pred, 'cate_m1', 'Y', 'T')\n",
    "curve_m2, pct_m2 = cumulative_gain_curve(test_pred, 'cate_m2', 'Y', 'T')\n",
    "\n",
    "# Random baseline\n",
    "overall_elast = compute_elasticity(test_pred, 'Y', 'T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative elasticity curves\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(pct_m1 * 100, curve_m1, color=COLORS['red'], linewidth=2, label='M1 (High R², Wrong)')\n",
    "ax.plot(pct_m2 * 100, curve_m2, color=COLORS['green'], linewidth=2, label='M2 (Low R², Correct)')\n",
    "ax.plot([0, 100], [0, overall_elast], 'k--', linewidth=1.5, label='Random Model')\n",
    "\n",
    "ax.set_xlabel('Percentage of Population (sorted by predicted CATE)')\n",
    "ax.set_ylabel('Cumulative Elasticity')\n",
    "ax.set_title('Cumulative Elasticity Curve: The Correct Metric\\n'\n",
    "             '(Higher = Better Causal Model)')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Model 2 (low R²) is the BETTER causal model!\")\n",
    "print(\"   Its curve is steeper, indicating better CATE ranking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Test R²', 'Cumulative Gain (AUC)', 'Correct for CATE?'],\n",
    "    'Model 1 (X)': [f'{r2_m1:.4f}', f'{np.trapz(curve_m1, pct_m1):.4f}', '❌ No'],\n",
    "    'Model 2 (W)': [f'{r2_m2:.4f}', f'{np.trapz(curve_m2, pct_m2):.4f}', '✅ Yes']\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Conclusion: R² is MISLEADING for causal model evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Key Takeaway ────────────────────────────────────────────────\n",
    "Why R² Fails for Causal Models:\n",
    "\n",
    "1. **R² rewards predicting Y**, not τ(X)\n",
    "2. **Nuisance g(X) can dominate** outcome variance\n",
    "3. **A model ignoring treatment** can have high R²\n",
    "\n",
    "Better alternatives:\n",
    "- Cumulative elasticity curves\n",
    "- Residualized outcome R² (next notebook)\n",
    "- CATE-specific metrics (Tau-Risk, T-learner MSE)\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: Why can't we just use R² to evaluate causal models on RCT data?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**The decomposition argument**:\n",
    "\n",
    "Outcome can be written as:\n",
    "$$Y = g(X) + \\tau(X) \\cdot T + \\epsilon$$\n",
    "\n",
    "where $g(X)$ is nuisance and $\\tau(X)$ is treatment effect heterogeneity.\n",
    "\n",
    "**R² evaluates**:\n",
    "$$R^2 = 1 - \\frac{\\sum(Y - \\hat{Y})^2}{\\sum(Y - \\bar{Y})^2}$$\n",
    "\n",
    "A model can achieve high R² by:\n",
    "- Accurately predicting $g(X)$ (large variance contribution)\n",
    "- Completely ignoring $\\tau(X)$ (small variance contribution)\n",
    "\n",
    "**Example**:\n",
    "- Model A: Predicts $\\hat{Y} = \\hat{g}(X)$, ignores treatment → R² = 0.92\n",
    "- Model B: Predicts $\\hat{Y} = \\hat{\\tau}(X) \\cdot T$, ignores nuisance → R² = 0.08\n",
    "\n",
    "Model A has higher R² but is useless for CATE estimation!\n",
    "\n",
    "**Bottom line**: R² measures prediction accuracy, not causal accuracy.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: What is the cumulative elasticity curve and why is it better?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Definition**:\n",
    "\n",
    "1. Sort units by predicted CATE (descending)\n",
    "2. For top k% of units, compute treatment elasticity:\n",
    "   $$\\text{Elasticity}_k = \\frac{\\text{Cov}(Y, T)}{\\text{Var}(T)} \\bigg|_{\\text{top } k\\%}$$\n",
    "3. Plot elasticity × k/100 vs k\n",
    "\n",
    "**Why it works**:\n",
    "\n",
    "- If model correctly identifies high-CATE units:\n",
    "  - Top k% has higher treatment effect than average\n",
    "  - Curve rises steeply initially\n",
    "  \n",
    "- If model is random:\n",
    "  - Top k% has same effect as overall\n",
    "  - Curve follows 45° line\n",
    "\n",
    "**Interpretation**:\n",
    "- **Steeper curve** = Better CATE ranking\n",
    "- **Area under curve** = Overall CATE quality\n",
    "\n",
    "**Key advantage**: Uses actual treatment effects (from Y and T), not predicted ones!\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: What's the fundamental problem with evaluating CATE models?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**The fundamental problem of causal inference**:\n",
    "\n",
    "We want to evaluate:\n",
    "$$\\text{MSE}_{CATE} = E[(\\hat{\\tau}(X) - \\tau(X))^2]$$\n",
    "\n",
    "But $\\tau(X) = E[Y(1) - Y(0) | X]$ is **never observed**!\n",
    "\n",
    "For any individual, we see:\n",
    "- $Y(1)$ if treated\n",
    "- $Y(0)$ if control\n",
    "- Never both\n",
    "\n",
    "**Consequences**:\n",
    "\n",
    "1. **Cannot compute direct CATE error**\n",
    "2. **Must use indirect metrics**:\n",
    "   - Cumulative gain curves (uses group-level effects)\n",
    "   - Residualized outcome prediction\n",
    "   - Plug-in estimators (T-learner, X-learner)\n",
    "\n",
    "3. **Simulation is gold standard**:\n",
    "   - Only in simulations do we know true $\\tau(X)$\n",
    "   - Can compute actual CATE MSE\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: In practice, how do you evaluate a CATE model without true effects?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Practical evaluation strategies**:\n",
    "\n",
    "1. **Cumulative gain curves** (this notebook)\n",
    "   - Sort by predicted CATE, check if actual effects align\n",
    "   - Works on randomized data\n",
    "\n",
    "2. **Residualized outcome prediction** (next notebook)\n",
    "   - Remove $g(X)$ from outcome\n",
    "   - Evaluate R² on $\\tilde{Y} = Y - \\hat{g}(X)$\n",
    "\n",
    "3. **Calibration plots**\n",
    "   - Bin by predicted CATE\n",
    "   - Check actual elasticity in each bin\n",
    "\n",
    "4. **Uplift metrics** (marketing applications)\n",
    "   - Qini curves\n",
    "   - Uplift at percentiles\n",
    "\n",
    "5. **Cross-validation with causal loss**\n",
    "   - R-loss: $\\sum(\\tilde{Y} - \\hat{\\tau}(X) \\cdot \\tilde{T})^2$\n",
    "\n",
    "**Best practice**: Use multiple metrics, not just one!\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: When IS R² appropriate in causal inference?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**R² is appropriate for**:\n",
    "\n",
    "1. **Nuisance model evaluation**\n",
    "   - Evaluating $\\hat{g}(X)$ in DML\n",
    "   - Evaluating propensity score model\n",
    "   - Goal IS prediction accuracy\n",
    "\n",
    "2. **Outcome prediction (not causal)**\n",
    "   - Forecasting Y regardless of treatment\n",
    "   - Risk scoring applications\n",
    "\n",
    "3. **When treatment effect is large**\n",
    "   - If $\\text{Var}(\\tau(X) \\cdot T) >> \\text{Var}(g(X))$\n",
    "   - Then high R² implies good treatment modeling\n",
    "   - Rare in practice\n",
    "\n",
    "**R² is NOT appropriate for**:\n",
    "\n",
    "1. **CATE model selection**\n",
    "2. **Treatment effect heterogeneity ranking**\n",
    "3. **Comparing causal models**\n",
    "\n",
    "**Rule of thumb**: If you're asking \"how does Y change with T?\", don't use R².\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Athey, S., & Imbens, G. (2016). Recursive Partitioning for Heterogeneous \n",
    "      Causal Effects. *PNAS*.\n",
    "\n",
    "[^2]: Nie, X., & Wager, S. (2021). Quasi-Oracle Estimation of Heterogeneous \n",
    "      Treatment Effects. *Biometrika*.\n",
    "\n",
    "[^3]: Chernozhukov, V., et al. (2018). Generic Machine Learning Inference on \n",
    "      Heterogeneous Treatment Effects. *Econometrica*.\n",
    "\n",
    "[^4]: Facure, M. (2022). *Causal Inference for the Brave and True*, Appendix: \n",
    "      Prediction Metrics for Causal Models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
