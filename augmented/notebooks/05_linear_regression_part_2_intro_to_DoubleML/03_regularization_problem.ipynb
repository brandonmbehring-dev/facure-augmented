{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 03. The Regularization Problem: Why Naive ML Fails\n",
    "\n",
    "**Part 2**: Linear Regression → Double Machine Learning Bridge  \n",
    "**Notebook**: 03 - The Regularization Problem  \n",
    "**Tier**: B (Applied) — Practical focus with numerical demonstrations  \n",
    "**Prerequisites**: Notebooks 01-02 (Robinson, Orthogonality)  \n",
    "**Forward Reference**: Notebook 04 (Cross-fitting)\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Problem\n",
    "\n",
    "> **Why can't we just use Lasso or Random Forest directly for causal inference?**\n",
    "\n",
    "This notebook explores the fundamental tension between prediction and causal inference, and why regularization bias is particularly problematic.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [High-Dimensional Causal Inference](#1-high-dimensional-causal-inference)\n",
    "2. [The Promise of ML](#2-the-promise-of-ml)\n",
    "3. [The Regularization Bias Problem](#3-the-regularization-bias-problem)\n",
    "4. [Prediction vs Causal Inference](#4-prediction-vs-causal-inference)\n",
    "5. [Numerical Demonstration](#5-numerical-demonstration)\n",
    "6. [How DML Solves This](#6-how-dml-solves-this)\n",
    "7. [Key Takeaways](#7-key-takeaways)\n",
    "8. [Interview Question](#8-interview-question)\n",
    "9. [References](#9-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import Lasso, Ridge, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. High-Dimensional Causal Inference\n",
    "\n",
    "Modern causal inference often faces **high-dimensional settings**:\n",
    "\n",
    "| Domain | Example | Dimensionality |\n",
    "|--------|---------|----------------|\n",
    "| Healthcare | Electronic Health Records | 10,000+ diagnosis codes |\n",
    "| Marketing | User features | 500+ behavioral variables |\n",
    "| Economics | Text features | 50,000+ word embeddings |\n",
    "| Finance | Technical indicators | 200+ derived features |\n",
    "\n",
    "### The Problem with OLS\n",
    "\n",
    "- **$p > n$**: More variables than observations → OLS undefined\n",
    "- **$p \\approx n$**: Overfitting, unstable estimates, multicollinearity\n",
    "- **$p < n$ but large**: High variance, poor finite-sample performance\n",
    "\n",
    "```\n",
    "★ The Challenge ─────────────────────────────────────────────\n",
    "  \n",
    "  We need to adjust for many confounders to satisfy\n",
    "  conditional ignorability: (Y₀, Y₁) ⊥ T | X\n",
    "  \n",
    "  But with many X variables, OLS becomes unstable or infeasible.\n",
    "─────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate OLS instability in high dimensions\n",
    "def ols_instability_demo(n=200, p_values=[10, 50, 100, 150, 190]):\n",
    "    \"\"\"Show how OLS becomes unstable as p approaches n.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for p in p_values:\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate data - only first 3 variables matter\n",
    "        X = np.random.randn(n, p)\n",
    "        true_beta = np.zeros(p)\n",
    "        true_beta[:3] = [1.0, 0.5, -0.5]\n",
    "        \n",
    "        Y = X @ true_beta + np.random.normal(0, 1, n)\n",
    "        \n",
    "        # Fit OLS\n",
    "        try:\n",
    "            X_const = sm.add_constant(X)\n",
    "            model = sm.OLS(Y, X_const).fit()\n",
    "            \n",
    "            # Extract first coefficient (should be ~1.0)\n",
    "            beta_1_hat = model.params[1]\n",
    "            se_1 = model.bse[1]\n",
    "            condition_number = np.linalg.cond(X_const)\n",
    "            \n",
    "            results.append({\n",
    "                'p': p,\n",
    "                'p/n': p/n,\n",
    "                'beta_1_hat': beta_1_hat,\n",
    "                'se': se_1,\n",
    "                'condition': condition_number,\n",
    "                'status': 'OK'\n",
    "            })\n",
    "        except:\n",
    "            results.append({\n",
    "                'p': p,\n",
    "                'p/n': p/n,\n",
    "                'beta_1_hat': np.nan,\n",
    "                'se': np.nan,\n",
    "                'condition': np.inf,\n",
    "                'status': 'SINGULAR'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "ols_results = ols_instability_demo()\n",
    "print(\"OLS Instability as p → n\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sample size n = 200, True β₁ = 1.0\")\n",
    "print()\n",
    "print(ols_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Promise of ML\n",
    "\n",
    "Machine learning methods seem ideal for high-dimensional settings:\n",
    "\n",
    "### Lasso (L1 Regularization)\n",
    "- **Automatic variable selection**: Sets irrelevant coefficients to exactly zero\n",
    "- **Works when $p > n$**: Regularization makes problem well-posed\n",
    "- **Sparse solutions**: Focuses on important predictors\n",
    "\n",
    "### Random Forest\n",
    "- **Handles nonlinearity**: Tree splits capture complex relationships\n",
    "- **Feature importance**: Automatic ranking of variable importance\n",
    "- **Robust to irrelevant variables**: Trees ignore noise variables\n",
    "\n",
    "### Gradient Boosting\n",
    "- **Extremely flexible**: Can approximate any function\n",
    "- **State-of-the-art prediction**: Wins Kaggle competitions\n",
    "- **Regularization via shrinkage**: Learning rate controls complexity\n",
    "\n",
    "```\n",
    "★ The Temptation ────────────────────────────────────────────\n",
    "  \n",
    "  \"Just throw ML at the problem!\"\n",
    "  \n",
    "  - Use Random Forest to estimate E[Y|X]\n",
    "  - Use Lasso to estimate E[T|X]\n",
    "  - Apply Robinson transformation\n",
    "  - Done?\n",
    "  \n",
    "  NOT SO FAST...\n",
    "─────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Regularization Bias Problem\n",
    "\n",
    "### The Fundamental Issue\n",
    "\n",
    "**Regularization introduces bias by design!**\n",
    "\n",
    "For Lasso:\n",
    "$$\\hat{\\beta}_{Lasso} = \\underset{\\beta}{\\text{argmin}} \\left[ \\sum_i (Y_i - X_i'\\beta)^2 + \\lambda \\|\\beta\\|_1 \\right]$$\n",
    "\n",
    "The penalty $\\lambda \\|\\beta\\|_1$ **shrinks coefficients toward zero**, introducing bias:\n",
    "$$E[\\hat{\\beta}_{Lasso}] \\neq \\beta \\quad \\text{(biased!)}$$\n",
    "\n",
    "For Ridge:\n",
    "$$E[\\hat{\\beta}_{Ridge}] = (I + \\lambda(X'X)^{-1})^{-1} \\beta \\quad \\text{(shrinkage toward zero)}$$\n",
    "\n",
    "### Why Is This a Problem for Causal Inference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-dimensional confounded data\n",
    "n = 2000\n",
    "p = 20  # High-dim confounders\n",
    "np.random.seed(42)\n",
    "\n",
    "# Confounders with complex structure\n",
    "X = np.random.randn(n, p)\n",
    "\n",
    "# Treatment depends on X nonlinearly\n",
    "# T = X₁² + X₂² + 0.5·X₃·X₄ + noise\n",
    "m0_X = X[:, 0]**2 + X[:, 1]**2 + 0.5 * X[:, 2] * X[:, 3]\n",
    "T = m0_X + np.random.normal(0, 1, n)\n",
    "\n",
    "# Outcome depends on T and X nonlinearly\n",
    "# Y = τ·T + sin(X₁) + exp(X₂/2) + X₃² + noise\n",
    "true_tau = 2.5\n",
    "g0_X = np.sin(X[:, 0]) + np.exp(X[:, 1] / 2) + X[:, 2]**2\n",
    "Y = true_tau * T + g0_X + np.random.normal(0, 1, n)\n",
    "\n",
    "# E[Y|X] = τ·E[T|X] + g₀(X)\n",
    "l0_X = true_tau * m0_X + g0_X\n",
    "\n",
    "print(f\"Data: n = {n}, p = {p}\")\n",
    "print(f\"True τ = {true_tau}\")\n",
    "print(f\"E[T|X] = X₁² + X₂² + 0.5·X₃·X₄\")\n",
    "print(f\"g₀(X) = sin(X₁) + exp(X₂/2) + X₃²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_ml_fwl(Y, T, X, model):\n",
    "    \"\"\"\n",
    "    Naive FWL with ML: Use ML to residualize, then regress.\n",
    "    \n",
    "    This does NOT use cross-fitting!\n",
    "    \"\"\"\n",
    "    # Fit on all data (in-sample predictions)\n",
    "    model_T = model.fit(X, T)\n",
    "    T_resid = T - model_T.predict(X)\n",
    "    \n",
    "    model_Y = model.fit(X, Y)\n",
    "    Y_resid = Y - model_Y.predict(X)\n",
    "    \n",
    "    # Regress residuals\n",
    "    tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "    return tau_hat\n",
    "\n",
    "\n",
    "# Compare different ML methods\n",
    "methods = {\n",
    "    'OLS (linear)': sm.OLS,\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42),\n",
    "}\n",
    "\n",
    "print(\"Naive FWL with Different ML Methods (NO cross-fitting)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True τ = {true_tau}\")\n",
    "print()\n",
    "\n",
    "results = []\n",
    "for name, model in methods.items():\n",
    "    if name == 'OLS (linear)':\n",
    "        # OLS baseline\n",
    "        X_const = sm.add_constant(X)\n",
    "        T_resid = T - sm.OLS(T, X_const).fit().fittedvalues\n",
    "        Y_resid = Y - sm.OLS(Y, X_const).fit().fittedvalues\n",
    "        tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "    else:\n",
    "        tau_hat = naive_ml_fwl(Y, T, X, model)\n",
    "    \n",
    "    bias = tau_hat - true_tau\n",
    "    results.append({'Method': name, 'τ̂': tau_hat, 'Bias': bias})\n",
    "    print(f\"{name:25s}: τ̂ = {tau_hat:.3f}  (bias = {bias:+.3f})\")\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bias\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 5))\n",
    "\n",
    "colors_list = [COLORS['blue'], COLORS['purple'], COLORS['orange'], \n",
    "               COLORS['green'], COLORS['red']]\n",
    "\n",
    "bars = ax.barh(results_df['Method'], results_df['τ̂'], color=colors_list, \n",
    "               height=0.6, edgecolor='white', linewidth=1.5)\n",
    "ax.axvline(true_tau, c='black', ls='--', lw=2.5, label=f'True τ = {true_tau}')\n",
    "ax.fill_betweenx([-1, 6], true_tau - 0.1, true_tau + 0.1, alpha=0.2, color='black')\n",
    "\n",
    "# Add bias annotations\n",
    "for i, (_, row) in enumerate(results_df.iterrows()):\n",
    "    ax.text(row['τ̂'] + 0.05, i, f'{row[\"Bias\"]:+.2f}', va='center', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Estimated τ', fontsize=11)\n",
    "ax.set_title('Naive FWL with ML (No Cross-Fitting)', fontweight='bold')\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "ax.set_xlim(1.5, 3.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n★ Key Observation: All methods are biased!\")\n",
    "print(\"  - Linear OLS: Can't capture nonlinear confounding\")\n",
    "print(\"  - Ridge/Lasso: Regularization bias shrinks predictions\")\n",
    "print(\"  - RF/GBM: Overfitting on same data used for residualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Prediction vs Causal Inference\n",
    "\n",
    "### The Bias-Variance Tradeoff\n",
    "\n",
    "In **prediction**, we optimize Mean Squared Error:\n",
    "$$MSE = Bias^2 + Variance$$\n",
    "\n",
    "Regularization *increases* bias but *decreases* variance. Often, this tradeoff improves MSE.\n",
    "\n",
    "### Why Causal Inference Is Different\n",
    "\n",
    "```\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║              PREDICTION vs CAUSAL INFERENCE                   ║\n",
    "╠═══════════════════════════════════════════════════════════════╣\n",
    "║                                                               ║\n",
    "║  PREDICTION:                                                  ║\n",
    "║  - Goal: Minimize E[(Y - Ŷ)²]                                ║\n",
    "║  - Bias-variance tradeoff works in our favor                 ║\n",
    "║  - Regularization often HELPS                                ║\n",
    "║                                                               ║\n",
    "║  CAUSAL INFERENCE:                                           ║\n",
    "║  - Goal: Unbiased estimate of τ                              ║\n",
    "║  - Bias in nuisance → Bias in τ̂                             ║\n",
    "║  - Regularization can HURT                                   ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: Good prediction ≠ Good causal inference\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"Prediction Performance vs Causal Bias\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prediction_results = []\n",
    "\n",
    "for name, model in methods.items():\n",
    "    if name == 'OLS (linear)':\n",
    "        X_const = sm.add_constant(X)\n",
    "        Y_pred = sm.OLS(Y, X_const).fit().fittedvalues\n",
    "        T_pred = sm.OLS(T, X_const).fit().fittedvalues\n",
    "        T_resid = T - T_pred\n",
    "        Y_resid = Y - Y_pred\n",
    "        tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "    else:\n",
    "        model_Y = model.__class__(**model.get_params()).fit(X, Y)\n",
    "        Y_pred = model_Y.predict(X)\n",
    "        model_T = model.__class__(**model.get_params()).fit(X, T)\n",
    "        T_pred = model_T.predict(X)\n",
    "        T_resid = T - T_pred\n",
    "        Y_resid = Y - Y_pred\n",
    "        tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "    \n",
    "    mse_Y = mean_squared_error(Y, Y_pred)\n",
    "    r2_Y = r2_score(Y, Y_pred)\n",
    "    causal_bias = abs(tau_hat - true_tau)\n",
    "    \n",
    "    prediction_results.append({\n",
    "        'Method': name,\n",
    "        'MSE(Y)': mse_Y,\n",
    "        'R²(Y)': r2_Y,\n",
    "        'Causal Bias': causal_bias\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(prediction_results)\n",
    "print(pred_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the disconnect\nfig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n\n# Panel 1: Prediction R² (higher is better)\nax = axes[0]\nax.barh(pred_df['Method'], pred_df['R²(Y)'], color=colors_list, \n        height=0.6, edgecolor='white', linewidth=1.5)\nax.set_xlabel('R² for Y prediction', fontsize=11)\nax.set_title('(a) Prediction Performance', fontweight='bold')\nax.set_xlim(0, 1)\n\n# Panel 2: Causal Bias (lower is better)\nax = axes[1]\nax.barh(pred_df['Method'], pred_df['Causal Bias'], color=colors_list, \n        height=0.6, edgecolor='white', linewidth=1.5)\nax.axvline(0, c='black', ls='--', lw=1.5)\nax.set_xlabel('|Bias in τ̂|', fontsize=11)\nax.set_title('(b) Causal Bias', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Correlation between prediction and causal performance\ncorr = np.corrcoef(pred_df['R²(Y)'], -pred_df['Causal Bias'])[0, 1]\nprint(f\"\\nCorrelation between R² and (negative) bias: {corr:.3f}\")\nprint(\"→ Better prediction does NOT guarantee better causal estimation!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Numerical Demonstration\n",
    "\n",
    "Let's run a systematic comparison across different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "def compare_naive_vs_dml(n=1000, p=20, n_sims=20):\n    \"\"\"\n    Compare naive ML FWL vs proper DML with cross-fitting.\n    \n    Note: n_sims kept small for execution speed. \n    Increase for more precise bias/variance estimates.\n    \"\"\"\n    true_tau = 2.5\n    \n    naive_rf_results = []\n    dml_rf_results = []\n    ols_results = []\n    \n    for sim in range(n_sims):\n        np.random.seed(sim)\n        \n        # Generate data\n        X = np.random.randn(n, p)\n        m0 = X[:, 0]**2 + X[:, 1]**2 + 0.5 * X[:, 2] * X[:, 3]\n        g0 = np.sin(X[:, 0]) + np.exp(X[:, 1] / 2) + X[:, 2]**2\n        \n        T = m0 + np.random.normal(0, 1, n)\n        Y = true_tau * T + g0 + np.random.normal(0, 1, n)\n        \n        # 1. OLS (linear) - baseline\n        X_const = sm.add_constant(X)\n        T_resid_ols = T - sm.OLS(T, X_const).fit().fittedvalues\n        Y_resid_ols = Y - sm.OLS(Y, X_const).fit().fittedvalues\n        tau_ols = np.sum(T_resid_ols * Y_resid_ols) / np.sum(T_resid_ols**2)\n        ols_results.append(tau_ols)\n        \n        # 2. Naive RF (no cross-fitting)\n        rf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=sim)\n        T_pred_naive = rf.fit(X, T).predict(X)\n        Y_pred_naive = rf.fit(X, Y).predict(X)\n        T_resid_naive = T - T_pred_naive\n        Y_resid_naive = Y - Y_pred_naive\n        tau_naive = np.sum(T_resid_naive * Y_resid_naive) / np.sum(T_resid_naive**2)\n        naive_rf_results.append(tau_naive)\n        \n        # 3. DML with cross-fitting\n        T_pred_cv = cross_val_predict(rf, X, T, cv=5)\n        Y_pred_cv = cross_val_predict(rf, X, Y, cv=5)\n        T_resid_cv = T - T_pred_cv\n        Y_resid_cv = Y - Y_pred_cv\n        tau_dml = np.sum(T_resid_cv * Y_resid_cv) / np.sum(T_resid_cv**2)\n        dml_rf_results.append(tau_dml)\n    \n    return {\n        'OLS': np.array(ols_results),\n        'Naive RF': np.array(naive_rf_results),\n        'DML RF': np.array(dml_rf_results),\n        'true_tau': true_tau\n    }\n\n# Run simulation (n_sims=20 for speed; increase for more precision)\nsim_results = compare_naive_vs_dml(n=1000, p=20, n_sims=20)\n\nprint(\"Monte Carlo Comparison (20 simulations)\")\nprint(\"=\" * 60)\nprint(f\"True τ = {sim_results['true_tau']}\")\nprint()\nfor name in ['OLS', 'Naive RF', 'DML RF']:\n    est = sim_results[name]\n    print(f\"{name:12s}: Mean = {np.mean(est):.3f}, Bias = {np.mean(est) - sim_results['true_tau']:+.3f}, SD = {np.std(est):.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize simulation results\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Distributions\n",
    "ax = axes[0]\n",
    "bins = np.linspace(1.5, 3.5, 40)\n",
    "ax.hist(sim_results['OLS'], bins=bins, alpha=0.5, color=COLORS['blue'], \n",
    "        label='OLS (linear)', edgecolor='white')\n",
    "ax.hist(sim_results['Naive RF'], bins=bins, alpha=0.5, color=COLORS['red'], \n",
    "        label='Naive RF', edgecolor='white')\n",
    "ax.hist(sim_results['DML RF'], bins=bins, alpha=0.5, color=COLORS['green'], \n",
    "        label='DML RF', edgecolor='white')\n",
    "ax.axvline(sim_results['true_tau'], c='black', ls='--', lw=2.5, label=f'True τ')\n",
    "ax.set_xlabel('Estimated τ', fontsize=11)\n",
    "ax.set_ylabel('Frequency', fontsize=11)\n",
    "ax.set_title('(a) Distribution of Estimates', fontweight='bold')\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# Panel 2: Bias-Variance summary\n",
    "ax = axes[1]\n",
    "names = ['OLS\\n(linear)', 'Naive\\nRF', 'DML\\nRF']\n",
    "biases = [np.mean(sim_results['OLS']) - sim_results['true_tau'],\n",
    "          np.mean(sim_results['Naive RF']) - sim_results['true_tau'],\n",
    "          np.mean(sim_results['DML RF']) - sim_results['true_tau']]\n",
    "sds = [np.std(sim_results['OLS']),\n",
    "       np.std(sim_results['Naive RF']),\n",
    "       np.std(sim_results['DML RF'])]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, biases, width, label='Bias', color=COLORS['red'], alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, sds, width, label='Std Dev', color=COLORS['blue'], alpha=0.7)\n",
    "\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=1)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_ylabel('Value', fontsize=11)\n",
    "ax.set_title('(b) Bias vs Variance', fontweight='bold')\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. How DML Solves This\n",
    "\n",
    "DML addresses the regularization problem through two key innovations:\n",
    "\n",
    "### 1. Neyman Orthogonality (Notebook 02)\n",
    "\n",
    "The DML score is designed so that **first-order bias in nuisance estimation has zero first-order effect on $\\hat{\\tau}$**:\n",
    "\n",
    "$$\\text{Bias}(\\hat{\\tau}) \\propto \\|\\hat{\\eta} - \\eta_0\\|^2 \\quad \\text{(second order, not first order)}$$\n",
    "\n",
    "This means ML's regularization bias becomes **negligible** at reasonable sample sizes.\n",
    "\n",
    "### 2. Cross-Fitting (Notebook 04)\n",
    "\n",
    "Even with orthogonality, using the **same data** for nuisance estimation and score evaluation causes **overfitting bias**. Cross-fitting solves this:\n",
    "\n",
    "```\n",
    "For each fold k:\n",
    "  1. Train nuisance models on OTHER folds\n",
    "  2. Predict on fold k (out-of-sample)\n",
    "  3. Compute residuals on fold k\n",
    "```\n",
    "\n",
    "This breaks the dependence between nuisance estimation and score evaluation.\n",
    "\n",
    "### The DML Recipe\n",
    "\n",
    "```\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║                    DML ALGORITHM                              ║\n",
    "╠═══════════════════════════════════════════════════════════════╣\n",
    "║                                                               ║\n",
    "║  1. Split data into K folds: I₁, ..., I_K                    ║\n",
    "║                                                               ║\n",
    "║  2. For each fold k:                                         ║\n",
    "║     a. Train ℓ̂^{(-k)}(X) on ∪_{j≠k} I_j                     ║\n",
    "║     b. Train m̂^{(-k)}(X) on ∪_{j≠k} I_j                     ║\n",
    "║     c. Compute residuals on fold k                           ║\n",
    "║                                                               ║\n",
    "║  3. Pool residuals from all folds                            ║\n",
    "║                                                               ║\n",
    "║  4. Final regression: τ̂ = Cov(Ỹ, T̃) / Var(T̃)               ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preview of DML improvement\n",
    "print(\"Summary: Naive vs DML\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNaive RF:\")\n",
    "print(f\"  Problem: Uses same data for nuisance and evaluation\")\n",
    "print(f\"  Result:  Bias = {np.mean(sim_results['Naive RF']) - sim_results['true_tau']:+.3f}\")\n",
    "print(f\"\\nDML RF:\")\n",
    "print(f\"  Solution: Cross-fitting + orthogonal score\")\n",
    "print(f\"  Result:   Bias = {np.mean(sim_results['DML RF']) - sim_results['true_tau']:+.3f}\")\n",
    "print(f\"\\nBias reduction: {abs((np.mean(sim_results['Naive RF']) - sim_results['true_tau']) / (np.mean(sim_results['DML RF']) - sim_results['true_tau'])):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "```\n",
    "★ Summary ──────────────────────────────────────────────────\n",
    "\n",
    "1. HIGH-DIMENSIONAL CAUSAL INFERENCE is common but challenging\n",
    "   - Many confounders to adjust for\n",
    "   - OLS becomes unstable as p → n\n",
    "\n",
    "2. ML SEEMS PROMISING but has hidden issues\n",
    "   - Regularization introduces bias by design\n",
    "   - Good prediction ≠ Good causal inference\n",
    "\n",
    "3. THE REGULARIZATION PROBLEM\n",
    "   - Lasso/Ridge shrink predictions toward mean\n",
    "   - This bias propagates to τ̂ in naive approaches\n",
    "\n",
    "4. WHY PREDICTION ≠ CAUSAL INFERENCE\n",
    "   - Prediction: Bias-variance tradeoff helps\n",
    "   - Causal: Bias in nuisance → Bias in target\n",
    "\n",
    "5. DML SOLUTIONS\n",
    "   - Orthogonality: Makes τ̂ insensitive to first-order bias\n",
    "   - Cross-fitting: Prevents overfitting bias\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Interview Question\n",
    "\n",
    "**Q (Citadel, Quant Researcher)**: *\"Why can't we just use Lasso for causal inference? When does regularization help vs hurt?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Lasso solves**: $\\min_\\beta \\|Y - X\\beta\\|^2 + \\lambda\\|\\beta\\|_1$\n",
    "   - The penalty shrinks coefficients toward zero\n",
    "   - This **introduces bias**: $E[\\hat{\\beta}_{Lasso}] \\neq \\beta$\n",
    "\n",
    "2. **In prediction**, this is fine:\n",
    "   - We optimize MSE = Bias² + Variance\n",
    "   - Regularization trades bias for variance reduction\n",
    "   - Net effect often positive\n",
    "\n",
    "3. **In causal inference**, this is problematic:\n",
    "   - We need unbiased $\\hat{\\tau}$, not minimum MSE\n",
    "   - Bias in nuisance (E[T|X], E[Y|X]) → Bias in $\\hat{\\tau}$\n",
    "   - Regularization bias propagates!\n",
    "\n",
    "4. **When regularization helps**:\n",
    "   - High-dimensional prediction (p > n)\n",
    "   - When we care about MSE, not unbiasedness\n",
    "   - Variable selection in exploratory analysis\n",
    "\n",
    "5. **When regularization hurts**:\n",
    "   - Causal effect estimation (need unbiased τ̂)\n",
    "   - Naive plug-in approaches (using Lasso predictions directly)\n",
    "\n",
    "6. **The DML solution**:\n",
    "   - Use regularized ML for nuisance estimation (where it helps)\n",
    "   - But use orthogonal score so regularization bias is second-order\n",
    "   - Combined with cross-fitting to prevent overfitting\n",
    "\n",
    "**One-liner**: \"Regularization biases nuisance estimates, which biases causal effects in naive approaches; DML's orthogonality makes this bias negligible.\"\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 9. References\n",
    "\n",
    "[^1]: Chernozhukov, V. et al. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^2]: Belloni, A., Chernozhukov, V., and Hansen, C. (2014). Inference on Treatment Effects after Selection among High-Dimensional Controls. *Review of Economic Studies*, 81(2), 608-650.\n",
    "\n",
    "[^3]: Hastie, T., Tibshirani, R., and Friedman, J. (2009). *The Elements of Statistical Learning*. Springer. Chapter 3.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [04. Cross-Fitting](./04_cross_fitting.ipynb) — Why sample splitting is necessary and how it works"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}