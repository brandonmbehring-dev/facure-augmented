{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 05. DML Implementation: Putting It All Together\n",
    "\n",
    "**Part 2**: Linear Regression → Double Machine Learning Bridge  \n",
    "**Notebook**: 05 - DML Implementation  \n",
    "**Tier**: B→A (Applied to Graduate) — Full algorithm, theory, and comprehensive interview prep  \n",
    "**Prerequisites**: Notebooks 01-04 (FWL, Orthogonality, Regularization, Cross-fitting)  \n",
    "**Forward Reference**: Chapter 22 (Advanced DML, CATE, Heterogeneity)\n",
    "\n",
    "---\n",
    "\n",
    "## The Complete Picture\n",
    "\n",
    "This notebook synthesizes everything from Parts 01-04:\n",
    "\n",
    "1. **Robinson transformation** (from FWL)\n",
    "2. **Neyman orthogonality** (robustness to bias)\n",
    "3. **Cross-fitting** (prevents overfitting)\n",
    "\n",
    "→ **Double Machine Learning (DML)**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The DML Algorithm](#1-the-dml-algorithm)\n",
    "2. [From-Scratch Implementation](#2-from-scratch-implementation)\n",
    "3. [Using EconML](#3-using-econml)\n",
    "4. [Variance Estimation](#4-variance-estimation)\n",
    "5. [Comparing ML Methods](#5-comparing-ml-methods)\n",
    "6. [Diagnostics](#6-diagnostics)\n",
    "7. [When to Use DML](#7-when-to-use-dml)\n",
    "8. [Bridge to Chapter 22](#8-bridge-to-chapter-22)\n",
    "9. [Interview Appendix](#9-interview-appendix)\n",
    "10. [References](#10-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm, smf, stats,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, LassoCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The DML Algorithm\n",
    "\n",
    "### Formal Statement\n",
    "\n",
    "**Setting**: Partial linear model\n",
    "$$Y = \\tau \\cdot T + g_0(X) + \\varepsilon, \\quad E[\\varepsilon|X,T] = 0$$\n",
    "$$T = m_0(X) + v, \\quad E[v|X] = 0$$\n",
    "\n",
    "**Goal**: Estimate $\\tau$ (average treatment effect)\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "```\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║               DOUBLE MACHINE LEARNING (DML)                   ║\n",
    "╠═══════════════════════════════════════════════════════════════╣\n",
    "║                                                               ║\n",
    "║  INPUT: Data (Y, T, X), ML models for nuisance, K folds      ║\n",
    "║                                                               ║\n",
    "║  STEP 1: Split data into K folds I₁, ..., I_K                ║\n",
    "║                                                               ║\n",
    "║  STEP 2: For each fold k = 1, ..., K:                        ║\n",
    "║    a) Train ℓ̂^{(-k)}(X) ≈ E[Y|X] on complement              ║\n",
    "║    b) Train m̂^{(-k)}(X) ≈ E[T|X] on complement              ║\n",
    "║    c) Compute residuals on fold k:                           ║\n",
    "║       Ỹᵢ = Yᵢ - ℓ̂^{(-k)}(Xᵢ)                                ║\n",
    "║       T̃ᵢ = Tᵢ - m̂^{(-k)}(Xᵢ)                                ║\n",
    "║                                                               ║\n",
    "║  STEP 3: Pool residuals from all folds                       ║\n",
    "║                                                               ║\n",
    "║  STEP 4: Final regression                                    ║\n",
    "║       τ̂ = Σᵢ T̃ᵢ · Ỹᵢ / Σᵢ T̃ᵢ²                             ║\n",
    "║                                                               ║\n",
    "║  STEP 5: Compute SE via influence function                   ║\n",
    "║                                                               ║\n",
    "║  OUTPUT: τ̂, SE, 95% CI                                       ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. From-Scratch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "class DoubleMachineLearning:\n    \"\"\"\n    Double Machine Learning estimator for Average Treatment Effect.\n    \n    Implements the partial linear model:\n        Y = τ·T + g₀(X) + ε\n        T = m₀(X) + v\n    \n    Uses cross-fitting to avoid overfitting bias.\n    Uses the Neyman-orthogonal score for robustness to nuisance estimation.\n    \n    Parameters\n    ----------\n    model_Y : sklearn estimator\n        Model for E[Y|X]. Default: RandomForest.\n    model_T : sklearn estimator\n        Model for E[T|X]. Default: RandomForest.\n    n_folds : int\n        Number of cross-fitting folds. Default: 5.\n    random_state : int\n        Random seed for reproducibility.\n    \n    Attributes\n    ----------\n    tau_ : float\n        Estimated treatment effect.\n    se_ : float\n        Standard error of tau_.\n    ci_ : tuple\n        95% confidence interval (lower, upper).\n    pvalue_ : float\n        P-value for H0: tau = 0.\n    \n    References\n    ----------\n    Chernozhukov et al. (2018). Double/Debiased Machine Learning.\n    \"\"\"\n    \n    def __init__(self, model_Y=None, model_T=None, n_folds=5, random_state=42):\n        # Use explicit None check (not 'or') because sklearn estimators\n        # implement __len__ which fails on unfitted cloned models\n        if model_Y is None:\n            self.model_Y = RandomForestRegressor(\n                n_estimators=100, max_depth=10, min_samples_leaf=5, random_state=random_state\n            )\n        else:\n            self.model_Y = model_Y\n            \n        if model_T is None:\n            self.model_T = RandomForestRegressor(\n                n_estimators=100, max_depth=10, min_samples_leaf=5, random_state=random_state\n            )\n        else:\n            self.model_T = model_T\n            \n        self.n_folds = n_folds\n        self.random_state = random_state\n        \n        # Results (set after fit)\n        self.tau_ = None\n        self.se_ = None\n        self.ci_ = None\n        self.pvalue_ = None\n        self.Y_resid_ = None\n        self.T_resid_ = None\n    \n    def fit(self, Y, T, X):\n        \"\"\"\n        Fit the DML estimator.\n        \n        Parameters\n        ----------\n        Y : array-like (n,)\n            Outcome variable.\n        T : array-like (n,)\n            Treatment variable (continuous or binary).\n        X : array-like (n, p)\n            Confounders/covariates.\n            \n        Returns\n        -------\n        self\n        \"\"\"\n        Y = np.asarray(Y)\n        T = np.asarray(T)\n        X = np.asarray(X)\n        n = len(Y)\n        \n        # Step 1-2: Cross-fitted residualization\n        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n        \n        Y_resid = np.zeros(n)\n        T_resid = np.zeros(n)\n        \n        for train_idx, test_idx in kf.split(X):\n            # Clone models to avoid state leakage\n            mY = clone(self.model_Y).fit(X[train_idx], Y[train_idx])\n            mT = clone(self.model_T).fit(X[train_idx], T[train_idx])\n            \n            # Out-of-fold predictions\n            Y_resid[test_idx] = Y[test_idx] - mY.predict(X[test_idx])\n            T_resid[test_idx] = T[test_idx] - mT.predict(X[test_idx])\n        \n        # Step 3-4: Final regression on residuals\n        tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n        \n        # Step 5: Influence function based SE\n        # ψᵢ = T̃ᵢ(Ỹᵢ - τ̂T̃ᵢ) / E[T̃²]\n        psi = T_resid * (Y_resid - tau_hat * T_resid) / np.mean(T_resid**2)\n        se = np.sqrt(np.var(psi, ddof=1) / n)\n        \n        # 95% CI and p-value\n        ci_lower = tau_hat - 1.96 * se\n        ci_upper = tau_hat + 1.96 * se\n        z_stat = tau_hat / se\n        pvalue = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n        \n        # Store results\n        self.tau_ = tau_hat\n        self.se_ = se\n        self.ci_ = (ci_lower, ci_upper)\n        self.pvalue_ = pvalue\n        self.Y_resid_ = Y_resid\n        self.T_resid_ = T_resid\n        \n        return self\n    \n    def summary(self):\n        \"\"\"Print formatted results.\"\"\"\n        if self.tau_ is None:\n            raise ValueError(\"Model not fitted. Call fit() first.\")\n        \n        print(\"Double Machine Learning Results\")\n        print(\"=\" * 50)\n        print(f\"τ̂ (ATE):      {self.tau_:.4f}\")\n        print(f\"Std. Error:   {self.se_:.4f}\")\n        print(f\"95% CI:       [{self.ci_[0]:.4f}, {self.ci_[1]:.4f}]\")\n        print(f\"P-value:      {self.pvalue_:.4f}\")\n        print(f\"Significant:  {'Yes' if self.pvalue_ < 0.05 else 'No'} (α=0.05)\")\n\nprint(\"DoubleMachineLearning class defined ✓\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "n = 2000\n",
    "p = 20\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.randn(n, p)\n",
    "\n",
    "# Nonlinear nuisance functions\n",
    "m0 = X[:, 0]**2 + X[:, 1]**2 + 0.5 * X[:, 2] * X[:, 3]\n",
    "g0 = np.sin(X[:, 0] * np.pi) + np.exp(X[:, 1] / 2) + X[:, 2]**2\n",
    "\n",
    "true_tau = 2.5\n",
    "T = m0 + np.random.normal(0, 1, n)\n",
    "Y = true_tau * T + g0 + np.random.normal(0, 1, n)\n",
    "\n",
    "print(f\"Generated data: n={n}, p={p}\")\n",
    "print(f\"True τ = {true_tau}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DML\n",
    "dml = DoubleMachineLearning(n_folds=5, random_state=42)\n",
    "dml.fit(Y, T, X)\n",
    "dml.summary()\n",
    "\n",
    "print(f\"\\nTrue τ = {true_tau}\")\n",
    "print(f\"Bias: {dml.tau_ - true_tau:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to naive OLS and naive ML\n",
    "print(\"Comparison of Estimators\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Naive OLS (ignoring confounders)\n",
    "tau_naive = sm.OLS(Y, sm.add_constant(T)).fit().params[1]\n",
    "print(f\"Naive OLS (no controls):      τ̂ = {tau_naive:.3f}  (bias = {tau_naive - true_tau:+.3f})\")\n",
    "\n",
    "# OLS with linear controls\n",
    "X_full = np.column_stack([T, X])\n",
    "tau_ols_controls = sm.OLS(Y, sm.add_constant(X_full)).fit().params[1]\n",
    "print(f\"OLS with linear controls:     τ̂ = {tau_ols_controls:.3f}  (bias = {tau_ols_controls - true_tau:+.3f})\")\n",
    "\n",
    "# DML with RF\n",
    "print(f\"DML with Random Forest:       τ̂ = {dml.tau_:.3f}  (bias = {dml.tau_ - true_tau:+.3f})\")\n",
    "\n",
    "print(f\"\\nTrue τ = {true_tau}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Using EconML\n",
    "\n",
    "For production use, the `econml` library provides polished implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EconML implementation (if available)\n",
    "try:\n",
    "    from econml.dml import LinearDML\n",
    "    \n",
    "    econml_dml = LinearDML(\n",
    "        model_y=RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        model_t=RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        cv=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    econml_dml.fit(Y, T, X=X)\n",
    "    \n",
    "    tau_econml = econml_dml.effect(X).mean()\n",
    "    ci_econml = econml_dml.effect_interval(X)\n",
    "    \n",
    "    print(\"EconML LinearDML Results\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ATE:     {tau_econml:.4f}\")\n",
    "    print(f\"95% CI:  [{ci_econml[0].mean():.4f}, {ci_econml[1].mean():.4f}]\")\n",
    "    print(f\"\\nMatches our implementation: {np.isclose(tau_econml, dml.tau_, rtol=0.05)}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"EconML not installed. Install with: pip install econml\")\n",
    "    print(\"Our from-scratch implementation works identically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Variance Estimation\n",
    "\n",
    "### The Influence Function\n",
    "\n",
    "DML achieves **asymptotic normality**:\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\tau} - \\tau_0) \\xrightarrow{d} N(0, \\Sigma)$$\n",
    "\n",
    "The variance $\\Sigma$ is estimated using the **influence function**:\n",
    "\n",
    "$$\\psi_i = \\frac{\\tilde{T}_i (\\tilde{Y}_i - \\hat{\\tau} \\tilde{T}_i)}{E[\\tilde{T}^2]}$$\n",
    "\n",
    "Then:\n",
    "$$\\hat{\\text{Var}}(\\hat{\\tau}) = \\frac{1}{n} \\text{Var}(\\psi_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate score function variance estimation\nT_resid = dml.T_resid_\nY_resid = dml.Y_resid_\ntau_hat = dml.tau_\n\n# Score function (psi)\npsi = T_resid * (Y_resid - tau_hat * T_resid) / np.mean(T_resid**2)\n\n# Variance estimate\nvar_tau = np.var(psi, ddof=1) / len(psi)\nse_tau = np.sqrt(var_tau)\n\nprint(\"Score Function Analysis\")\nprint(\"=\" * 50)\nprint(f\"Mean ψ:  {np.mean(psi):.6f}  (should be ≈ 0)\")\nprint(f\"Var ψ:   {np.var(psi):.4f}\")\nprint(f\"SE(τ̂):  {se_tau:.4f}\")\n\n# Visualize score function\nfig, axes = create_tufte_figure(1, 2, figsize=(12, 4))\n\nax = axes[0]\nax.hist(psi, bins=50, alpha=0.7, color=COLORS['blue'], edgecolor='white')\nax.axvline(0, c='black', ls='--', lw=2)\nax.set_xlabel('ψᵢ (score function)')\nax.set_ylabel('Frequency')\nax.set_title('(a) Distribution of Score Function', fontweight='bold')\n\nax = axes[1]\nax.scatter(T_resid, psi, alpha=0.3, s=10, c=COLORS['blue'])\nax.axhline(0, c='black', ls='--', lw=1)\nax.set_xlabel('T̃ (treatment residual)')\nax.set_ylabel('ψᵢ')\nax.set_title('(b) Score vs Treatment Residual', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Comparing ML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Compare different ML methods for nuisance\ndef compare_ml_methods(Y, T, X, true_tau, n_sims=10):\n    \"\"\"\n    Compare DML with different ML methods for nuisance.\n    \n    Note: n_sims kept small for execution speed.\n    Increase for more precise bias/variance estimates.\n    \"\"\"\n    \n    methods = {\n        'Lasso': Lasso(alpha=0.1),\n        'Ridge': Ridge(alpha=1.0),\n        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42),\n    }\n    \n    results = []\n    \n    for name, model in methods.items():\n        estimates = []\n        \n        for sim in range(n_sims):\n            np.random.seed(sim)\n            \n            # Resample with replacement\n            idx = np.random.choice(len(Y), len(Y), replace=True)\n            Y_b, T_b, X_b = Y[idx], T[idx], X[idx]\n            \n            dml = DoubleMachineLearning(model_Y=clone(model), model_T=clone(model), n_folds=5)\n            dml.fit(Y_b, T_b, X_b)\n            estimates.append(dml.tau_)\n        \n        estimates = np.array(estimates)\n        results.append({\n            'Method': name,\n            'Mean τ̂': np.mean(estimates),\n            'Bias': np.mean(estimates) - true_tau,\n            'SD': np.std(estimates),\n            'RMSE': np.sqrt(np.mean((estimates - true_tau)**2))\n        })\n    \n    return pd.DataFrame(results)\n\n# Run comparison (n_sims=10 for speed; increase for more precision)\nprint(\"Comparing ML Methods for Nuisance (10 bootstrap samples)...\")\ncomparison_df = compare_ml_methods(Y, T, X, true_tau, n_sims=10)\nprint(\"\\nResults:\")\nprint(comparison_df.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 5))\n",
    "\n",
    "colors_list = [COLORS['orange'], COLORS['purple'], COLORS['green'], COLORS['red']]\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Bias'].abs(), width, \n",
    "               label='|Bias|', color=COLORS['red'], alpha=0.7)\n",
    "bars2 = ax.bar(x + width/2, comparison_df['SD'], width, \n",
    "               label='Std Dev', color=COLORS['blue'], alpha=0.7)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Method'], rotation=15, ha='right')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('DML Performance by ML Method', fontweight='bold')\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Diagnostics\n",
    "\n",
    "### First-Stage R²\n",
    "\n",
    "Check that nuisance models explain reasonable variation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dml_diagnostics(Y, T, X, model_Y, model_T, n_folds=5):\n",
    "    \"\"\"Compute DML diagnostics.\"\"\"\n",
    "    \n",
    "    # Get cross-validated predictions\n",
    "    Y_pred = cross_val_predict(model_Y, X, Y, cv=n_folds)\n",
    "    T_pred = cross_val_predict(model_T, X, T, cv=n_folds)\n",
    "    \n",
    "    # R² for each nuisance\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2_Y = r2_score(Y, Y_pred)\n",
    "    r2_T = r2_score(T, T_pred)\n",
    "    \n",
    "    # Residual variance\n",
    "    var_Y_resid = np.var(Y - Y_pred)\n",
    "    var_T_resid = np.var(T - T_pred)\n",
    "    \n",
    "    print(\"DML Diagnostics\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nFirst-Stage Performance:\")\n",
    "    print(f\"  R²(Y|X): {r2_Y:.3f}  (explains {r2_Y*100:.1f}% of Y variance)\")\n",
    "    print(f\"  R²(T|X): {r2_T:.3f}  (explains {r2_T*100:.1f}% of T variance)\")\n",
    "    print(f\"\\nResidual Variances:\")\n",
    "    print(f\"  Var(Ỹ): {var_Y_resid:.3f}\")\n",
    "    print(f\"  Var(T̃): {var_T_resid:.3f}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    if r2_T < 0.1:\n",
    "        print(f\"  ⚠ Low R²(T|X) suggests weak confounding. Consider whether DML is needed.\")\n",
    "    if r2_Y < 0.1:\n",
    "        print(f\"  ⚠ Low R²(Y|X) suggests X explains little of Y. Check covariate set.\")\n",
    "    if r2_T > 0.1 and r2_Y > 0.1:\n",
    "        print(f\"  ✓ Reasonable first-stage performance.\")\n",
    "    \n",
    "    return {'r2_Y': r2_Y, 'r2_T': r2_T}\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "diagnostics = dml_diagnostics(Y, T, X, rf, rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. When to Use DML\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "```\n",
    "           Is treatment randomized?\n",
    "                    |\n",
    "          +---------+---------+\n",
    "          |                   |\n",
    "         YES                  NO\n",
    "          |                   |\n",
    "   Use simple               Are confounders\n",
    "   diff-in-means            observed?\n",
    "                              |\n",
    "                    +---------+---------+\n",
    "                    |                   |\n",
    "                   YES                  NO\n",
    "                    |                   |\n",
    "             Is relationship      Use IV, RDD, or\n",
    "             linear in X?         other methods\n",
    "                    |\n",
    "          +---------+---------+\n",
    "          |                   |\n",
    "         YES                  NO\n",
    "          |                   |\n",
    "    Use OLS with           Use DML\n",
    "    controls\n",
    "```\n",
    "\n",
    "### When DML Shines\n",
    "\n",
    "- **High-dimensional confounders** (p large)\n",
    "- **Nonlinear confounding** (complex X → Y, X → T relationships)\n",
    "- **Observational data** with selection on observables\n",
    "- **Want valid inference** (CI, p-values)\n",
    "\n",
    "### When to Use Alternatives\n",
    "\n",
    "| Situation | Preferred Method |\n",
    "|-----------|------------------|\n",
    "| Randomized trial | Simple diff-in-means |\n",
    "| Linear, low-dim | OLS with controls |\n",
    "| Unobserved confounding | IV, RDD, DiD |\n",
    "| Small sample (n < 500) | Regularized regression |\n",
    "| Heterogeneous effects | CausalForestDML (Ch 22) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Bridge to Chapter 22\n",
    "\n",
    "This notebook covered the **Average Treatment Effect (ATE)**. Chapter 22 extends to:\n",
    "\n",
    "### Conditional Average Treatment Effects (CATE)\n",
    "\n",
    "$$\\tau(x) = E[Y(1) - Y(0) | X = x]$$\n",
    "\n",
    "- **CausalForestDML**: Combines DML with causal forests\n",
    "- **R-learner**: Alternative formulation using Robinson transformation\n",
    "\n",
    "### Meta-Learners\n",
    "\n",
    "- **S-learner**: Single model for $E[Y|X,T]$\n",
    "- **T-learner**: Separate models for treatment/control\n",
    "- **X-learner**: Impute counterfactuals, then average\n",
    "\n",
    "### Advanced Topics\n",
    "\n",
    "- **Dynamic treatment regimes**\n",
    "- **Sensitivity analysis**\n",
    "- **Policy learning**\n",
    "\n",
    "```\n",
    "★ Forward Reference ─────────────────────────────────────────\n",
    "  \n",
    "  Chapter 22 covers:\n",
    "  - CATE estimation with heterogeneous effects\n",
    "  - CausalForestDML implementation\n",
    "  - Policy optimization\n",
    "  \n",
    "  The foundations from this Part 2 carry over directly.\n",
    "─────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Interview Appendix\n",
    "\n",
    "### Comprehensive Interview Questions\n",
    "\n",
    "---\n",
    "\n",
    "**Q1 (Google L5, Data Scientist)**: *\"When would you use DML vs traditional regression?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Use DML when:**\n",
    "\n",
    "1. **High-dimensional confounders**: Many covariates (p ≈ n or p > n)\n",
    "2. **Nonlinear confounding**: E[T|X] or E[Y|X] are nonlinear\n",
    "3. **Flexible adjustment needed**: Can't specify correct functional form\n",
    "4. **Want valid inference**: CI and p-values that account for ML uncertainty\n",
    "\n",
    "**Use traditional regression when:**\n",
    "\n",
    "1. **Low-dimensional, linear**: Few confounders, relationships are linear\n",
    "2. **Randomized experiment**: No confounding, simple diff-in-means works\n",
    "3. **Small sample**: DML needs reasonable n for ML to work well\n",
    "4. **Coefficient interpretation**: Need interpretable covariate effects\n",
    "\n",
    "**Key insight**: DML is **robust to model misspecification** in the nuisance (E[Y|X], E[T|X]), while OLS requires correct specification.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Meta IC5, Economist)**: *\"What is the intuition behind cross-fitting in DML?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**The problem without cross-fitting:**\n",
    "\n",
    "When we use the **same data** to:\n",
    "1. Train nuisance models (m̂, ℓ̂)\n",
    "2. Compute residuals (T̃, Ỹ)\n",
    "\n",
    "The models **overfit** to training noise, making residuals too small and distorting correlations.\n",
    "\n",
    "**The cross-fitting solution:**\n",
    "\n",
    "For each observation i in fold k:\n",
    "- Train nuisance on **other folds** (not fold k)\n",
    "- Predict on fold k (out-of-sample)\n",
    "- The model cannot have \"memorized\" observation i's noise\n",
    "\n",
    "**Result**: Prediction errors are **independent** of true noise, restoring valid inference.\n",
    "\n",
    "**Analogy**: It's like cross-validation for prediction, but applied to causal inference.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Amazon, Applied Scientist)**: *\"How do you validate a DML implementation?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Validation checklist:**\n",
    "\n",
    "1. **Simulation study** (most important):\n",
    "   - Generate data with **known τ**\n",
    "   - Run DML many times (Monte Carlo)\n",
    "   - Check: Bias ≈ 0, Coverage ≈ 95%\n",
    "\n",
    "2. **First-stage diagnostics**:\n",
    "   - R²(T|X): How much of T is explained by X?\n",
    "   - R²(Y|X): How much of Y is explained by X?\n",
    "   - If R² too low, confounding may be weak\n",
    "\n",
    "3. **Residual checks**:\n",
    "   - T̃ should be uncorrelated with X (by construction)\n",
    "   - No remaining pattern in residuals\n",
    "\n",
    "4. **Sensitivity analysis**:\n",
    "   - Vary K (number of folds)\n",
    "   - Try different ML methods\n",
    "   - Results should be stable\n",
    "\n",
    "5. **Comparison to benchmarks**:\n",
    "   - Compare to OLS with controls\n",
    "   - Compare to other causal methods (IPW, etc.)\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q4 (Two Sigma, Quant)**: *\"Derive the influence function for the DML estimator.\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Setup**: DML solves the moment condition\n",
    "$$E[\\psi(W; \\tau, \\eta_0)] = E[\\tilde{T}(\\tilde{Y} - \\tau\\tilde{T})] = 0$$\n",
    "\n",
    "where $\\tilde{T} = T - m_0(X)$ and $\\tilde{Y} = Y - \\ell_0(X)$.\n",
    "\n",
    "**Derivation**:\n",
    "\n",
    "At the true value τ₀:\n",
    "$$0 = E[\\tilde{T}(\\tilde{Y} - \\tau_0\\tilde{T})]$$\n",
    "\n",
    "Taking a Taylor expansion around $\\hat{\\tau}$:\n",
    "$$0 \\approx \\frac{1}{n}\\sum_i \\tilde{T}_i(\\tilde{Y}_i - \\hat{\\tau}\\tilde{T}_i) + (\\tau_0 - \\hat{\\tau}) \\cdot \\frac{1}{n}\\sum_i \\tilde{T}_i^2$$\n",
    "\n",
    "Rearranging:\n",
    "$$\\hat{\\tau} - \\tau_0 = \\frac{\\frac{1}{n}\\sum_i \\tilde{T}_i(\\tilde{Y}_i - \\tau_0\\tilde{T}_i)}{E[\\tilde{T}^2]}$$\n",
    "\n",
    "The **influence function** is:\n",
    "$$\\psi_i = \\frac{\\tilde{T}_i(\\tilde{Y}_i - \\tau_0\\tilde{T}_i)}{E[\\tilde{T}^2]}$$\n",
    "\n",
    "**Variance estimator**:\n",
    "$$\\hat{\\text{Var}}(\\hat{\\tau}) = \\frac{1}{n} \\text{Var}(\\psi_i)$$\n",
    "\n",
    "This is the Eicker-Huber-White heteroskedasticity-robust variance.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q5 (Citadel, ML Researcher)**: *\"What are the key assumptions for DML?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Identification Assumptions** (for causal interpretation):\n",
    "\n",
    "1. **Conditional Ignorability (Unconfoundedness)**:\n",
    "   $$(Y(0), Y(1)) \\perp T | X$$\n",
    "   All confounders are observed and included in X.\n",
    "\n",
    "2. **Overlap (Positivity)**:\n",
    "   $$0 < P(T=1|X) < 1$$\n",
    "   Treatment not deterministic given X.\n",
    "\n",
    "**Statistical Assumptions** (for valid inference):\n",
    "\n",
    "3. **Partial Linear Model**:\n",
    "   $$Y = \\tau T + g_0(X) + \\varepsilon$$\n",
    "   The effect τ is constant (homogeneous).\n",
    "\n",
    "4. **Nuisance Convergence Rates**:\n",
    "   $$\\|\\hat{m} - m_0\\| \\cdot \\|\\hat{\\ell} - \\ell_0\\| = o_p(n^{-1/2})$$\n",
    "   Product of nuisance errors shrinks fast enough.\n",
    "\n",
    "5. **Regularity Conditions**:\n",
    "   - Bounded moments\n",
    "   - Non-zero treatment variation conditional on X\n",
    "\n",
    "**Key insight**: The orthogonality property means we only need the **product** of nuisance errors to be small, not each one individually. This is why n^{-1/4} rates suffice.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q6 (Netflix, Senior DS)**: *\"How does the choice of ML model affect DML results?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Model flexibility vs sample size tradeoff:**\n",
    "\n",
    "| Model | Pros | Cons |\n",
    "|-------|------|------|\n",
    "| Lasso | Fast, interpretable | Only linear, may miss nonlinearity |\n",
    "| Ridge | Stable | Only linear |\n",
    "| Random Forest | Handles nonlinearity | Can overfit, slower |\n",
    "| Gradient Boosting | Most flexible | Computationally expensive |\n",
    "| Neural Network | Universal approximator | Needs lots of data, hard to tune |\n",
    "\n",
    "**Practical recommendations:**\n",
    "\n",
    "1. **Default**: Random Forest with regularization (max_depth, min_samples_leaf)\n",
    "2. **High-dim, sparse**: Lasso or elastic net\n",
    "3. **Complex nonlinearity**: Gradient Boosting\n",
    "4. **Very large n**: Neural networks\n",
    "\n",
    "**Robustness check**: Always try multiple models. If results differ substantially, investigate why.\n",
    "\n",
    "**Key insight**: DML is robust to **moderate** model misspecification due to orthogonality. But if nuisance estimation is very poor, results will be biased.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 10. References\n",
    "\n",
    "[^1]: Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^2]: Robinson, P. M. (1988). Root-N-Consistent Semiparametric Regression. *Econometrica*, 56(4), 931-954.\n",
    "\n",
    "[^3]: Nie, X. and Wager, S. (2021). Quasi-Oracle Estimation of Heterogeneous Treatment Effects. *Biometrika*, 108(2), 299-319.\n",
    "\n",
    "[^4]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapters 5 & 22.\n",
    "\n",
    "[^5]: Ruiz de Villa, A. (2024). *Causal Inference for Data Science*. Manning Publications.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Part 2: Linear Regression → Double ML Bridge**\n",
    "\n",
    "Continue to **Chapter 22** for advanced DML topics: CATE, heterogeneity, and policy learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}