{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 22.2 DML Estimation: Cross-Fitting and Inference\n",
    "\n",
    "**Chapter**: 22 - Debiased/Double Machine Learning  \n",
    "**Section**: 2 - Estimation Mechanics  \n",
    "**Facure Source**: 22-Debiased-Orthogonal-Machine-Learning.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [The Cross-Fitting Algorithm](#11-the-cross-fitting-algorithm)\n",
    "   - 1.2 [Why Not Just Sample Split?](#12-why-not-just-sample-split)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [The DML2 Estimator](#21-the-dml2-estimator)\n",
    "   - 2.2 [Variance Estimation](#22-variance-estimation)\n",
    "   - 2.3 [Repeated Cross-Fitting](#23-repeated-cross-fitting)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [Cross-Fitting Step by Step](#31-cross-fitting-step-by-step)\n",
    "   - 3.2 [Inference and Confidence Intervals](#32-inference-and-confidence-intervals)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm, smf,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    ols_summary_table,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats\n",
    "\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: Understanding *why* cross-fitting works shows deep understanding of the DML framework. It's the key to combining flexible ML with valid causal inference.\n",
    "\n",
    "### 1.1 The Cross-Fitting Algorithm\n",
    "\n",
    "Facure's insight: **Use different data for training ML models vs. computing treatment effects.**\n",
    "\n",
    "**Cross-fitting procedure**:\n",
    "1. Split data into $K$ folds (typically $K=5$ or $K=10$)\n",
    "2. For each fold $k$:\n",
    "   - Train outcome model $\\hat{g}_{-k}$ on all data *except* fold $k$\n",
    "   - Train treatment model $\\hat{m}_{-k}$ on all data *except* fold $k$\n",
    "   - Compute residuals for fold $k$:\n",
    "     - $\\tilde{Y}_i = Y_i - \\hat{g}_{-k}(X_i)$ for $i \\in \\text{fold } k$\n",
    "     - $\\tilde{T}_i = T_i - \\hat{m}_{-k}(X_i)$ for $i \\in \\text{fold } k$\n",
    "3. Pool all residuals\n",
    "4. Estimate: $\\hat{\\theta} = \\frac{\\sum_i \\tilde{T}_i \\tilde{Y}_i}{\\sum_i \\tilde{T}_i^2}$\n",
    "\n",
    "### 1.2 Why Not Just Sample Split?\n",
    "\n",
    "**Simple sample splitting**: Use half for nuisance, half for estimation.\n",
    "\n",
    "**Problems**:\n",
    "- Wastes half the data for estimation → larger standard errors\n",
    "- Results depend on which half you use → not reproducible\n",
    "\n",
    "**Cross-fitting advantages**:\n",
    "- Uses ALL data for both nuisance and estimation\n",
    "- Symmetric treatment of observations\n",
    "- More efficient (smaller variance)\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- Cross-fitting = \"cross-validation for causal inference\"\n",
    "- Breaks overfitting correlation without wasting data\n",
    "- Each observation contributes to final estimate\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 The DML2 Estimator\n",
    "\n",
    "**DML2** (pooled estimator): The recommended DML variant.\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "1. **Partition** data into $K$ folds: $\\{I_1, ..., I_K\\}$\n",
    "\n",
    "2. **For each** $k = 1, ..., K$:\n",
    "   - Let $I_k^c = \\{1, ..., n\\} \\setminus I_k$ (complement)\n",
    "   - Fit $\\hat{g}_k$ on $\\{(X_i, Y_i) : i \\in I_k^c\\}$\n",
    "   - Fit $\\hat{m}_k$ on $\\{(X_i, T_i) : i \\in I_k^c\\}$\n",
    "   - Compute: $\\tilde{Y}_i = Y_i - \\hat{g}_k(X_i)$ for $i \\in I_k$\n",
    "   - Compute: $\\tilde{T}_i = T_i - \\hat{m}_k(X_i)$ for $i \\in I_k$\n",
    "\n",
    "3. **Pool** all residuals and estimate:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{DML2}} = \\left(\\frac{1}{n} \\sum_{i=1}^n \\tilde{T}_i^2\\right)^{-1} \\left(\\frac{1}{n} \\sum_{i=1}^n \\tilde{T}_i \\tilde{Y}_i\\right)$$\n",
    "\n",
    "### 2.2 Variance Estimation\n",
    "\n",
    "**Asymptotic variance** of $\\hat{\\theta}_{\\text{DML2}}$:\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\xrightarrow{d} N(0, V)$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$V = \\frac{E[(Y - g_0(X) - \\theta_0 T)^2 (T - m_0(X))^2]}{(E[(T - m_0(X))^2])^2}$$\n",
    "\n",
    "**Estimator**:\n",
    "\n",
    "$$\\hat{V} = \\frac{\\frac{1}{n}\\sum_i (\\tilde{Y}_i - \\hat{\\theta} \\tilde{T}_i)^2 \\tilde{T}_i^2}{\\left(\\frac{1}{n}\\sum_i \\tilde{T}_i^2\\right)^2}$$\n",
    "\n",
    "$$\\hat{\\text{SE}}(\\hat{\\theta}) = \\sqrt{\\hat{V}/n}$$\n",
    "\n",
    "### 2.3 Repeated Cross-Fitting\n",
    "\n",
    "**Issue**: Results can depend on the random fold assignment.\n",
    "\n",
    "**Solution**: Repeat cross-fitting $R$ times with different random splits.\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{median}} = \\text{median}(\\hat{\\theta}^{(1)}, ..., \\hat{\\theta}^{(R)})$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{mean}} = \\frac{1}{R} \\sum_{r=1}^R \\hat{\\theta}^{(r)}$$\n",
    "\n",
    "Standard errors are computed accounting for all repetitions.\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- DML2 pools residuals across folds (recommended)\n",
    "- DML1 (alternative) averages fold-specific estimates\n",
    "- Repeated cross-fitting reduces sensitivity to fold assignment\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 Cross-Fitting Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ice_cream = load_facure_data('ice_cream_sales.csv')\n",
    "ice_cream_rnd = load_facure_data('ice_cream_sales_rnd.csv')\n",
    "\n",
    "X = ice_cream[['temp', 'weekday', 'cost']].values\n",
    "T = ice_cream['price'].values\n",
    "Y = ice_cream['sales'].values\n",
    "\n",
    "# True effect from RCT\n",
    "true_effect = smf.ols('sales ~ price', data=ice_cream_rnd).fit().params['price']\n",
    "\n",
    "print(f\"Sample size: n = {len(Y)}\")\n",
    "print(f\"True effect (from RCT): {true_effect:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dml2_detailed(Y, T, X, ml_model_class, n_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    DML2 with detailed fold-by-fold output.\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Initialize residual arrays\n",
    "    Y_res = np.zeros(n)\n",
    "    T_res = np.zeros(n)\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "        T_train, T_test = T[train_idx], T[test_idx]\n",
    "        \n",
    "        # Fit models on training fold\n",
    "        model_y = ml_model_class(random_state=random_state)\n",
    "        model_t = ml_model_class(random_state=random_state)\n",
    "        \n",
    "        model_y.fit(X_train, Y_train)\n",
    "        model_t.fit(X_train, T_train)\n",
    "        \n",
    "        # Predict on test fold\n",
    "        Y_pred = model_y.predict(X_test)\n",
    "        T_pred = model_t.predict(X_test)\n",
    "        \n",
    "        # Compute residuals\n",
    "        Y_res[test_idx] = Y_test - Y_pred\n",
    "        T_res[test_idx] = T_test - T_pred\n",
    "        \n",
    "        # Fold-specific estimate (for illustration)\n",
    "        fold_theta = np.sum(T_res[test_idx] * Y_res[test_idx]) / np.sum(T_res[test_idx]**2)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'n_train': len(train_idx),\n",
    "            'n_test': len(test_idx),\n",
    "            'r2_y': model_y.score(X_test, Y_test),\n",
    "            'r2_t': model_t.score(X_test, T_test),\n",
    "            'theta_fold': fold_theta\n",
    "        })\n",
    "    \n",
    "    # Pool all residuals for DML2 estimate\n",
    "    theta_dml2 = np.sum(T_res * Y_res) / np.sum(T_res**2)\n",
    "    \n",
    "    # Variance estimation\n",
    "    psi = Y_res - theta_dml2 * T_res  # Final residuals\n",
    "    var_theta = np.sum(psi**2 * T_res**2) / (np.sum(T_res**2))**2\n",
    "    se_theta = np.sqrt(var_theta)\n",
    "    \n",
    "    return {\n",
    "        'theta': theta_dml2,\n",
    "        'se': se_theta,\n",
    "        'ci_lower': theta_dml2 - 1.96 * se_theta,\n",
    "        'ci_upper': theta_dml2 + 1.96 * se_theta,\n",
    "        'fold_results': pd.DataFrame(fold_results),\n",
    "        'Y_residual': Y_res,\n",
    "        'T_residual': T_res\n",
    "    }\n",
    "\n",
    "# Run DML2 with Gradient Boosting\n",
    "result = dml2_detailed(\n",
    "    Y, T, X, \n",
    "    lambda **kwargs: GradientBoostingRegressor(n_estimators=100, max_depth=3, **kwargs),\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "print(\"\\nCROSS-FITTING RESULTS BY FOLD\")\n",
    "print(\"=\" * 70)\n",
    "print(result['fold_results'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDML2 POOLED ESTIMATE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Estimate: {result['theta']:.4f}\")\n",
    "print(f\"SE: {result['se']:.4f}\")\n",
    "print(f\"95% CI: [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")\n",
    "print(f\"True effect: {true_effect:.4f}\")\n",
    "print(f\"Bias: {result['theta'] - true_effect:.4f}\")\n",
    "print(f\"\\nCI covers true value: {result['ci_lower'] < true_effect < result['ci_upper']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-fitting\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Fold-specific estimates\n",
    "ax = axes[0]\n",
    "folds = result['fold_results']['fold']\n",
    "fold_thetas = result['fold_results']['theta_fold']\n",
    "\n",
    "ax.bar(folds, fold_thetas, color=TUFTE_PALETTE['secondary'], \n",
    "       edgecolor='white', linewidth=1.5, alpha=0.7)\n",
    "ax.axhline(result['theta'], color=TUFTE_PALETTE['effect'], linewidth=2.5,\n",
    "           linestyle='-', label=f'Pooled DML2: {result[\"theta\"]:.4f}')\n",
    "ax.axhline(true_effect, color=TUFTE_PALETTE['treatment'], linewidth=2,\n",
    "           linestyle='--', label=f'True effect: {true_effect:.4f}')\n",
    "\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('Fold-Specific Estimate')\n",
    "ax.set_title('(a) Cross-Fitting: Fold-Specific vs Pooled')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: R² by fold\n",
    "ax = axes[1]\n",
    "width = 0.35\n",
    "x = np.array(folds)\n",
    "ax.bar(x - width/2, result['fold_results']['r2_y'], width, \n",
    "       color=TUFTE_PALETTE['control'], label='Outcome model (Y)', alpha=0.8)\n",
    "ax.bar(x + width/2, result['fold_results']['r2_t'], width, \n",
    "       color=TUFTE_PALETTE['treatment'], label='Treatment model (T)', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('R² (out-of-fold)')\n",
    "ax.set_title('(b) Nuisance Model Performance')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 3.2 Inference and Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated cross-fitting for stability\n",
    "def dml2_repeated(Y, T, X, ml_model_class, n_folds=5, n_rep=20):\n",
    "    \"\"\"DML2 with repeated cross-fitting.\"\"\"\n",
    "    estimates = []\n",
    "    \n",
    "    for rep in range(n_rep):\n",
    "        result = dml2_detailed(\n",
    "            Y, T, X, ml_model_class,\n",
    "            n_folds=n_folds,\n",
    "            random_state=rep * 42\n",
    "        )\n",
    "        estimates.append(result['theta'])\n",
    "    \n",
    "    return {\n",
    "        'estimates': estimates,\n",
    "        'mean': np.mean(estimates),\n",
    "        'median': np.median(estimates),\n",
    "        'std': np.std(estimates),\n",
    "        'q025': np.percentile(estimates, 2.5),\n",
    "        'q975': np.percentile(estimates, 97.5)\n",
    "    }\n",
    "\n",
    "# Run repeated cross-fitting\n",
    "rep_result = dml2_repeated(\n",
    "    Y, T, X,\n",
    "    lambda **kwargs: GradientBoostingRegressor(n_estimators=100, max_depth=3, **kwargs),\n",
    "    n_folds=5,\n",
    "    n_rep=20\n",
    ")\n",
    "\n",
    "print(\"REPEATED CROSS-FITTING (20 repetitions)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean estimate: {rep_result['mean']:.4f}\")\n",
    "print(f\"Median estimate: {rep_result['median']:.4f}\")\n",
    "print(f\"Std across reps: {rep_result['std']:.4f}\")\n",
    "print(f\"Range: [{min(rep_result['estimates']):.4f}, {max(rep_result['estimates']):.4f}]\")\n",
    "print(f\"True effect: {true_effect:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different ML models\n",
    "models = {\n",
    "    'GradientBoosting': lambda **kw: GradientBoostingRegressor(n_estimators=100, max_depth=3, **kw),\n",
    "    'RandomForest': lambda **kw: RandomForestRegressor(n_estimators=100, max_depth=5, **kw),\n",
    "}\n",
    "\n",
    "print(\"\\nDML2 WITH DIFFERENT ML MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_results = {}\n",
    "for name, model_class in models.items():\n",
    "    res = dml2_detailed(Y, T, X, model_class, n_folds=5)\n",
    "    model_results[name] = res\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Estimate: {res['theta']:.4f} (SE: {res['se']:.4f})\")\n",
    "    print(f\"  95% CI: [{res['ci_lower']:.4f}, {res['ci_upper']:.4f}]\")\n",
    "    print(f\"  Bias: {res['theta'] - true_effect:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stability and model comparison\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Distribution across repetitions\n",
    "ax = axes[0]\n",
    "ax.hist(rep_result['estimates'], bins=10, color=TUFTE_PALETTE['secondary'], \n",
    "        edgecolor='white', alpha=0.7)\n",
    "ax.axvline(true_effect, color=TUFTE_PALETTE['treatment'], linewidth=2.5,\n",
    "           linestyle='--', label=f'True: {true_effect:.4f}')\n",
    "ax.axvline(rep_result['mean'], color=TUFTE_PALETTE['effect'], linewidth=2.5,\n",
    "           linestyle='-', label=f'Mean: {rep_result[\"mean\"]:.4f}')\n",
    "\n",
    "ax.set_xlabel('DML2 Estimate')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('(a) Distribution Across Repetitions')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: Model comparison\n",
    "ax = axes[1]\n",
    "names = list(model_results.keys())\n",
    "estimates = [model_results[n]['theta'] for n in names]\n",
    "errors = [1.96 * model_results[n]['se'] for n in names]\n",
    "\n",
    "y_pos = np.arange(len(names))\n",
    "ax.barh(y_pos, estimates, xerr=errors, color=TUFTE_PALETTE['effect'], \n",
    "        capsize=5, alpha=0.8)\n",
    "ax.axvline(true_effect, color=TUFTE_PALETTE['treatment'], linewidth=2.5,\n",
    "           linestyle='--', label=f'True: {true_effect:.4f}')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(names)\n",
    "ax.set_xlabel('DML2 Estimate (with 95% CI)')\n",
    "ax.set_title('(b) ML Model Comparison')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "**Key observations**:\n\n1. **Repeated cross-fitting**: Reduces sensitivity to random fold assignment\n2. **Model choice**: Different ML models give similar results (robustness)\n3. **Coverage**: 95% CIs contain the true effect (valid inference)\n\n---\n\n## Additional Content: Nuisance Model Diagnostics\n\n### First-Stage R² Interpretation\n\nThe R² of nuisance models ($\\hat{g}(X)$ for outcome, $\\hat{m}(X)$ for treatment) provides crucial diagnostics:\n\n**Treatment model R² ($\\hat{m}(X)$)**:\n| R² Range | Interpretation | Action |\n|----------|----------------|--------|\n| < 0.05 | Treatment nearly random given X | DML adds little over naive regression |\n| 0.05 - 0.20 | Moderate confounding | DML provides meaningful adjustment |\n| 0.20 - 0.50 | Strong confounding | DML is valuable; check overlap |\n| > 0.50 | Very strong confounding | Watch for positivity violations |\n\n**Outcome model R² ($\\hat{g}(X)$)**:\n| R² Range | Interpretation | Action |\n|----------|----------------|--------|\n| < 0.10 | X explains little of Y | Large residual variance; wider CIs |\n| 0.10 - 0.40 | Moderate explanatory power | Standard DML benefits |\n| > 0.40 | Strong explanatory power | DML provides precision gains |\n\n### When DML May Fail\n\n**Scenario 1: Poor overlap (positivity violation)**\n- If $\\hat{m}(X) \\approx 0$ or $\\approx 1$ for some $X$\n- Residualized treatment $\\tilde{T} = T - \\hat{m}(X)$ has near-zero variance in regions\n- Solution: Trim extreme propensity scores or restrict to overlap region\n\n**Scenario 2: Treatment model too good (near-determinism)**\n- If treatment is almost perfectly predictable from X\n- $\\text{Var}(\\tilde{T})$ is tiny → $\\hat{\\theta}$ has huge variance\n- This is honest—it says \"no variation to learn from\"\n\n**Scenario 3: Both models perform poorly**\n- Both R² < 0.05 suggests X doesn't capture confounding\n- DML won't save you if X doesn't include confounders\n- Revisit variable selection or consider selection bias remains\n\n### Diagnostic Checklist\n\n```python\ndef dml_diagnostics(result):\n    \"\"\"Check DML health before trusting results.\"\"\"\n    issues = []\n    \n    # 1. Nuisance model quality\n    if result.r2_t < 0.05:\n        issues.append(\"⚠️ Treatment model R² < 5%: X may not explain treatment assignment\")\n    if result.r2_y < 0.10:\n        issues.append(\"⚠️ Outcome model R² < 10%: Limited precision gains from DML\")\n    \n    # 2. Overlap check\n    ps_range = result.propensity_scores.max() - result.propensity_scores.min()\n    if ps_range < 0.3:\n        issues.append(\"⚠️ Narrow propensity score range: Limited overlap\")\n    \n    # 3. Residualized treatment variance\n    t_res_var = np.var(result.T_residual)\n    if t_res_var < 0.01:\n        issues.append(\"⚠️ Low residualized treatment variance: Estimate may be unstable\")\n    \n    # 4. Fold stability\n    fold_std = np.std(result.fold_estimates)\n    if fold_std > 0.5 * abs(result.theta):\n        issues.append(\"⚠️ High fold-to-fold variation: Consider more stable ML models\")\n    \n    return issues\n```\n\n★ Insight ─────────────────────────────────────\n- Report R² for both nuisance models—it's essential context\n- Low treatment R² isn't bad; it means less confounding to adjust for\n- Very high treatment R² suggests overlap concerns\n- DML can't fix missing confounders—only adjusts for observed X\n─────────────────────────────────────────────────"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "The `causal_inference_mastery` library provides DML:\n",
    "\n",
    "```python\n",
    "from causal_inference.dml import DML2, DMLResult\n",
    "\n",
    "# DML2 with repeated cross-fitting\n",
    "dml = DML2(\n",
    "    model_y=GradientBoostingRegressor(n_estimators=100),\n",
    "    model_t=GradientBoostingRegressor(n_estimators=100),\n",
    "    n_folds=5,\n",
    "    n_rep=10\n",
    ")\n",
    "\n",
    "result = dml.fit(Y, T, X)\n",
    "\n",
    "print(f\"Estimate: {result.theta:.4f}\")\n",
    "print(f\"SE: {result.se:.4f}\")\n",
    "print(f\"95% CI: [{result.ci_lower:.4f}, {result.ci_upper:.4f}]\")\n",
    "print(f\"Nuisance R² (Y): {result.r2_y:.3f}\")\n",
    "print(f\"Nuisance R² (T): {result.r2_t:.3f}\")\n",
    "```\n",
    "\n",
    "Production implementation: `econml.dml.LinearDML`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Meta E5, DS)**: *\"What's the difference between DML1 and DML2?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**DML1** (fold-by-fold estimation):\n",
    "- Compute treatment effect separately for each fold\n",
    "- Average the fold-specific estimates: $\\hat{\\theta}_{\\text{DML1}} = \\frac{1}{K}\\sum_{k=1}^K \\hat{\\theta}_k$\n",
    "- Each fold contributes equally to final estimate\n",
    "\n",
    "**DML2** (pooled estimation):\n",
    "- Pool all residuals across folds\n",
    "- Compute single estimate from pooled residuals\n",
    "- More efficient (smaller variance) when fold sizes differ\n",
    "\n",
    "**Comparison**:\n",
    "\n",
    "| Feature | DML1 | DML2 |\n",
    "|---------|------|------|\n",
    "| Estimation | Fold-by-fold, then average | Pool, then estimate |\n",
    "| Weighting | Equal weight per fold | Weight by fold size |\n",
    "| Efficiency | Less efficient | More efficient |\n",
    "| Recommended | When folds are equal | Generally preferred |\n",
    "\n",
    "**When DML1 is preferred**:\n",
    "- Want to diagnose fold-specific issues\n",
    "- Folds are of very different quality\n",
    "- Debugging purposes\n",
    "\n",
    "**Recommendation**: Use DML2 as default (as in Chernozhukov et al.).\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Google L5, Quant)**: *\"How do you compute standard errors in DML?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**DML standard errors** use the influence function approach.\n",
    "\n",
    "**For the partially linear model**:\n",
    "\n",
    "$$\\hat{\\text{Var}}(\\hat{\\theta}) = \\frac{1}{n} \\cdot \\frac{\\sum_i \\psi_i^2 \\tilde{T}_i^2}{(\\sum_i \\tilde{T}_i^2 / n)^2}$$\n",
    "\n",
    "where $\\psi_i = \\tilde{Y}_i - \\hat{\\theta} \\tilde{T}_i$ are the final residuals.\n",
    "\n",
    "**Derivation**:\n",
    "\n",
    "1. The orthogonal moment is $\\psi(W; \\theta) = (Y - g(X) - \\theta T)(T - m(X))$\n",
    "\n",
    "2. Influence function: $IF_i = \\frac{\\psi_i \\tilde{T}_i}{E[\\tilde{T}^2]}$\n",
    "\n",
    "3. Variance: $\\text{Var}(\\hat{\\theta}) = \\frac{1}{n} \\text{Var}(IF_i)$\n",
    "\n",
    "**With repeated cross-fitting**:\n",
    "- Compute SE for each repetition\n",
    "- Combine using: $\\text{SE}_{\\text{final}}^2 = \\text{mean}(\\text{SE}_r^2) + \\text{Var}(\\hat{\\theta}_r)$\n",
    "\n",
    "**Key point**: Standard errors account for both estimation uncertainty and cross-fitting variability.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Amazon L6, Econ)**: *\"What happens if the nuisance models perform poorly in DML?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Poor nuisance estimation affects DML in different ways**:\n",
    "\n",
    "1. **Variance inflation**:\n",
    "   - If $\\hat{m}(X)$ is poor: $\\tilde{T} = T - \\hat{m}(X)$ has high variance\n",
    "   - Lower \"effective\" signal in residualized treatment\n",
    "   - Larger standard errors for $\\hat{\\theta}$\n",
    "\n",
    "2. **Potential bias** (if really bad):\n",
    "   - Orthogonality protects against *small* nuisance errors\n",
    "   - With *large* errors, second-order bias can matter\n",
    "   - Need nuisance convergence rate > $n^{-1/4}$\n",
    "\n",
    "3. **Specific issues**:\n",
    "   - Bad $\\hat{g}(X)$: Residuals $\\tilde{Y}$ still have confounding\n",
    "   - Bad $\\hat{m}(X)$: Residuals $\\tilde{T}$ correlated with confounders\n",
    "\n",
    "**Diagnostics**:\n",
    "- Report R² for both nuisance models\n",
    "- If R² < 0.1, results may be unreliable\n",
    "- Compare results across different ML models\n",
    "\n",
    "**Solutions**:\n",
    "- Try more flexible models (deeper trees, more features)\n",
    "- Increase sample size\n",
    "- Consider if confounding is actually predictable from $X$\n",
    "- If $m(X) \\approx \\text{const}$, DML adds little beyond naive estimation\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 22: \"Debiased/Orthogonal Machine Learning.\"\n",
    "\n",
    "[^2]: Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^3]: Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., and Newey, W. (2017). Double/Debiased/Neyman Machine Learning of Treatment Effects. *American Economic Review Papers and Proceedings*, 107(5), 261-265.\n",
    "\n",
    "[^4]: Syrgkanis, V., Lei, V., Oprescu, M., Hei, M., Battocchi, K., and Lewis, G. (2019). Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments. *Advances in Neural Information Processing Systems*, 32.\n",
    "\n",
    "---\n",
    "\n",
    "**Precision Improvement:**\n",
    "- You said: \"Build DML estimation notebook\"\n",
    "- Concise: \"Build 02_dml_estimation.ipynb\"\n",
    "- Precise: `/facure_augment 22.2 --cross-fitting --dml2 --variance-estimation`\n",
    "- Pattern: [build] [target] [content-flags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}