{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 22.3 Advanced DML: R-Learner and Heterogeneous Effects\n",
    "\n",
    "**Chapter**: 22 - Debiased/Double Machine Learning  \n",
    "**Section**: 3 - Extensions and CATE  \n",
    "**Facure Source**: 22-Debiased-Orthogonal-Machine-Learning.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [From ATE to CATE](#11-from-ate-to-cate)\n",
    "   - 1.2 [The R-Learner Idea](#12-the-r-learner-idea)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [R-Loss Formulation](#21-r-loss-formulation)\n",
    "   - 2.2 [Connection to DML](#22-connection-to-dml)\n",
    "   - 2.3 [Extensions to Other Models](#23-extensions-to-other-models)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [Heterogeneous Price Effects](#31-heterogeneous-price-effects)\n",
    "   - 3.2 [R-Learner Implementation](#32-r-learner-implementation)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm, smf,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    ols_summary_table,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: CATE estimation is hot in industry. Understanding how DML extends to heterogeneous effects via R-learner shows cutting-edge ML+causal knowledge.\n",
    "\n",
    "### 1.1 From ATE to CATE\n",
    "\n",
    "So far we've estimated the **Average Treatment Effect (ATE)**:\n",
    "\n",
    "$$\\text{ATE} = E[Y(1) - Y(0)]$$\n",
    "\n",
    "But effects often vary by individual characteristics. The **Conditional Average Treatment Effect (CATE)**:\n",
    "\n",
    "$$\\tau(x) = E[Y(1) - Y(0) | X = x]$$\n",
    "\n",
    "**Why CATE matters**:\n",
    "- Targeting: Who benefits most from treatment?\n",
    "- Personalization: Optimal policy for each individual\n",
    "- Understanding: What drives treatment effect heterogeneity?\n",
    "\n",
    "### 1.2 The R-Learner Idea\n",
    "\n",
    "Facure presents the **R-learner** (Robinson, 1988; Nie and Wager, 2021):\n",
    "\n",
    "**Key insight**: The DML residualization can be used to learn $\\tau(x)$, not just a constant $\\theta$.\n",
    "\n",
    "**DML for ATE**:\n",
    "$$\\tilde{Y} = \\theta \\tilde{T} + \\varepsilon$$\n",
    "\n",
    "**R-Learner for CATE**:\n",
    "$$\\tilde{Y} = \\tau(X) \\tilde{T} + \\varepsilon$$\n",
    "\n",
    "We minimize the **R-loss**:\n",
    "$$\\hat{\\tau} = \\arg\\min_\\tau \\sum_i \\left(\\tilde{Y}_i - \\tau(X_i) \\tilde{T}_i\\right)^2$$\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- R-learner = DML + flexible CATE function\n",
    "- Same orthogonalization, but learn $\\tau(x)$ instead of $\\theta$\n",
    "- Can use any ML model for $\\tau(x)$\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 R-Loss Formulation\n",
    "\n",
    "**Model**:\n",
    "\n",
    "$$Y = \\tau(X) \\cdot T + g(X) + \\varepsilon$$\n",
    "\n",
    "where:\n",
    "- $\\tau(X)$: Heterogeneous treatment effect function\n",
    "- $g(X)$: Baseline outcome (nuisance)\n",
    "- $T$: Treatment\n",
    "\n",
    "**R-Loss** (Robinson loss):\n",
    "\n",
    "$$\\mathcal{L}(\\tau) = E\\left[(Y - m(X) - \\tau(X)(T - e(X)))^2\\right]$$\n",
    "\n",
    "where:\n",
    "- $m(X) = E[Y|X]$: Conditional outcome mean\n",
    "- $e(X) = E[T|X]$: Propensity score (or conditional treatment mean)\n",
    "\n",
    "**Equivalently**, using residuals:\n",
    "\n",
    "$$\\mathcal{L}(\\tau) = E\\left[(\\tilde{Y} - \\tau(X) \\tilde{T})^2\\right]$$\n",
    "\n",
    "### 2.2 Connection to DML\n",
    "\n",
    "**Proposition (R-Learner as DML Generalization)**:\n",
    "\n",
    "When $\\tau(X) = \\theta$ (constant), the R-learner reduces to DML:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{DML}} = \\arg\\min_\\theta \\sum_i (\\tilde{Y}_i - \\theta \\tilde{T}_i)^2 = \\frac{\\sum_i \\tilde{T}_i \\tilde{Y}_i}{\\sum_i \\tilde{T}_i^2}$$\n",
    "\n",
    "**Proof**: Take derivative, set to zero:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta} \\sum_i (\\tilde{Y}_i - \\theta \\tilde{T}_i)^2 = -2\\sum_i \\tilde{T}_i(\\tilde{Y}_i - \\theta \\tilde{T}_i) = 0$$\n",
    "\n",
    "$$\\Rightarrow \\hat{\\theta} = \\frac{\\sum_i \\tilde{T}_i \\tilde{Y}_i}{\\sum_i \\tilde{T}_i^2} \\quad \\blacksquare$$\n",
    "\n",
    "### 2.3 Extensions to Other Models\n",
    "\n",
    "**DML extends beyond partially linear models**:\n",
    "\n",
    "| Model | Nuisance | Target |\n",
    "|-------|----------|--------|\n",
    "| Partially Linear | $g(X), m(X)$ | $\\theta$ (constant) |\n",
    "| R-Learner | $m(X), e(X)$ | $\\tau(X)$ (function) |\n",
    "| IV/LATE | $g(X), m(X), \\pi(X)$ | $\\theta$ with endogeneity |\n",
    "| Panel DiD | Time/unit FE | $\\tau$ with fixed effects |\n",
    "\n",
    "**Key principle**: Orthogonalize, then estimate target parameter.\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- R-loss = weighted least squares with weights $\\tilde{T}_i^2$\n",
    "- Observations with high $|\\tilde{T}_i|$ contribute more\n",
    "- Same cross-fitting logic applies\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 Heterogeneous Price Effects\n",
    "\n",
    "Let's explore whether the price effect varies with temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ice_cream = load_facure_data('ice_cream_sales.csv')\n",
    "\n",
    "X = ice_cream[['temp', 'weekday', 'cost']].values\n",
    "T = ice_cream['price'].values\n",
    "Y = ice_cream['sales'].values\n",
    "temp = ice_cream['temp'].values\n",
    "\n",
    "print(f\"Sample size: n = {len(Y)}\")\n",
    "print(f\"Temperature range: {temp.min():.1f} to {temp.max():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check for heterogeneity with interaction model\n",
    "ice_cream['price_x_temp'] = ice_cream['price'] * ice_cream['temp']\n",
    "interaction_model = smf.ols('sales ~ price * temp + weekday + cost', data=ice_cream).fit()\n",
    "\n",
    "print(\"OLS WITH INTERACTION (price × temperature)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPrice effect: {interaction_model.params['price']:.4f}\")\n",
    "print(f\"Temperature effect: {interaction_model.params['temp']:.4f}\")\n",
    "print(f\"Interaction (price × temp): {interaction_model.params['price:temp']:.4f}\")\n",
    "print(f\"p-value for interaction: {interaction_model.pvalues['price:temp']:.4f}\")\n",
    "print(f\"\\nInterpretation: Price effect {'varies' if interaction_model.pvalues['price:temp'] < 0.05 else 'does not significantly vary'} with temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 3.2 R-Learner Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_learner(Y, T, X, nuisance_model, cate_model, n_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    R-Learner for CATE estimation.\n",
    "    \n",
    "    1. Cross-fit nuisance models (m(X), e(X))\n",
    "    2. Compute residuals\n",
    "    3. Fit CATE model minimizing R-loss\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Stage 1: Cross-fit nuisance models\n",
    "    Y_pred = np.zeros(n)\n",
    "    T_pred = np.zeros(n)\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # Fit on train\n",
    "        model_y = nuisance_model(random_state=random_state)\n",
    "        model_t = nuisance_model(random_state=random_state)\n",
    "        model_y.fit(X[train_idx], Y[train_idx])\n",
    "        model_t.fit(X[train_idx], T[train_idx])\n",
    "        \n",
    "        # Predict on test\n",
    "        Y_pred[test_idx] = model_y.predict(X[test_idx])\n",
    "        T_pred[test_idx] = model_t.predict(X[test_idx])\n",
    "    \n",
    "    # Residuals\n",
    "    Y_res = Y - Y_pred\n",
    "    T_res = T - T_pred\n",
    "    \n",
    "    # Stage 2: Fit CATE model minimizing R-loss\n",
    "    # R-loss: sum_i (Y_res_i - tau(X_i) * T_res_i)^2\n",
    "    # This is equivalent to weighted regression:\n",
    "    # Y_res / T_res ~ tau(X), weighted by T_res^2\n",
    "    \n",
    "    # Pseudo-outcome for R-learner\n",
    "    pseudo_outcome = Y_res / (T_res + 1e-10)  # Add small constant to avoid division by zero\n",
    "    weights = T_res ** 2\n",
    "    \n",
    "    # Fit CATE model\n",
    "    cate_fitted = cate_model()\n",
    "    cate_fitted.fit(X, pseudo_outcome, sample_weight=weights)\n",
    "    \n",
    "    # Predict CATE\n",
    "    tau_hat = cate_fitted.predict(X)\n",
    "    \n",
    "    return {\n",
    "        'tau_hat': tau_hat,\n",
    "        'Y_residual': Y_res,\n",
    "        'T_residual': T_res,\n",
    "        'cate_model': cate_fitted,\n",
    "        'ate': np.mean(tau_hat)\n",
    "    }\n",
    "\n",
    "# Run R-Learner\n",
    "r_result = r_learner(\n",
    "    Y, T, X,\n",
    "    nuisance_model=lambda **kw: GradientBoostingRegressor(n_estimators=100, max_depth=3, **kw),\n",
    "    cate_model=lambda: GradientBoostingRegressor(n_estimators=50, max_depth=2),\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "print(\"R-LEARNER RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average Treatment Effect (ATE): {r_result['ate']:.4f}\")\n",
    "print(f\"CATE range: [{r_result['tau_hat'].min():.4f}, {r_result['tau_hat'].max():.4f}]\")\n",
    "print(f\"CATE std: {r_result['tau_hat'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CATE heterogeneity\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: CATE by temperature\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(temp, r_result['tau_hat'], c=temp, cmap='coolwarm', \n",
    "                     s=20, alpha=0.5)\n",
    "\n",
    "# Add trend line (LOWESS or binned means)\n",
    "temp_bins = pd.cut(temp, bins=10)\n",
    "binned = pd.DataFrame({'temp': temp, 'tau': r_result['tau_hat']}).groupby(\n",
    "    temp_bins, observed=True)['tau'].mean()\n",
    "bin_centers = [interval.mid for interval in binned.index]\n",
    "ax.plot(bin_centers, binned.values, 'k-', linewidth=2.5, label='Binned mean')\n",
    "\n",
    "ax.axhline(r_result['ate'], color=TUFTE_PALETTE['effect'], linestyle='--',\n",
    "           linewidth=2, label=f'ATE = {r_result[\"ate\"]:.4f}')\n",
    "ax.set_xlabel('Temperature')\n",
    "ax.set_ylabel('Estimated CATE (τ̂(x))')\n",
    "ax.set_title('(a) CATE by Temperature')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: Distribution of CATE\n",
    "ax = axes[1]\n",
    "ax.hist(r_result['tau_hat'], bins=30, color=TUFTE_PALETTE['secondary'], \n",
    "        edgecolor='white', alpha=0.7)\n",
    "ax.axvline(r_result['ate'], color=TUFTE_PALETTE['effect'], linewidth=2.5,\n",
    "           linestyle='--', label=f'ATE = {r_result[\"ate\"]:.4f}')\n",
    "ax.axvline(0, color='gray', linewidth=1.5, linestyle=':')\n",
    "\n",
    "ax.set_xlabel('CATE Estimate')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('(b) Distribution of Individual Treatment Effects')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare R-learner ATE to DML ATE\n",
    "def dml_ate(Y, T, X, ml_model, n_folds=5):\n",
    "    \"\"\"Simple DML for ATE comparison.\"\"\"\n",
    "    Y_pred = cross_val_predict(ml_model(), X, Y, cv=n_folds)\n",
    "    T_pred = cross_val_predict(ml_model(), X, T, cv=n_folds)\n",
    "    \n",
    "    Y_res = Y - Y_pred\n",
    "    T_res = T - T_pred\n",
    "    \n",
    "    return np.sum(T_res * Y_res) / np.sum(T_res ** 2)\n",
    "\n",
    "dml_estimate = dml_ate(\n",
    "    Y, T, X,\n",
    "    lambda: GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42),\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "print(\"\\nCOMPARISON: R-LEARNER vs DML\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"DML (constant θ):        {dml_estimate:.4f}\")\n",
    "print(f\"R-Learner (mean of τ(x)): {r_result['ate']:.4f}\")\n",
    "print(f\"Difference:               {r_result['ate'] - dml_estimate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze heterogeneity by temperature quartile\n",
    "ice_cream['tau_hat'] = r_result['tau_hat']\n",
    "ice_cream['temp_quartile'] = pd.qcut(ice_cream['temp'], q=4, labels=['Cold', 'Cool', 'Warm', 'Hot'])\n",
    "\n",
    "quartile_effects = ice_cream.groupby('temp_quartile', observed=True)['tau_hat'].agg(['mean', 'std', 'count'])\n",
    "quartile_effects.columns = ['Mean CATE', 'Std CATE', 'N']\n",
    "\n",
    "print(\"\\nCATE BY TEMPERATURE QUARTILE\")\n",
    "print(\"=\" * 50)\n",
    "print(quartile_effects.round(4))\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "if quartile_effects['Mean CATE'].iloc[-1] > quartile_effects['Mean CATE'].iloc[0]:\n",
    "    print(f\"  Price elasticity is LESS negative on hot days\")\n",
    "    print(f\"  → Customers are less price-sensitive when it's hot\")\n",
    "else:\n",
    "    print(f\"  Price elasticity is MORE negative on hot days\")\n",
    "    print(f\"  → Customers are more price-sensitive when it's hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Key findings**:\n",
    "\n",
    "1. **Heterogeneity exists**: CATE varies across observations\n",
    "2. **Temperature matters**: Price sensitivity varies with weather\n",
    "3. **Policy implication**: Optimal pricing should consider temperature\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- R-learner extends DML to learn $\\tau(x)$\n",
    "- Same orthogonalization, different target\n",
    "- Can use any ML model for the CATE function\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "The `causal_inference_mastery` library provides R-learner:\n",
    "\n",
    "```python\n",
    "from causal_inference.cate import RLearner, CATEResult\n",
    "\n",
    "# R-Learner for CATE\n",
    "rlearner = RLearner(\n",
    "    nuisance_model=GradientBoostingRegressor(n_estimators=100),\n",
    "    cate_model=GradientBoostingRegressor(n_estimators=50),\n",
    "    n_folds=5\n",
    ")\n",
    "\n",
    "result = rlearner.fit(Y, T, X)\n",
    "\n",
    "# CATE predictions\n",
    "tau_hat = result.predict(X_new)\n",
    "\n",
    "# Summary\n",
    "print(f\"ATE: {result.ate:.4f}\")\n",
    "print(f\"CATE std: {result.tau_std:.4f}\")\n",
    "\n",
    "# Feature importance for heterogeneity\n",
    "result.plot_feature_importance()\n",
    "```\n",
    "\n",
    "Production implementations:\n",
    "- `econml.dml.LinearDML` (linear CATE)\n",
    "- `econml.dml.CausalForestDML` (forest-based CATE)\n",
    "- `econml.metalearners.RLearner` (generic R-learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Meta E5, DS)**: *\"What's the R-learner and how does it relate to DML?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**R-Learner** (Robinson-learner):\n",
    "\n",
    "An approach to estimate heterogeneous treatment effects $\\tau(x)$ using the same orthogonalization idea as DML.\n",
    "\n",
    "**Connection to DML**:\n",
    "\n",
    "| Aspect | DML | R-Learner |\n",
    "|--------|-----|----------|\n",
    "| Target | Constant $\\theta$ | Function $\\tau(x)$ |\n",
    "| Nuisance | $m(X), e(X)$ | $m(X), e(X)$ (same) |\n",
    "| Residuals | $\\tilde{Y}, \\tilde{T}$ | $\\tilde{Y}, \\tilde{T}$ (same) |\n",
    "| Final step | $\\theta = \\frac{\\sum \\tilde{T}\\tilde{Y}}{\\sum \\tilde{T}^2}$ | Minimize $\\sum(\\tilde{Y} - \\tau(X)\\tilde{T})^2$ |\n",
    "\n",
    "**Key insight**: DML is R-learner with $\\tau(x) = \\theta$ (constant).\n",
    "\n",
    "**R-loss**:\n",
    "$$\\mathcal{L}(\\tau) = \\sum_i (\\tilde{Y}_i - \\tau(X_i)\\tilde{T}_i)^2$$\n",
    "\n",
    "This is equivalent to weighted regression with pseudo-outcome $\\tilde{Y}/\\tilde{T}$ and weights $\\tilde{T}^2$.\n",
    "\n",
    "**When to use R-learner**:\n",
    "- Suspect heterogeneous effects\n",
    "- Want to target different subgroups\n",
    "- Personalization applications\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Google L5, Quant)**: *\"How do you estimate CATE with machine learning? What are the main approaches?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Main approaches to CATE estimation**:\n",
    "\n",
    "1. **Meta-learners** (indirect):\n",
    "   - **T-learner**: Fit separate models for treated/control, take difference\n",
    "   - **S-learner**: Fit one model with treatment as feature\n",
    "   - **X-learner**: T-learner + impute counterfactuals + weighted average\n",
    "   - **R-learner**: Orthogonalized residual regression\n",
    "\n",
    "2. **Direct methods**:\n",
    "   - **Causal Forest**: Modified random forest that targets CATE directly\n",
    "   - **Bayesian CART**: Trees with Bayesian estimation\n",
    "\n",
    "3. **DML-based**:\n",
    "   - **LinearDML**: CATE linear in features, flexible nuisance\n",
    "   - **CausalForestDML**: Causal forest for CATE, DML for nuisance\n",
    "\n",
    "**Comparison**:\n",
    "\n",
    "| Method | Pros | Cons |\n",
    "|--------|------|------|\n",
    "| T-learner | Simple | High variance, biased with imbalance |\n",
    "| S-learner | Simple | Regularization biases CATE |\n",
    "| X-learner | Good with imbalance | Complex |\n",
    "| R-learner | Orthogonal, valid inference | Requires good nuisance |\n",
    "| Causal Forest | Direct optimization | Black box |\n",
    "\n",
    "**Recommendation**: Start with R-learner or CausalForestDML for valid inference.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Amazon L6, Econ)**: *\"Your R-learner shows large CATE heterogeneity, but your DML confidence interval for the ATE is wide. What's happening?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**This situation is common and has several explanations**:\n",
    "\n",
    "1. **Heterogeneity is real, but average is uncertain**:\n",
    "   - CATE varies a lot → ATE is an average of different effects\n",
    "   - Wide CI reflects uncertainty about the *average*\n",
    "   - Individual CATEs may be precisely estimated\n",
    "\n",
    "2. **Weak treatment variation**:\n",
    "   - If $\\text{Var}(\\tilde{T})$ is small, both DML and R-learner struggle\n",
    "   - SE scales with $1/\\sqrt{\\sum \\tilde{T}_i^2}$\n",
    "   - Heterogeneity may be overfitted to noise\n",
    "\n",
    "3. **Overfitting in CATE model**:\n",
    "   - R-learner can overfit the CATE function\n",
    "   - May show spurious heterogeneity\n",
    "   - Need cross-validation for CATE model too\n",
    "\n",
    "**What to check**:\n",
    "- R² for nuisance models (if low, estimates unreliable)\n",
    "- Effective sample size $\\sum \\tilde{T}_i^2 / (\\max \\tilde{T}_i^2)$\n",
    "- CATE stability across different ML models\n",
    "- Cross-validated CATE predictions\n",
    "\n",
    "**Interpretation**:\n",
    "- Report both ATE CI and CATE distribution\n",
    "- Be cautious about extreme CATE estimates\n",
    "- Heterogeneity findings may need larger samples to confirm\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 22: \"Debiased/Orthogonal Machine Learning.\"\n",
    "\n",
    "[^2]: Nie, X. and Wager, S. (2021). Quasi-Oracle Estimation of Heterogeneous Treatment Effects. *Biometrika*, 108(2), 299-319.\n",
    "\n",
    "[^3]: Robinson, P. M. (1988). Root-N-Consistent Semiparametric Regression. *Econometrica*, 56(4), 931-954.\n",
    "\n",
    "[^4]: Künzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for Estimating Heterogeneous Treatment Effects Using Machine Learning. *PNAS*, 116(10), 4156-4165.\n",
    "\n",
    "[^5]: Athey, S. and Wager, S. (2019). Estimating Treatment Effects with Causal Forests: An Application. *Observational Studies*, 5(2), 37-51.\n",
    "\n",
    "---\n",
    "\n",
    "**Precision Improvement:**\n",
    "- You said: \"Build advanced DML notebook\"\n",
    "- Concise: \"Build 03_dml_advanced.ipynb\"\n",
    "- Precise: `/augmented 22.3 --r-learner --cate --heterogeneity`\n",
    "- Pattern: [build] [target] [content-flags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
