{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing with Orthogonalization: Cross-Fitting\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Appendix A2 | Notebook 3 of 3**\n",
    "\n",
    "Why sample splitting is essential for ML-based debiasing,\n",
    "and how to implement it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from augmented.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### The Overfitting Problem\n",
    "\n",
    "ML models are powerful enough to **perfectly fit** training data.\n",
    "\n",
    "**Problem**: If we use the same data to:\n",
    "1. Train the model\n",
    "2. Compute residuals\n",
    "\n",
    "Then residuals → 0, and we lose all variation.\n",
    "\n",
    "### Why This Matters for Causal Inference\n",
    "\n",
    "If $\\hat{m}(X) \\approx T$ (overfitting):\n",
    "- $T^* = T - \\hat{m}(X) \\approx 0$\n",
    "- No treatment variation left to correlate with $Y^*$\n",
    "- $\\hat{\\tau}$ becomes undefined or biased\n",
    "\n",
    "### The Solution: Sample Splitting\n",
    "\n",
    "**K-Fold Cross-Fitting**:\n",
    "1. Split data into K folds\n",
    "2. For each fold k:\n",
    "   - Train model on all OTHER folds\n",
    "   - Predict on fold k\n",
    "3. Residuals computed on data the model hasn't seen\n",
    "\n",
    "**Result**: Models can't overfit to their prediction data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Cross-Fitting Procedure\n",
    "\n",
    "**Setup**: Data $\\{(Y_i, T_i, X_i)\\}_{i=1}^n$, K folds $I_1, ..., I_K$\n",
    "\n",
    "**For each fold k**:\n",
    "1. Train $\\hat{\\ell}^{(-k)}(X)$ on $\\{i : i \\notin I_k\\}$\n",
    "2. Train $\\hat{m}^{(-k)}(X)$ on $\\{i : i \\notin I_k\\}$\n",
    "3. Compute residuals for $i \\in I_k$:\n",
    "   - $\\tilde{Y}_i = Y_i - \\hat{\\ell}^{(-k)}(X_i)$\n",
    "   - $\\tilde{T}_i = T_i - \\hat{m}^{(-k)}(X_i)$\n",
    "\n",
    "**Final estimate**:\n",
    "$$\\hat{\\tau} = \\frac{\\sum_{i=1}^n \\tilde{Y}_i \\tilde{T}_i}{\\sum_{i=1}^n \\tilde{T}_i^2}$$\n",
    "\n",
    "### 2.2 Why Cross-Fitting Works\n",
    "\n",
    "**Key insight**: For observation $i \\in I_k$:\n",
    "- Model $\\hat{m}^{(-k)}$ was trained on data NOT containing $i$\n",
    "- Model cannot overfit to observation $i$\n",
    "- Residual $\\tilde{T}_i$ preserves genuine variation\n",
    "\n",
    "### 2.3 Theoretical Guarantees\n",
    "\n",
    "Under regularity conditions (Chernozhukov et al. 2018):\n",
    "\n",
    "$$\\sqrt{n}(\\hat{\\tau} - \\tau_0) \\xrightarrow{d} N(0, \\sigma^2)$$\n",
    "\n",
    "where $\\sigma^2$ depends on:\n",
    "- Variance of true residuals\n",
    "- NOT on ML estimation errors (due to Neyman orthogonality)\n",
    "\n",
    "**Requirements**:\n",
    "- ML converges at rate $n^{-1/4}$ or faster\n",
    "- Cross-fitting used\n",
    "- Finite variance conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Setup: Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dml_data(n=5000, seed=42):\n",
    "    \"\"\"Generate data for DML demonstration.\n",
    "    \n",
    "    DGP with nonlinear confounding:\n",
    "    - True effect τ = -2.0\n",
    "    - Nonlinear m(X) and g(X)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X1 = np.random.uniform(0, 10, n)\n",
    "    X2 = np.random.uniform(0, 5, n)\n",
    "    \n",
    "    # E[T|X] - nonlinear\n",
    "    m_X = 5 + 2*np.sin(X1) + 0.5*X2**2\n",
    "    T = m_X + np.random.normal(0, 1, n)\n",
    "    \n",
    "    # True effect and E[Y|X]\n",
    "    TAU = -2.0\n",
    "    g_X = 10 + 3*np.cos(X1) + X1*X2\n",
    "    Y = TAU * T + g_X + np.random.normal(0, 2, n)\n",
    "    \n",
    "    return pd.DataFrame({'X1': X1, 'X2': X2, 'T': T, 'Y': Y}), TAU\n",
    "\n",
    "data, TRUE_TAU = generate_dml_data()\n",
    "print(f\"True τ = {TRUE_TAU}\")\n",
    "print(f\"n = {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Same-Sample vs Cross-Fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = ['X1', 'X2']\n",
    "\n",
    "# Method 1: Same-sample (naive, problematic)\n",
    "m_t_naive = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "m_t_naive.fit(data[X_cols], data['T'])\n",
    "T_pred_naive = m_t_naive.predict(data[X_cols])\n",
    "\n",
    "m_y_naive = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "m_y_naive.fit(data[X_cols], data['Y'])\n",
    "Y_pred_naive = m_y_naive.predict(data[X_cols])\n",
    "\n",
    "T_star_naive = data['T'] - T_pred_naive\n",
    "Y_star_naive = data['Y'] - Y_pred_naive\n",
    "tau_naive = np.sum(T_star_naive * Y_star_naive) / np.sum(T_star_naive**2)\n",
    "\n",
    "print(\"Same-Sample (Naive):\")\n",
    "print(f\"  Var(T*): {np.var(T_star_naive):.4f} (should be ~1.0)\")\n",
    "print(f\"  τ̂ = {tau_naive:.4f}, bias = {tau_naive - TRUE_TAU:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Cross-fitting (correct)\n",
    "folds = 5\n",
    "\n",
    "m_t_cf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "T_pred_cf = cross_val_predict(m_t_cf, data[X_cols], data['T'], cv=folds)\n",
    "\n",
    "m_y_cf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "Y_pred_cf = cross_val_predict(m_y_cf, data[X_cols], data['Y'], cv=folds)\n",
    "\n",
    "T_star_cf = data['T'] - T_pred_cf\n",
    "Y_star_cf = data['Y'] - Y_pred_cf\n",
    "tau_cf = np.sum(T_star_cf * Y_star_cf) / np.sum(T_star_cf**2)\n",
    "\n",
    "print(\"\\nCross-Fitted (5 folds):\")\n",
    "print(f\"  Var(T*): {np.var(T_star_cf):.4f} (should be ~1.0)\")\n",
    "print(f\"  τ̂ = {tau_cf:.4f}, bias = {tau_cf - TRUE_TAU:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"True τ = {TRUE_TAU}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Same-sample:  τ̂ = {tau_naive:7.4f}, bias = {tau_naive - TRUE_TAU:+.4f}\")\n",
    "print(f\"Cross-fitted: τ̂ = {tau_cf:7.4f}, bias = {tau_cf - TRUE_TAU:+.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Residual distributions\n",
    "ax = axes[0]\n",
    "ax.hist(T_star_naive, bins=50, alpha=0.5, label=f'Same-sample (Var={np.var(T_star_naive):.3f})', color=COLORS['red'])\n",
    "ax.hist(T_star_cf, bins=50, alpha=0.5, label=f'Cross-fitted (Var={np.var(T_star_cf):.3f})', color=COLORS['blue'])\n",
    "ax.set_xlabel('T* (treatment residual)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Treatment Residual Distribution')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Same-sample scatter\n",
    "ax = axes[1]\n",
    "sample_idx = np.random.choice(len(data), 1000, replace=False)\n",
    "ax.scatter(T_star_naive.iloc[sample_idx], Y_star_naive.iloc[sample_idx], alpha=0.3, s=10, c=COLORS['red'])\n",
    "x_line = np.linspace(-2, 2, 100)\n",
    "ax.plot(x_line, tau_naive * x_line, 'k-', linewidth=2, label=f'τ̂ = {tau_naive:.2f}')\n",
    "ax.set_xlabel('T* (same-sample)')\n",
    "ax.set_ylabel('Y*')\n",
    "ax.set_title('Same-Sample Residuals')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Cross-fitted scatter\n",
    "ax = axes[2]\n",
    "ax.scatter(T_star_cf.iloc[sample_idx], Y_star_cf.iloc[sample_idx], alpha=0.3, s=10, c=COLORS['blue'])\n",
    "x_line = np.linspace(-3, 3, 100)\n",
    "ax.plot(x_line, tau_cf * x_line, 'k-', linewidth=2, label=f'τ̂ = {tau_cf:.2f}')\n",
    "ax.set_xlabel('T* (cross-fitted)')\n",
    "ax.set_ylabel('Y*')\n",
    "ax.set_title('Cross-Fitted Residuals')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "Same-sample vs Cross-fitted comparison:\n",
    "\n",
    "**Same-sample** (naive):\n",
    "- T* has LOWER variance (overfitting shrinks residuals)\n",
    "- Estimate can be biased\n",
    "\n",
    "**Cross-fitted** (correct):\n",
    "- T* has PROPER variance (~noise level)\n",
    "- Estimate is unbiased (asymptotically)\n",
    "\n",
    "Rule: **Always use cross-fitting with ML residualization.**\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Ice Cream Sales with Cross-Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data\n",
    "prices = load_facure_data(\"ice_cream_sales.csv\")\n",
    "print(f\"Data shape: {prices.shape}\")\n",
    "\n",
    "X_cols = ['cost', 'weekday', 'temp']\n",
    "folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-fitted residuals with Random Forest\n",
    "np.random.seed(42)\n",
    "\n",
    "m_t = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "T_star = prices['price'] - cross_val_predict(m_t, prices[X_cols], prices['price'], cv=folds)\n",
    "\n",
    "m_y = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "Y_star = prices['sales'] - cross_val_predict(m_y, prices[X_cols], prices['sales'], cv=folds)\n",
    "\n",
    "# Estimate treatment effect\n",
    "tau_dml = np.sum(T_star * Y_star) / np.sum(T_star**2)\n",
    "\n",
    "# Bootstrap standard error\n",
    "def bootstrap_tau(T_star, Y_star, n_boot=200):\n",
    "    taus = []\n",
    "    n = len(T_star)\n",
    "    for _ in range(n_boot):\n",
    "        idx = np.random.choice(n, n, replace=True)\n",
    "        t_boot = T_star.iloc[idx] if hasattr(T_star, 'iloc') else T_star[idx]\n",
    "        y_boot = Y_star.iloc[idx] if hasattr(Y_star, 'iloc') else Y_star[idx]\n",
    "        tau_boot = np.sum(t_boot * y_boot) / np.sum(t_boot**2)\n",
    "        taus.append(tau_boot)\n",
    "    return np.std(taus)\n",
    "\n",
    "se_dml = bootstrap_tau(T_star, Y_star)\n",
    "\n",
    "print(\"Cross-Fitted DML Estimate:\")\n",
    "print(f\"  Price elasticity: τ̂ = {tau_dml:.4f}\")\n",
    "print(f\"  Bootstrap SE:     {se_dml:.4f}\")\n",
    "print(f\"  95% CI: [{tau_dml - 1.96*se_dml:.4f}, {tau_dml + 1.96*se_dml:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to linear regression (full model)\n",
    "linear_model = smf.ols(\"sales ~ price + cost + temp + C(weekday)\", data=prices).fit()\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  Linear regression: τ̂ = {linear_model.params['price']:.4f} (SE: {linear_model.bse['price']:.4f})\")\n",
    "print(f\"  Cross-fitted DML:  τ̂ = {tau_dml:.4f} (SE: {se_dml:.4f})\")\n",
    "print(\"\\nBoth show negative price elasticity (sensible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete DML Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_ml(data, X_cols, T_col, Y_col, model_class=RandomForestRegressor,\n",
    "              n_folds=5, n_bootstrap=200, random_state=42, **model_kwargs):\n",
    "    \"\"\"Complete Double/Debiased ML estimator with cross-fitting.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        Data containing X, T, Y columns\n",
    "    X_cols : list\n",
    "        Names of control/confounder columns\n",
    "    T_col : str\n",
    "        Name of treatment column\n",
    "    Y_col : str\n",
    "        Name of outcome column\n",
    "    model_class : class\n",
    "        Sklearn-compatible regressor class\n",
    "    n_folds : int\n",
    "        Number of cross-fitting folds\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples for SE\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with tau, se, ci_lower, ci_upper, T_star, Y_star\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    X = data[X_cols]\n",
    "    T = data[T_col].values\n",
    "    Y = data[Y_col].values\n",
    "    \n",
    "    # Cross-fitted residuals\n",
    "    m_t = model_class(**model_kwargs)\n",
    "    T_star = T - cross_val_predict(m_t, X, T, cv=n_folds)\n",
    "    \n",
    "    m_y = model_class(**model_kwargs)\n",
    "    Y_star = Y - cross_val_predict(m_y, X, Y, cv=n_folds)\n",
    "    \n",
    "    # Point estimate\n",
    "    tau = np.sum(T_star * Y_star) / np.sum(T_star**2)\n",
    "    \n",
    "    # Bootstrap SE\n",
    "    taus = []\n",
    "    n = len(T)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n, n, replace=True)\n",
    "        tau_boot = np.sum(T_star[idx] * Y_star[idx]) / np.sum(T_star[idx]**2)\n",
    "        taus.append(tau_boot)\n",
    "    se = np.std(taus)\n",
    "    \n",
    "    return {\n",
    "        'tau': tau,\n",
    "        'se': se,\n",
    "        'ci_lower': tau - 1.96 * se,\n",
    "        'ci_upper': tau + 1.96 * se,\n",
    "        'T_star': T_star,\n",
    "        'Y_star': Y_star\n",
    "    }\n",
    "\n",
    "# Apply to ice cream data\n",
    "result = double_ml(prices, X_cols, 'price', 'sales',\n",
    "                   model_class=RandomForestRegressor,\n",
    "                   n_estimators=100, max_depth=5)\n",
    "\n",
    "print(\"Double ML Results:\")\n",
    "print(f\"  τ̂ = {result['tau']:.4f}\")\n",
    "print(f\"  SE = {result['se']:.4f}\")\n",
    "print(f\"  95% CI: [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Key Takeaway ────────────────────────────────────────────────\n",
    "Cross-fitting is **essential** for valid ML-based causal inference.\n",
    "\n",
    "**Implementation checklist**:\n",
    "1. Split data into K folds (typically K=5)\n",
    "2. For each fold, train on complement, predict on fold\n",
    "3. Use `cross_val_predict` from sklearn\n",
    "4. Bootstrap for standard errors\n",
    "\n",
    "**Without cross-fitting**:\n",
    "- Overfitting shrinks residuals\n",
    "- Estimates become biased\n",
    "- Confidence intervals invalid\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: Why is cross-fitting necessary for Double ML?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Without cross-fitting**:\n",
    "- ML model can overfit to training data\n",
    "- Overfitting means $\\hat{m}(X_i) \\approx T_i$\n",
    "- Residuals $\\hat{T}^* = T - \\hat{m}(X) \\approx 0$\n",
    "- No treatment variation left for estimation\n",
    "\n",
    "**With cross-fitting**:\n",
    "- Model trained on fold A, predicts on fold B\n",
    "- Model hasn't seen fold B data\n",
    "- Cannot overfit to fold B observations\n",
    "- Residuals preserve genuine variation\n",
    "\n",
    "**Mathematical reason**: Neyman orthogonality protects against first-order bias, but overfitting creates second-order bias that cross-fitting eliminates.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: How many folds should you use for cross-fitting?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Common choices**:\n",
    "- K=2: Simple, but each model sees only 50% of data\n",
    "- K=5: Standard, good balance (each model sees 80%)\n",
    "- K=10: More training data per model, more computation\n",
    "- K=n (LOO): Maximum training data, expensive\n",
    "\n",
    "**Theory**: Any K ≥ 2 yields √n-consistent, asymptotically normal estimates.\n",
    "\n",
    "**Practice**: K=5 is most common:\n",
    "- Each fold has enough data for good predictions\n",
    "- Each model trained on 80% of data\n",
    "- Reasonable computation time\n",
    "\n",
    "**Recommendation**: Start with K=5, increase if predictions are poor.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: What's the difference between cross-validation and cross-fitting?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Cross-validation** (model selection):\n",
    "- Goal: Choose hyperparameters or select models\n",
    "- Compute out-of-sample prediction error\n",
    "- Average across folds to get CV score\n",
    "- Output: A single number (CV error)\n",
    "\n",
    "**Cross-fitting** (DML):\n",
    "- Goal: Generate predictions for all observations\n",
    "- Each observation predicted by model trained without it\n",
    "- Concatenate predictions across folds\n",
    "- Output: A vector of predictions (one per observation)\n",
    "\n",
    "**Key distinction**:\n",
    "- CV: Evaluates model performance\n",
    "- Cross-fitting: Generates unbiased predictions for downstream use\n",
    "\n",
    "**Implementation**: `cross_val_predict` does cross-fitting, not CV.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: How do you estimate standard errors for DML?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Three approaches**:\n",
    "\n",
    "1. **Bootstrap** (most common):\n",
    "   - Resample residuals (T*, Y*) with replacement\n",
    "   - Recompute τ̂ for each bootstrap sample\n",
    "   - SE = std(bootstrap τ̂s)\n",
    "\n",
    "2. **Influence function** (theoretically correct):\n",
    "   - Compute influence function ψ for each observation\n",
    "   - SE² = Var(ψ) / n\n",
    "   - More complex but avoids bootstrap\n",
    "\n",
    "3. **Heteroskedasticity-robust** (simple):\n",
    "   - Treat final regression Y* ~ T* as OLS\n",
    "   - Use HC standard errors\n",
    "   - May underestimate uncertainty from first stage\n",
    "\n",
    "**Recommendation**: Bootstrap is simple and robust. Use 200+ samples.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: Can cross-fitting fail? When?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Cross-fitting can fail when**:\n",
    "\n",
    "1. **Data is too small**:\n",
    "   - Each fold too small for good ML predictions\n",
    "   - Solution: Use simpler models or more folds\n",
    "\n",
    "2. **Strong time dependence**:\n",
    "   - Random folds break time structure\n",
    "   - Solution: Use time-based folds (rolling window)\n",
    "\n",
    "3. **Extreme heterogeneity**:\n",
    "   - Folds have very different distributions\n",
    "   - Solution: Stratified folding\n",
    "\n",
    "4. **Unmeasured confounding**:\n",
    "   - Cross-fitting doesn't fix identification problems\n",
    "   - Still need conditional ignorability\n",
    "\n",
    "**Important**: Cross-fitting is necessary but not sufficient. It solves the overfitting problem, not the identification problem.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Chernozhukov, V., et al. (2018). Double/Debiased Machine Learning for\n",
    "      Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^2]: Chernozhukov, V., et al. (2017). Double/Debiased/Neyman Machine Learning\n",
    "      of Treatment Effects. *American Economic Review Papers & Proceedings*, 107(5), 261-265.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, Appendix:\n",
    "      Debiasing with Orthogonalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
