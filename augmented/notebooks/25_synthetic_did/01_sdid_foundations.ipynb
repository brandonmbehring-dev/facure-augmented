{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Difference-in-Differences: Foundations\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Chapter 25 | Notebook 1 of 3**\n",
    "\n",
    "This notebook introduces the Synthetic Difference-in-Differences (SDID) method, which\n",
    "elegantly combines the best features of both Difference-in-Differences (DiD) and\n",
    "Synthetic Control (SC) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from augmented.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "# Additional imports for this notebook\n",
    "from scipy.optimize import fmin_slsqp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### The Best of Both Worlds\n",
    "\n",
    "Consider the challenge of estimating treatment effects in panel data:\n",
    "\n",
    "**Difference-in-Differences (DiD):**\n",
    "- Uses all control units equally\n",
    "- Relies on parallel trends assumption\n",
    "- Simple, but sensitive to trends mismatch\n",
    "\n",
    "**Synthetic Control (SC):**\n",
    "- Weights control units to match pre-treatment trends\n",
    "- Excellent pre-treatment fit\n",
    "- No time-period balancing\n",
    "\n",
    "**Synthetic DiD (SDID):**\n",
    "- Weights *both* units AND time periods\n",
    "- Unit weights ensure good pre-treatment fit\n",
    "- Time weights ensure good \"parallel trends\" matching\n",
    "- Combines strengths of both methods\n",
    "\n",
    "> **Key Insight**: SDID asks: \"Which control units look most like treated units?\"\n",
    "> AND \"Which pre-treatment periods are most informative about counterfactual trends?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Block Matrix Representation\n",
    "\n",
    "Panel data can be organized into a **block matrix** structure:\n",
    "\n",
    "$$Y = \\begin{bmatrix} Y_{co}^{pre} & Y_{co}^{post} \\\\ Y_{tr}^{pre} & Y_{tr}^{post} \\end{bmatrix}$$\n",
    "\n",
    "Where:\n",
    "- $Y_{co}^{pre}$: Control units, pre-treatment periods ($N_{co} \\times T_{pre}$)\n",
    "- $Y_{co}^{post}$: Control units, post-treatment periods ($N_{co} \\times T_{post}$)\n",
    "- $Y_{tr}^{pre}$: Treated units, pre-treatment periods ($N_{tr} \\times T_{pre}$)\n",
    "- $Y_{tr}^{post}$: Treated units, post-treatment periods ($N_{tr} \\times T_{post}$)\n",
    "\n",
    "This organization reveals the fundamental estimation problem: we observe all four blocks,\n",
    "but the counterfactual (what $Y_{tr}^{post}$ would have been without treatment) is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the block matrix structure\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Create block matrix visualization\n",
    "blocks = [\n",
    "    # (x, y, width, height, color, label)\n",
    "    (0, 2, 3, 2, COLORS['blue'], r'$Y_{co}^{pre}$'),\n",
    "    (3, 2, 2, 2, COLORS['blue'], r'$Y_{co}^{post}$'),\n",
    "    (0, 0, 3, 2, COLORS['red'], r'$Y_{tr}^{pre}$'),\n",
    "    (3, 0, 2, 2, COLORS['orange'], r'$Y_{tr}^{post}$ (treated)'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, color, label in blocks:\n",
    "    rect = plt.Rectangle((x, y), w, h, facecolor=color, edgecolor='black', \n",
    "                          alpha=0.5, linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Add dimension annotations\n",
    "ax.annotate('', xy=(0, 4.3), xytext=(3, 4.3), \n",
    "            arrowprops=dict(arrowstyle='<->', color='gray'))\n",
    "ax.text(1.5, 4.5, r'$T_{pre}$', ha='center', fontsize=10)\n",
    "\n",
    "ax.annotate('', xy=(3, 4.3), xytext=(5, 4.3),\n",
    "            arrowprops=dict(arrowstyle='<->', color='gray'))\n",
    "ax.text(4, 4.5, r'$T_{post}$', ha='center', fontsize=10)\n",
    "\n",
    "ax.annotate('', xy=(-0.3, 2), xytext=(-0.3, 4),\n",
    "            arrowprops=dict(arrowstyle='<->', color='gray'))\n",
    "ax.text(-0.6, 3, r'$N_{co}$', ha='center', va='center', fontsize=10, rotation=90)\n",
    "\n",
    "ax.annotate('', xy=(-0.3, 0), xytext=(-0.3, 2),\n",
    "            arrowprops=dict(arrowstyle='<->', color='gray'))\n",
    "ax.text(-0.6, 1, r'$N_{tr}$', ha='center', va='center', fontsize=10, rotation=90)\n",
    "\n",
    "ax.set_xlim(-1, 6)\n",
    "ax.set_ylim(-0.5, 5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Block Matrix Structure of Panel Data', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Standard DiD Estimator\n",
    "\n",
    "The classic DiD estimator with equal weighting:\n",
    "\n",
    "$$\\hat{\\tau}^{did} = \\left( \\bar{Y}_{tr}^{post} - \\bar{Y}_{tr}^{pre} \\right) - \\left( \\bar{Y}_{co}^{post} - \\bar{Y}_{co}^{pre} \\right)$$\n",
    "\n",
    "This can be written as a **weighted double-difference**:\n",
    "\n",
    "$$\\hat{\\tau}^{did} = \\sum_{i,t} D_{it} Y_{it} \\cdot w_i^{did} \\cdot \\lambda_t^{did}$$\n",
    "\n",
    "Where:\n",
    "- $w_i^{did} = \\frac{1}{N_{co}}$ for control units (equal weight)\n",
    "- $\\lambda_t^{did} = \\frac{1}{T_{pre}}$ for pre-periods, $\\frac{1}{T_{post}}$ for post-periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Synthetic Control Estimator\n",
    "\n",
    "SC finds optimal unit weights to match pre-treatment trends:\n",
    "\n",
    "$$\\hat{\\tau}^{sc} = \\bar{Y}_{tr}^{post} - \\sum_{i \\in co} \\hat{w}_i^{sc} \\bar{Y}_i^{post}$$\n",
    "\n",
    "Where weights $\\hat{w}^{sc}$ minimize pre-treatment MSE:\n",
    "\n",
    "$$\\hat{w}^{sc} = \\arg\\min_w \\sum_{t \\leq T_0} \\left( Y_{tr,t} - \\sum_{i \\in co} w_i Y_{i,t} \\right)^2$$\n",
    "\n",
    "Subject to: $\\sum_i w_i = 1$, $w_i \\geq 0$\n",
    "\n",
    "**Key limitation**: SC uses only post-treatment outcomes, ignoring pre/post change structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Synthetic Difference-in-Differences\n",
    "\n",
    "SDID combines both approaches via a **weighted two-way fixed effects** regression:\n",
    "\n",
    "$$\\hat{\\tau}^{sdid} = \\underset{\\mu, \\alpha, \\beta, \\tau}{\\arg\\min} \\left\\{ \\sum_{i,t} \\left( Y_{it} - \\mu - \\alpha_i - \\beta_t - \\tau \\cdot D_{it} \\right)^2 \\hat{w}_i \\hat{\\lambda}_t \\right\\}$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{w}_i$: **Unit weights** (like SC, match pre-treatment trends)\n",
    "- $\\hat{\\lambda}_t$: **Time weights** (balance pre/post periods for controls)\n",
    "- $\\alpha_i$: Unit fixed effects\n",
    "- $\\beta_t$: Time fixed effects\n",
    "- $D_{it}$: Treatment indicator (1 if treated unit in post period)\n",
    "\n",
    "**The key innovation**: SDID estimates both unit weights (like SC) AND time weights\n",
    "(novel contribution), then runs weighted TWFE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Unit Weight Estimation\n",
    "\n",
    "SDID unit weights solve a modified SC problem with an **intercept** and **L2 penalty**:\n",
    "\n",
    "$$\\hat{w}^{sdid} = \\arg\\min_{w_0, w} \\sum_{t \\leq T_0} \\left( w_0 + \\sum_{i \\in co} w_i Y_{it} - \\bar{Y}_{tr,t} \\right)^2 + \\zeta^2 T_{pre} \\|w\\|_2^2$$\n",
    "\n",
    "Subject to: $\\sum_{i} w_i = 1$, $w_i \\geq 0$\n",
    "\n",
    "Key differences from SC:\n",
    "1. **Intercept $w_0$**: Allows level shift, reducing sensitivity to scale\n",
    "2. **L2 penalty**: Regularization parameter $\\zeta$ prevents overfitting\n",
    "\n",
    "The regularization parameter is calibrated as:\n",
    "$$\\zeta = (N_{tr} \\cdot T_{post})^{1/4} \\cdot \\hat{\\sigma}(\\Delta_{it})$$\n",
    "\n",
    "Where $\\hat{\\sigma}(\\Delta_{it})$ is the standard deviation of first-differences for control units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Time Weight Estimation\n",
    "\n",
    "Time weights balance the pre-treatment periods to match post-treatment periods for controls:\n",
    "\n",
    "$$\\hat{\\lambda}^{sdid} = \\arg\\min_{\\lambda_0, \\lambda} \\sum_{i \\in co} \\left( \\lambda_0 + \\sum_{t \\leq T_0} \\lambda_t Y_{it} - \\frac{1}{T_{post}}\\sum_{t > T_0} Y_{it} \\right)^2$$\n",
    "\n",
    "Subject to: $\\sum_t \\lambda_t = 1$, $\\lambda_t \\geq 0$\n",
    "\n",
    "**Intuition**: Find pre-treatment periods that best predict post-treatment outcomes for controls.\n",
    "This ensures DiD comparison uses comparable time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Proposition 99: California's Tobacco Control Program\n",
    "\n",
    "We use the classic Proposition 99 dataset from Abadie et al. (2010):\n",
    "- **Treated**: California (passed tobacco tax in 1988)\n",
    "- **Controls**: 38 other US states\n",
    "- **Outcome**: Per-capita cigarette sales\n",
    "- **Pre-period**: 1970-1988\n",
    "- **Post-period**: 1989-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the smoking data\ncigar = load_facure_data(\"smoking.csv\")\n\nprint(f\"Dataset shape: {cigar.shape}\")\nprint(f\"States: {cigar['state'].nunique()}\")\nprint(f\"Years: {cigar['year'].min()} - {cigar['year'].max()}\")\nprint(f\"\\nColumns: {cigar.columns.tolist()}\")\n\n# Show only the columns we'll use (state, year, cigsale)\ncigar[['state', 'year', 'cigsale']].head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify California (state 3)\n",
    "calif_state = 3\n",
    "treatment_year = 1989\n",
    "\n",
    "# Create treatment indicators\n",
    "cigar['treated'] = (cigar['state'] == calif_state).astype(int)\n",
    "cigar['post'] = (cigar['year'] >= treatment_year).astype(int)\n",
    "cigar['D'] = cigar['treated'] * cigar['post']\n",
    "\n",
    "print(f\"California observations: {cigar['treated'].sum()}\")\n",
    "print(f\"Control observations: {(~cigar['treated'].astype(bool)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the block matrix structure\n",
    "T_pre = (cigar['year'] < treatment_year).sum() // cigar['state'].nunique()\n",
    "T_post = (cigar['year'] >= treatment_year).sum() // cigar['state'].nunique()\n",
    "N_co = cigar['state'].nunique() - 1\n",
    "N_tr = 1\n",
    "\n",
    "print(f\"Block matrix dimensions:\")\n",
    "print(f\"  Pre-treatment periods (T_pre): {T_pre}\")\n",
    "print(f\"  Post-treatment periods (T_post): {T_post}\")\n",
    "print(f\"  Control units (N_co): {N_co}\")\n",
    "print(f\"  Treated units (N_tr): {N_tr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format for block operations\n",
    "Y_wide = cigar.pivot(index='state', columns='year', values='cigsale')\n",
    "\n",
    "# Extract blocks\n",
    "pre_years = [y for y in Y_wide.columns if y < treatment_year]\n",
    "post_years = [y for y in Y_wide.columns if y >= treatment_year]\n",
    "\n",
    "Y_co_pre = Y_wide.loc[Y_wide.index != calif_state, pre_years].values\n",
    "Y_co_post = Y_wide.loc[Y_wide.index != calif_state, post_years].values\n",
    "Y_tr_pre = Y_wide.loc[calif_state, pre_years].values.reshape(1, -1)\n",
    "Y_tr_post = Y_wide.loc[calif_state, post_years].values.reshape(1, -1)\n",
    "\n",
    "print(f\"Y_co_pre shape: {Y_co_pre.shape}\")\n",
    "print(f\"Y_co_post shape: {Y_co_post.shape}\")\n",
    "print(f\"Y_tr_pre shape: {Y_tr_pre.shape}\")\n",
    "print(f\"Y_tr_post shape: {Y_tr_post.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard DiD Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard DiD: equal weighting\n",
    "did_treated_change = Y_tr_post.mean() - Y_tr_pre.mean()\n",
    "did_control_change = Y_co_post.mean() - Y_co_pre.mean()\n",
    "tau_did = did_treated_change - did_control_change\n",
    "\n",
    "print(f\"DiD Estimate:\")\n",
    "print(f\"  Treated change (post - pre): {did_treated_change:.2f}\")\n",
    "print(f\"  Control change (post - pre): {did_control_change:.2f}\")\n",
    "print(f\"  DiD effect (tau_did): {tau_did:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Control Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_loss(W, X, y):\n",
    "    \"\"\"MSE loss for synthetic control weights.\"\"\"\n",
    "    return np.sqrt(np.mean((y - X @ W)**2))\n",
    "\n",
    "def get_sc_weights(X, y):\n",
    "    \"\"\"Estimate synthetic control weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray (T_pre, N_co)\n",
    "        Control unit outcomes in pre-period\n",
    "    y : ndarray (T_pre,)\n",
    "        Treated unit outcomes in pre-period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    W : ndarray (N_co,)\n",
    "        Optimal weights for control units\n",
    "    \"\"\"\n",
    "    n_controls = X.shape[1]\n",
    "    w_start = np.ones(n_controls) / n_controls\n",
    "    \n",
    "    weights = fmin_slsqp(\n",
    "        partial(sc_loss, X=X, y=y),\n",
    "        w_start,\n",
    "        f_eqcons=lambda w: np.sum(w) - 1,\n",
    "        bounds=[(0.0, 1.0)] * n_controls,\n",
    "        disp=False\n",
    "    )\n",
    "    return weights\n",
    "\n",
    "# Estimate SC weights\n",
    "# X is (T_pre, N_co), y is (T_pre,)\n",
    "X_sc = Y_co_pre.T  # (T_pre, N_co)\n",
    "y_sc = Y_tr_pre.flatten()  # (T_pre,)\n",
    "\n",
    "w_sc = get_sc_weights(X_sc, y_sc)\n",
    "\n",
    "print(f\"SC Weights (top 10 by magnitude):\")\n",
    "state_names = Y_wide.index[Y_wide.index != calif_state].tolist()\n",
    "weight_df = pd.DataFrame({'state': state_names, 'weight': w_sc})\n",
    "print(weight_df.nlargest(10, 'weight').to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SC estimate: compare treated post to synthetic post\n",
    "Y_synth_post = Y_co_post.T @ w_sc  # (T_post,)\n",
    "tau_sc = Y_tr_post.mean() - Y_synth_post.mean()\n",
    "\n",
    "print(f\"SC Estimate:\")\n",
    "print(f\"  California post-treatment mean: {Y_tr_post.mean():.2f}\")\n",
    "print(f\"  Synthetic California post-treatment mean: {Y_synth_post.mean():.2f}\")\n",
    "print(f\"  SC effect (tau_sc): {tau_sc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: DiD vs SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three counterfactuals\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "years = np.array(pre_years + post_years)\n",
    "calif_series = np.concatenate([Y_tr_pre.flatten(), Y_tr_post.flatten()])\n",
    "control_mean = np.concatenate([Y_co_pre.mean(axis=0), Y_co_post.mean(axis=0)])\n",
    "synth_calif = np.concatenate([X_sc @ w_sc, Y_synth_post])\n",
    "\n",
    "# Left: Raw series\n",
    "ax = axes[0]\n",
    "ax.plot(years, calif_series, 'o-', color=COLORS['red'], label='California', linewidth=2)\n",
    "ax.plot(years, control_mean, 's--', color=COLORS['blue'], label='Control Mean (DiD)', alpha=0.7)\n",
    "ax.plot(years, synth_calif, '^--', color=COLORS['green'], label='Synthetic California (SC)', alpha=0.7)\n",
    "ax.axvline(treatment_year - 0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(treatment_year - 0.5, ax.get_ylim()[1], 'Prop 99', ha='center', va='bottom', fontsize=9)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Cigarette Sales (packs per capita)')\n",
    "ax.set_title('California vs Counterfactuals')\n",
    "ax.legend(loc='upper right')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Right: Gap (treatment effect over time)\n",
    "ax = axes[1]\n",
    "gap_did = calif_series - control_mean\n",
    "gap_sc = calif_series - synth_calif\n",
    "\n",
    "ax.plot(years, gap_did, 's-', color=COLORS['blue'], label=f'DiD Gap (ATT = {tau_did:.1f})', alpha=0.7)\n",
    "ax.plot(years, gap_sc, '^-', color=COLORS['green'], label=f'SC Gap (ATT = {tau_sc:.1f})', alpha=0.7)\n",
    "ax.axvline(treatment_year - 0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Gap (California - Counterfactual)')\n",
    "ax.set_title('Treatment Effect Gap')\n",
    "ax.legend(loc='lower left')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Insight: SC provides better pre-treatment fit (gap \u2248 0),\")\n",
    "print(f\"but DiD explicitly models the change structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### SDID as Weighted TWFE\n",
    "\n",
    "The key insight of SDID: use SC-style unit weights + novel time weights, then run TWFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zeta(Y_co_pre, N_tr, T_post):\n",
    "    \"\"\"Compute regularization parameter for SDID unit weights.\n",
    "    \n",
    "    zeta = (N_tr * T_post)^(1/4) * std(first_differences)\n",
    "    \"\"\"\n",
    "    # First differences for control units\n",
    "    delta = np.diff(Y_co_pre, axis=1)  # (N_co, T_pre-1)\n",
    "    sigma_delta = np.std(delta)\n",
    "    \n",
    "    zeta = (N_tr * T_post) ** 0.25 * sigma_delta\n",
    "    return zeta\n",
    "\n",
    "zeta = compute_zeta(Y_co_pre, N_tr, T_post)\n",
    "print(f\"Regularization parameter zeta: {zeta:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdid_unit_loss(params, X, y, zeta, T_pre):\n",
    "    \"\"\"Loss function for SDID unit weights with intercept and L2 penalty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : ndarray (N_co + 1,)\n",
    "        First element is intercept w0, rest are weights\n",
    "    X : ndarray (T_pre, N_co)\n",
    "        Control outcomes in pre-period\n",
    "    y : ndarray (T_pre,)\n",
    "        Treated outcomes in pre-period  \n",
    "    zeta : float\n",
    "        Regularization parameter\n",
    "    T_pre : int\n",
    "        Number of pre-treatment periods\n",
    "    \"\"\"\n",
    "    w0 = params[0]  # intercept\n",
    "    w = params[1:]  # unit weights\n",
    "    \n",
    "    # Prediction error\n",
    "    pred = w0 + X @ w\n",
    "    mse = np.mean((y - pred)**2)\n",
    "    \n",
    "    # L2 penalty (scaled by T_pre as in paper)\n",
    "    penalty = zeta**2 * T_pre * np.sum(w**2)\n",
    "    \n",
    "    return mse + penalty\n",
    "\n",
    "def get_sdid_unit_weights(X, y, zeta, T_pre):\n",
    "    \"\"\"Estimate SDID unit weights with intercept and L2 penalty.\"\"\"\n",
    "    n_controls = X.shape[1]\n",
    "    \n",
    "    # Initial: intercept=0, equal weights\n",
    "    params_start = np.concatenate([[0], np.ones(n_controls) / n_controls])\n",
    "    \n",
    "    # Constraints: weights sum to 1, weights >= 0 (intercept unconstrained)\n",
    "    def weight_sum_constraint(params):\n",
    "        return np.sum(params[1:]) - 1\n",
    "    \n",
    "    # Bounds: intercept unbounded, weights in [0, 1]\n",
    "    bounds = [(None, None)] + [(0.0, 1.0)] * n_controls\n",
    "    \n",
    "    result = fmin_slsqp(\n",
    "        partial(sdid_unit_loss, X=X, y=y, zeta=zeta, T_pre=T_pre),\n",
    "        params_start,\n",
    "        f_eqcons=weight_sum_constraint,\n",
    "        bounds=bounds,\n",
    "        disp=False\n",
    "    )\n",
    "    \n",
    "    w0 = result[0]\n",
    "    w = result[1:]\n",
    "    return w0, w\n",
    "\n",
    "# Estimate SDID unit weights\n",
    "w0_sdid, w_sdid = get_sdid_unit_weights(X_sc, y_sc, zeta, T_pre)\n",
    "\n",
    "print(f\"SDID Unit Weights:\")\n",
    "print(f\"  Intercept (w0): {w0_sdid:.4f}\")\n",
    "print(f\"  Number of non-zero weights: {np.sum(w_sdid > 0.01)}\")\n",
    "\n",
    "# Compare to SC weights\n",
    "print(f\"\\nTop 5 states by SDID weight:\")\n",
    "sdid_weight_df = pd.DataFrame({'state': state_names, 'sdid_weight': w_sdid, 'sc_weight': w_sc})\n",
    "print(sdid_weight_df.nlargest(5, 'sdid_weight').to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdid_time_loss(params, Y_co, T_pre):\n",
    "    \"\"\"Loss function for SDID time weights.\n",
    "    \n",
    "    Find pre-period weights that predict post-period mean for controls.\n",
    "    \"\"\"\n",
    "    lambda0 = params[0]  # intercept\n",
    "    lam = params[1:]  # time weights\n",
    "    \n",
    "    # Y_co: (N_co, T_total)\n",
    "    Y_co_pre = Y_co[:, :T_pre]\n",
    "    Y_co_post = Y_co[:, T_pre:]\n",
    "    \n",
    "    # For each control unit, predict post-mean from weighted pre-period\n",
    "    # pred_i = lambda0 + sum_t(lambda_t * Y_it_pre)\n",
    "    pred = lambda0 + Y_co_pre @ lam  # (N_co,)\n",
    "    target = Y_co_post.mean(axis=1)  # (N_co,)\n",
    "    \n",
    "    return np.mean((target - pred)**2)\n",
    "\n",
    "def get_sdid_time_weights(Y_co, T_pre):\n",
    "    \"\"\"Estimate SDID time weights.\"\"\"\n",
    "    # Initial: intercept=0, equal weights\n",
    "    params_start = np.concatenate([[0], np.ones(T_pre) / T_pre])\n",
    "    \n",
    "    def time_sum_constraint(params):\n",
    "        return np.sum(params[1:]) - 1\n",
    "    \n",
    "    bounds = [(None, None)] + [(0.0, 1.0)] * T_pre\n",
    "    \n",
    "    result = fmin_slsqp(\n",
    "        partial(sdid_time_loss, Y_co=Y_co, T_pre=T_pre),\n",
    "        params_start,\n",
    "        f_eqcons=time_sum_constraint,\n",
    "        bounds=bounds,\n",
    "        disp=False\n",
    "    )\n",
    "    \n",
    "    lambda0 = result[0]\n",
    "    lam = result[1:]\n",
    "    return lambda0, lam\n",
    "\n",
    "# Stack controls for time weight estimation\n",
    "Y_co_full = np.hstack([Y_co_pre, Y_co_post])  # (N_co, T_total)\n",
    "\n",
    "lambda0_sdid, lambda_sdid = get_sdid_time_weights(Y_co_full, T_pre)\n",
    "\n",
    "print(f\"SDID Time Weights:\")\n",
    "print(f\"  Intercept (lambda0): {lambda0_sdid:.4f}\")\n",
    "print(f\"  Number of non-zero time weights: {np.sum(lambda_sdid > 0.01)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time weights\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.bar(pre_years, lambda_sdid, color=COLORS['blue'], alpha=0.7, edgecolor='black')\n",
    "ax.axhline(1/T_pre, color=COLORS['red'], linestyle='--', \n",
    "           label=f'Equal weight (DiD): {1/T_pre:.3f}')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Time Weight')\n",
    "ax.set_title('SDID Time Weights (Pre-treatment Periods)')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNote: Time weights concentrate on periods most predictive of post-treatment.\")\n",
    "print(f\"This ensures the DiD comparison uses informative pre-periods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDID Estimate via Weighted Double-Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdid_estimate(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post, w, lam):\n",
    "    \"\"\"Compute SDID estimate using weighted double-difference formula.\n",
    "    \n",
    "    tau_sdid = (Y_tr_post_mean - w'Y_co_post_mean) - (lambda'Y_tr_pre - lambda'(w'Y_co_pre))\n",
    "    \n",
    "    This is the \"basic\" SDID estimator from Arkhangelsky et al. (2021).\n",
    "    \"\"\"\n",
    "    # Post-period comparison (weighted by unit weights)\n",
    "    Y_tr_post_mean = Y_tr_post.mean()\n",
    "    Y_co_post_weighted = (Y_co_post.T @ w).mean()  # w-weighted control post mean\n",
    "    \n",
    "    # Pre-period comparison (weighted by time weights)\n",
    "    Y_tr_pre_weighted = Y_tr_pre.flatten() @ lam  # lambda-weighted treated pre\n",
    "    Y_co_pre_weighted = (Y_co_pre.T @ w) @ lam  # both weights for controls pre\n",
    "    \n",
    "    # Double difference\n",
    "    tau = (Y_tr_post_mean - Y_co_post_weighted) - (Y_tr_pre_weighted - Y_co_pre_weighted)\n",
    "    \n",
    "    return tau\n",
    "\n",
    "tau_sdid = sdid_estimate(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post, w_sdid, lambda_sdid)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"COMPARISON OF ESTIMATORS\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"DiD estimate:  {tau_did:.2f} packs/capita\")\n",
    "print(f\"SC estimate:   {tau_sc:.2f} packs/capita\")\n",
    "print(f\"SDID estimate: {tau_sdid:.2f} packs/capita\")\n",
    "print(f\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all three counterfactuals\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# California actual\n",
    "ax.plot(years, calif_series, 'o-', color=COLORS['red'], \n",
    "        label='California (actual)', linewidth=2, markersize=5)\n",
    "\n",
    "# DiD counterfactual (control mean + parallel shift)\n",
    "did_shift = Y_tr_pre.mean() - Y_co_pre.mean(axis=0).mean()\n",
    "did_counterfactual = control_mean + did_shift\n",
    "ax.plot(years, did_counterfactual, 's--', color=COLORS['blue'], \n",
    "        label=f'DiD Counterfactual (ATT={tau_did:.1f})', alpha=0.7)\n",
    "\n",
    "# SC counterfactual\n",
    "ax.plot(years, synth_calif, '^--', color=COLORS['green'], \n",
    "        label=f'SC Counterfactual (ATT={tau_sc:.1f})', alpha=0.7)\n",
    "\n",
    "# SDID counterfactual\n",
    "sdid_pre = w0_sdid + X_sc @ w_sdid\n",
    "sdid_post = Y_co_post.T @ w_sdid\n",
    "# Add the pre-period weighted difference\n",
    "pre_gap = Y_tr_pre.flatten() - sdid_pre\n",
    "pre_gap_weighted = pre_gap @ lambda_sdid\n",
    "sdid_counterfactual = np.concatenate([sdid_pre + pre_gap_weighted, sdid_post + pre_gap_weighted])\n",
    "\n",
    "ax.plot(years, sdid_counterfactual, 'D--', color=COLORS['orange'], \n",
    "        label=f'SDID Counterfactual (ATT={tau_sdid:.1f})', alpha=0.7)\n",
    "\n",
    "ax.axvline(treatment_year - 0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(treatment_year - 0.5, ax.get_ylim()[1] * 0.98, 'Prop 99', \n",
    "        ha='center', va='top', fontsize=9, style='italic')\n",
    "\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Cigarette Sales (packs per capita)')\n",
    "ax.set_title('Proposition 99: Comparing Counterfactual Methods')\n",
    "ax.legend(loc='upper right')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Why SDID Differs from SC and DiD\n",
    "\n",
    "```\n",
    "\u2605 Insight \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "SDID typically falls between DiD and SC estimates because:\n",
    "\n",
    "1. Unit weights (like SC): Emphasize controls with similar trends\n",
    "2. Time weights (novel): Emphasize pre-periods most predictive of post\n",
    "3. Fixed effects (like DiD): Model level differences explicitly\n",
    "\n",
    "The regularization parameter zeta controls sparsity:\n",
    "- Large zeta \u2192 More uniform weights (closer to DiD)\n",
    "- Small zeta \u2192 Sparser weights (closer to SC)\n",
    "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: What problem does SDID solve that DiD and SC don't?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **DiD limitation**: Assumes parallel trends hold for ALL control units equally. \n",
    "   If some controls have different trends, the estimate is biased.\n",
    "\n",
    "2. **SC limitation**: Focuses only on matching pre-treatment levels/trends.\n",
    "   Ignores the change structure that DiD exploits.\n",
    "\n",
    "3. **SDID solution**: \n",
    "   - Unit weights: Find controls most similar to treated (like SC)\n",
    "   - Time weights: Find pre-periods most predictive of post (novel)\n",
    "   - Fixed effects: Model level shifts (like DiD)\n",
    "\n",
    "4. **Practical benefit**: More robust to violations of parallel trends because\n",
    "   it down-weights controls with dissimilar trends.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Explain the role of the regularization parameter $\\zeta$ in SDID.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Formula**: $\\zeta = (N_{tr} \\cdot T_{post})^{1/4} \\cdot \\hat{\\sigma}(\\Delta)$\n",
    "\n",
    "2. **Role**: Controls the L2 penalty on unit weights in the optimization.\n",
    "\n",
    "3. **Effect**:\n",
    "   - Large $\\zeta$ \u2192 Strong penalty \u2192 More uniform weights (DiD-like)\n",
    "   - Small $\\zeta$ \u2192 Weak penalty \u2192 Sparser weights (SC-like)\n",
    "\n",
    "4. **Why this formula?**:\n",
    "   - Scales with treatment intensity ($N_{tr} \\cdot T_{post}$)\n",
    "   - Scales with outcome variance ($\\sigma(\\Delta)$)\n",
    "   - Ensures good finite-sample properties\n",
    "\n",
    "5. **Interview insight**: The $1/4$ power comes from optimal rate theory\u2014\n",
    "   it balances bias from regularization vs variance from noisy weights.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: When would you prefer SDID over standard DiD?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Prefer SDID when:**\n",
    "\n",
    "1. **Few treated units**: SC-style weighting helps with limited treatment data\n",
    "2. **Heterogeneous controls**: Not all controls follow parallel trends\n",
    "3. **Long pre-period**: Time weights can identify most informative periods\n",
    "4. **Pre-trends concern**: SDID explicitly addresses this via weighting\n",
    "\n",
    "**Prefer standard DiD when:**\n",
    "\n",
    "1. **Many treated units**: Averaging is efficient\n",
    "2. **Homogeneous controls**: All controls are valid comparisons  \n",
    "3. **Strong parallel trends**: Visual evidence supports assumption\n",
    "4. **Simplicity needed**: DiD is easier to interpret and explain\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Arkhangelsky, D., Athey, S., Hirshberg, D., Imbens, G., & Wager, S. (2021). \n",
    "      Synthetic Difference-in-Differences. *American Economic Review*, 111(12), 4088-4118.\n",
    "\n",
    "[^2]: Abadie, A., Diamond, A., & Hainmueller, J. (2010). \n",
    "      Synthetic Control Methods for Comparative Case Studies. \n",
    "      *Journal of the American Statistical Association*, 105(490), 493-505.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, Chapter 25."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}