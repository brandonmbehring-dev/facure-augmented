{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDID: Staggered Adoption and Inference\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Chapter 25 | Notebook 3 of 3**\n",
    "\n",
    "This notebook covers inference for SDID using placebo variance estimation,\n",
    "and extends to staggered adoption settings with multiple treatment cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from augmented.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "# Additional imports\n",
    "import cvxpy as cp\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### The Inference Challenge\n",
    "\n",
    "SDID produces a point estimate, but how do we construct confidence intervals?\n",
    "\n",
    "**Challenges:**\n",
    "1. Weights are estimated (not known)\n",
    "2. Few treated units â†’ asymptotic theory may not apply\n",
    "3. Clustering structure unknown\n",
    "\n",
    "**Solution: Placebo Variance Estimation**\n",
    "\n",
    "The key insight: estimate variance by applying SDID to **control units as if treated**.\n",
    "\n",
    "> For each control unit $j$:\n",
    "> 1. Pretend unit $j$ was treated\n",
    "> 2. Use remaining controls to estimate \"placebo effect\"\n",
    "> 3. Under null (no effect), placebo effects should be noise\n",
    "> 4. Use variance of placebos to estimate SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staggered Adoption\n",
    "\n",
    "In many applications, treatment rolls out over time:\n",
    "\n",
    "| Unit | 2015 | 2016 | 2017 | 2018 | 2019 |\n",
    "|------|------|------|------|------|------|\n",
    "| A    |  0   |  0   |  1   |  1   |  1   |\n",
    "| B    |  0   |  0   |  0   |  1   |  1   |\n",
    "| C    |  0   |  0   |  0   |  0   |  1   |\n",
    "| D    |  0   |  0   |  0   |  0   |  0   |\n",
    "\n",
    "**SDID with staggered adoption:**\n",
    "1. Apply SDID separately for each treatment cohort\n",
    "2. Pool estimates using inverse-variance weighting\n",
    "3. Account for using treated units as controls for later cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Placebo Variance Estimator\n",
    "\n",
    "For each control unit $j \\in \\{1, ..., N_{co}\\}$:\n",
    "\n",
    "1. Treat unit $j$ as if it were treated\n",
    "2. Estimate SDID using remaining $N_{co} - 1$ controls\n",
    "3. Compute placebo estimate $\\hat{\\tau}_j^{placebo}$\n",
    "\n",
    "The variance estimator:\n",
    "\n",
    "$$\\hat{V}^{placebo} = \\frac{N_{co}}{N_{co} - 1} \\cdot \\frac{1}{N_{co}} \\sum_{j=1}^{N_{co}} (\\hat{\\tau}_j^{placebo})^2$$\n",
    "\n",
    "Under the null hypothesis of no treatment effect:\n",
    "- True placebo effects are zero\n",
    "- Estimated placebos reflect sampling variance\n",
    "- $\\sqrt{\\hat{V}^{placebo}}$ estimates the standard error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Confidence Intervals\n",
    "\n",
    "The $(1-\\alpha)$ confidence interval:\n",
    "\n",
    "$$\\hat{\\tau}^{sdid} \\pm z_{1-\\alpha/2} \\cdot \\sqrt{\\hat{V}^{placebo}}$$\n",
    "\n",
    "**Notes:**\n",
    "1. Uses normal approximation (may need adjustment for small $N_{co}$)\n",
    "2. Placebo variance accounts for weight estimation uncertainty\n",
    "3. Valid under assumption that placebo units behave like treated unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Staggered Adoption Estimator\n",
    "\n",
    "With cohorts $g \\in \\{g_1, ..., g_K\\}$ (treatment timing):\n",
    "\n",
    "1. For each cohort $g$, define:\n",
    "   - Treated units: Units first treated at time $g$\n",
    "   - Control units: Units not yet treated by time $g$\n",
    "   \n",
    "2. Estimate cohort-specific effect: $\\hat{\\tau}_g^{sdid}$\n",
    "\n",
    "3. Aggregate:\n",
    "$$\\hat{\\tau}^{staggered} = \\sum_g \\omega_g \\hat{\\tau}_g^{sdid}$$\n",
    "\n",
    "Where weights $\\omega_g$ can be:\n",
    "- **Sample size**: $\\omega_g \\propto N_{tr,g}$\n",
    "- **Inverse variance**: $\\omega_g \\propto 1/\\hat{V}_g$\n",
    "- **Post-treatment periods**: $\\omega_g \\propto T_{post,g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Load Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load smoking data\n",
    "cigar = load_facure_data(\"smoking.csv\")\n",
    "\n",
    "# Setup\n",
    "calif_state = 3\n",
    "treatment_year = 1989\n",
    "\n",
    "# Pivot to wide format\n",
    "Y_wide = cigar.pivot(index='state', columns='year', values='cigsale')\n",
    "\n",
    "# Extract blocks\n",
    "pre_years = [y for y in Y_wide.columns if y < treatment_year]\n",
    "post_years = [y for y in Y_wide.columns if y >= treatment_year]\n",
    "\n",
    "Y_co_pre = Y_wide.loc[Y_wide.index != calif_state, pre_years].values\n",
    "Y_co_post = Y_wide.loc[Y_wide.index != calif_state, post_years].values\n",
    "Y_tr_pre = Y_wide.loc[calif_state, pre_years].values\n",
    "Y_tr_post = Y_wide.loc[calif_state, post_years].values\n",
    "\n",
    "control_states = Y_wide.index[Y_wide.index != calif_state].values\n",
    "N_co, T_pre = Y_co_pre.shape\n",
    "T_post = Y_co_post.shape[1]\n",
    "\n",
    "print(f\"Setup: {N_co} control states, {T_pre} pre-periods, {T_post} post-periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDID Point Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdid_estimate_full(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post, zeta=None):\n",
    "    \"\"\"\n",
    "    Full SDID estimation with optional zeta specification.\n",
    "    \n",
    "    Returns tau, w, lambda, zeta\n",
    "    \"\"\"\n",
    "    N_co, T_pre = Y_co_pre.shape\n",
    "    T_post = Y_co_post.shape[1]\n",
    "    N_tr = 1\n",
    "    \n",
    "    # Compute zeta if not provided\n",
    "    if zeta is None:\n",
    "        delta = np.diff(Y_co_pre, axis=1)\n",
    "        sigma = np.std(delta, ddof=1)\n",
    "        zeta = (N_tr * T_post) ** 0.25 * sigma\n",
    "    \n",
    "    # Unit weights\n",
    "    X_unit = Y_co_pre.T\n",
    "    y_unit = Y_tr_pre.flatten()\n",
    "    \n",
    "    w = cp.Variable(N_co)\n",
    "    w0 = cp.Variable()\n",
    "    pred = w0 + X_unit @ w\n",
    "    obj = cp.Minimize(cp.sum_squares(pred - y_unit) + zeta**2 * T_pre * cp.sum_squares(w))\n",
    "    prob = cp.Problem(obj, [cp.sum(w) == 1, w >= 0])\n",
    "    prob.solve(solver=cp.OSQP, verbose=False)\n",
    "    w_hat = w.value\n",
    "    \n",
    "    # Time weights\n",
    "    y_target = Y_co_post.mean(axis=1)\n",
    "    lam = cp.Variable(T_pre)\n",
    "    lam0 = cp.Variable()\n",
    "    pred_t = lam0 + Y_co_pre @ lam\n",
    "    obj_t = cp.Minimize(cp.sum_squares(pred_t - y_target))\n",
    "    prob_t = cp.Problem(obj_t, [cp.sum(lam) == 1, lam >= 0])\n",
    "    prob_t.solve(solver=cp.OSQP, verbose=False)\n",
    "    lam_hat = lam.value\n",
    "    \n",
    "    # SDID estimate\n",
    "    Y_tr_post_mean = Y_tr_post.mean()\n",
    "    Y_co_post_weighted = (Y_co_post.T @ w_hat).mean()\n",
    "    Y_tr_pre_weighted = Y_tr_pre.flatten() @ lam_hat\n",
    "    Y_co_pre_weighted = (Y_co_pre.T @ w_hat) @ lam_hat\n",
    "    \n",
    "    tau = (Y_tr_post_mean - Y_co_post_weighted) - (Y_tr_pre_weighted - Y_co_pre_weighted)\n",
    "    \n",
    "    return tau, w_hat, lam_hat, zeta\n",
    "\n",
    "# Estimate SDID\n",
    "tau_sdid, w_sdid, lam_sdid, zeta = sdid_estimate_full(\n",
    "    Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post\n",
    ")\n",
    "\n",
    "print(f\"SDID Point Estimate: {tau_sdid:.2f} packs/capita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placebo Variance Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placebo_estimate(j, Y_co_pre, Y_co_post, zeta):\n",
    "    \"\"\"\n",
    "    Compute placebo SDID estimate treating control unit j as treated.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    j : int\n",
    "        Index of control unit to treat as placebo treated\n",
    "    Y_co_pre : ndarray (N_co, T_pre)\n",
    "        All control units pre-treatment\n",
    "    Y_co_post : ndarray (N_co, T_post)\n",
    "        All control units post-treatment\n",
    "    zeta : float\n",
    "        Regularization parameter (from main estimation)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tau_placebo : float\n",
    "        Placebo treatment effect estimate\n",
    "    \"\"\"\n",
    "    # Treat unit j as \"treated\"\n",
    "    Y_placebo_tr_pre = Y_co_pre[j, :]\n",
    "    Y_placebo_tr_post = Y_co_post[j, :]\n",
    "    \n",
    "    # Use remaining units as controls\n",
    "    mask = np.ones(Y_co_pre.shape[0], dtype=bool)\n",
    "    mask[j] = False\n",
    "    Y_placebo_co_pre = Y_co_pre[mask, :]\n",
    "    Y_placebo_co_post = Y_co_post[mask, :]\n",
    "    \n",
    "    # Estimate SDID with fixed zeta\n",
    "    tau, _, _, _ = sdid_estimate_full(\n",
    "        Y_placebo_tr_pre, Y_placebo_tr_post,\n",
    "        Y_placebo_co_pre, Y_placebo_co_post,\n",
    "        zeta=zeta\n",
    "    )\n",
    "    \n",
    "    return tau\n",
    "\n",
    "# Compute placebo estimates for all control units\n",
    "print(\"Computing placebo estimates...\")\n",
    "placebo_effects = Parallel(n_jobs=-1)(\n",
    "    delayed(placebo_estimate)(j, Y_co_pre, Y_co_post, zeta) \n",
    "    for j in range(N_co)\n",
    ")\n",
    "placebo_effects = np.array(placebo_effects)\n",
    "\n",
    "print(f\"Computed {len(placebo_effects)} placebo estimates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placebo variance estimator\n",
    "V_placebo = (N_co / (N_co - 1)) * np.mean(placebo_effects**2)\n",
    "SE_placebo = np.sqrt(V_placebo)\n",
    "\n",
    "# 95% confidence interval\n",
    "alpha = 0.05\n",
    "z_crit = norm.ppf(1 - alpha/2)\n",
    "ci_lower = tau_sdid - z_crit * SE_placebo\n",
    "ci_upper = tau_sdid + z_crit * SE_placebo\n",
    "\n",
    "print(f\"\\nInference Results:\")\n",
    "print(f\"  Point estimate: {tau_sdid:.2f}\")\n",
    "print(f\"  Placebo SE: {SE_placebo:.2f}\")\n",
    "print(f\"  95% CI: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "print(f\"  t-statistic: {tau_sdid / SE_placebo:.2f}\")\n",
    "print(f\"  p-value: {2 * (1 - norm.cdf(abs(tau_sdid / SE_placebo))):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize placebo distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Histogram of placebos\n",
    "ax = axes[0]\n",
    "ax.hist(placebo_effects, bins=15, color=COLORS['blue'], alpha=0.7, \n",
    "        edgecolor='black', density=True)\n",
    "ax.axvline(tau_sdid, color=COLORS['red'], linewidth=2, \n",
    "           label=f'California: {tau_sdid:.1f}')\n",
    "ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Treatment Effect Estimate')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of Placebo Effects')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Right: Sorted placebo effects with California\n",
    "ax = axes[1]\n",
    "all_effects = np.concatenate([placebo_effects, [tau_sdid]])\n",
    "all_labels = list(control_states) + ['California']\n",
    "sort_idx = np.argsort(all_effects)\n",
    "\n",
    "colors = [COLORS['red'] if all_labels[i] == 'California' else COLORS['blue'] \n",
    "          for i in sort_idx]\n",
    "ax.barh(range(len(all_effects)), all_effects[sort_idx], color=colors, alpha=0.7)\n",
    "ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Treatment Effect Estimate')\n",
    "ax.set_ylabel('State Rank')\n",
    "ax.set_title('California vs Placebo Effects (Ranked)')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Fisher exact p-value\n",
    "rank = np.sum(all_effects <= tau_sdid)\n",
    "p_fisher = rank / len(all_effects)\n",
    "print(f\"\\nFisher Exact Test:\")\n",
    "print(f\"  California rank: {rank} / {len(all_effects)}\")\n",
    "print(f\"  One-sided p-value: {p_fisher:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Complete SDID Class with Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDIDWithInference:\n",
    "    \"\"\"\n",
    "    SDID estimator with placebo-based inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_jobs=-1):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.tau_ = None\n",
    "        self.se_ = None\n",
    "        self.placebo_effects_ = None\n",
    "        self.zeta_ = None\n",
    "        self.w_ = None\n",
    "        self.lambda_ = None\n",
    "        \n",
    "    def fit(self, Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post):\n",
    "        \"\"\"\n",
    "        Fit SDID and compute inference.\n",
    "        \"\"\"\n",
    "        # Point estimate\n",
    "        self.tau_, self.w_, self.lambda_, self.zeta_ = sdid_estimate_full(\n",
    "            Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post\n",
    "        )\n",
    "        \n",
    "        # Placebo inference\n",
    "        N_co = Y_co_pre.shape[0]\n",
    "        self.placebo_effects_ = np.array(Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(placebo_estimate)(j, Y_co_pre, Y_co_post, self.zeta_)\n",
    "            for j in range(N_co)\n",
    "        ))\n",
    "        \n",
    "        # Variance estimate\n",
    "        V = (N_co / (N_co - 1)) * np.mean(self.placebo_effects_**2)\n",
    "        self.se_ = np.sqrt(V)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def confidence_interval(self, alpha=0.05):\n",
    "        \"\"\"Return (1-alpha) confidence interval.\"\"\"\n",
    "        z = norm.ppf(1 - alpha/2)\n",
    "        return self.tau_ - z * self.se_, self.tau_ + z * self.se_\n",
    "    \n",
    "    def pvalue(self, alternative='two-sided'):\n",
    "        \"\"\"P-value from normal approximation.\"\"\"\n",
    "        z = abs(self.tau_ / self.se_)\n",
    "        if alternative == 'two-sided':\n",
    "            return 2 * (1 - norm.cdf(z))\n",
    "        else:\n",
    "            return 1 - norm.cdf(z)\n",
    "    \n",
    "    def fisher_pvalue(self):\n",
    "        \"\"\"Fisher exact p-value from placebo distribution.\"\"\"\n",
    "        all_effects = np.concatenate([self.placebo_effects_, [self.tau_]])\n",
    "        rank = np.sum(all_effects <= self.tau_)\n",
    "        return rank / len(all_effects)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print summary of results.\"\"\"\n",
    "        ci_low, ci_high = self.confidence_interval()\n",
    "        print(f\"SDID Estimation Results\")\n",
    "        print(f\"=\" * 40)\n",
    "        print(f\"Treatment Effect: {self.tau_:.4f}\")\n",
    "        print(f\"Standard Error:   {self.se_:.4f}\")\n",
    "        print(f\"95% CI:          [{ci_low:.4f}, {ci_high:.4f}]\")\n",
    "        print(f\"t-statistic:     {self.tau_/self.se_:.4f}\")\n",
    "        print(f\"p-value (normal): {self.pvalue():.4f}\")\n",
    "        print(f\"p-value (Fisher): {self.fisher_pvalue():.4f}\")\n",
    "        print(f\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the class\n",
    "sdid_inf = SDIDWithInference(n_jobs=-1)\n",
    "sdid_inf.fit(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post)\n",
    "sdid_inf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staggered Adoption Example\n",
    "\n",
    "Let's simulate a staggered adoption scenario to demonstrate the methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate staggered adoption data\n",
    "np.random.seed(42)\n",
    "\n",
    "N_units = 50\n",
    "T_periods = 20\n",
    "treatment_times = {15: 5, 17: 3}  # cohort: n_treated\n",
    "\n",
    "# Generate base outcomes\n",
    "unit_fe = np.random.normal(0, 2, N_units)\n",
    "time_fe = np.cumsum(np.random.normal(0, 0.5, T_periods))\n",
    "Y_base = unit_fe[:, None] + time_fe[None, :] + np.random.normal(0, 1, (N_units, T_periods))\n",
    "\n",
    "# Assign treatment\n",
    "treatment_status = np.zeros((N_units, T_periods), dtype=int)\n",
    "cohort_assignment = np.zeros(N_units, dtype=int)  # 0 = never treated\n",
    "\n",
    "idx = 0\n",
    "for cohort, n_tr in treatment_times.items():\n",
    "    for i in range(n_tr):\n",
    "        treatment_status[idx, cohort:] = 1\n",
    "        cohort_assignment[idx] = cohort\n",
    "        idx += 1\n",
    "\n",
    "# True treatment effects (heterogeneous by cohort)\n",
    "true_effects = {15: -3.0, 17: -2.0}\n",
    "\n",
    "# Add treatment effects\n",
    "Y_obs = Y_base.copy()\n",
    "for cohort, effect in true_effects.items():\n",
    "    mask = cohort_assignment == cohort\n",
    "    Y_obs[mask, cohort:] += effect\n",
    "\n",
    "print(f\"Simulated Data:\")\n",
    "print(f\"  Total units: {N_units}\")\n",
    "print(f\"  Never treated: {np.sum(cohort_assignment == 0)}\")\n",
    "print(f\"  Cohort 15 (treated period 15+): {np.sum(cohort_assignment == 15)}, true effect: {true_effects[15]}\")\n",
    "print(f\"  Cohort 17 (treated period 17+): {np.sum(cohort_assignment == 17)}, true effect: {true_effects[17]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdid_staggered(Y_obs, cohort_assignment, cohort, treatment_time):\n",
    "    \"\"\"\n",
    "    SDID for a single cohort in staggered setting.\n",
    "    \n",
    "    Uses not-yet-treated units as controls.\n",
    "    \"\"\"\n",
    "    # Treated units: units in this cohort\n",
    "    treated_mask = cohort_assignment == cohort\n",
    "    \n",
    "    # Control units: units not yet treated by treatment_time\n",
    "    # (never treated OR treated later)\n",
    "    control_mask = (cohort_assignment == 0) | (cohort_assignment > cohort)\n",
    "    \n",
    "    # Extract data\n",
    "    Y_tr = Y_obs[treated_mask, :]\n",
    "    Y_co = Y_obs[control_mask, :]\n",
    "    \n",
    "    # Split into pre/post\n",
    "    Y_tr_pre = Y_tr[:, :treatment_time].mean(axis=0)  # Average across treated units\n",
    "    Y_tr_post = Y_tr[:, treatment_time:].mean(axis=0)\n",
    "    Y_co_pre = Y_co[:, :treatment_time]\n",
    "    Y_co_post = Y_co[:, treatment_time:]\n",
    "    \n",
    "    # Estimate SDID\n",
    "    tau, w, lam, zeta = sdid_estimate_full(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post)\n",
    "    \n",
    "    return tau, np.sum(treated_mask), np.sum(control_mask)\n",
    "\n",
    "# Estimate effects for each cohort\n",
    "cohort_results = {}\n",
    "for cohort, true_effect in true_effects.items():\n",
    "    tau, n_tr, n_co = sdid_staggered(Y_obs, cohort_assignment, cohort, cohort)\n",
    "    cohort_results[cohort] = {\n",
    "        'estimate': tau,\n",
    "        'true': true_effect,\n",
    "        'n_treated': n_tr,\n",
    "        'n_control': n_co\n",
    "    }\n",
    "\n",
    "print(f\"\\nStaggered SDID Results:\")\n",
    "print(f\"{'Cohort':<10} {'Estimate':<12} {'True':<10} {'N_tr':<8} {'N_co':<8}\")\n",
    "print(\"-\" * 48)\n",
    "for cohort, res in cohort_results.items():\n",
    "    print(f\"{cohort:<10} {res['estimate']:<12.2f} {res['true']:<10.2f} {res['n_treated']:<8} {res['n_control']:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate estimate (inverse-variance weighted would require SE estimation)\n",
    "# Here we use sample-size weighting for simplicity\n",
    "total_treated = sum(r['n_treated'] for r in cohort_results.values())\n",
    "tau_agg = sum(\n",
    "    r['estimate'] * r['n_treated'] / total_treated \n",
    "    for r in cohort_results.values()\n",
    ")\n",
    "true_agg = sum(\n",
    "    r['true'] * r['n_treated'] / total_treated \n",
    "    for r in cohort_results.values()\n",
    ")\n",
    "\n",
    "print(f\"\\nAggregated Estimate (sample-size weighted):\")\n",
    "print(f\"  SDID: {tau_agg:.2f}\")\n",
    "print(f\"  True: {true_agg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize staggered adoption\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Treatment rollout\n",
    "ax = axes[0]\n",
    "for cohort in sorted(treatment_times.keys()):\n",
    "    mask = cohort_assignment == cohort\n",
    "    Y_cohort = Y_obs[mask, :].mean(axis=0)\n",
    "    ax.plot(range(T_periods), Y_cohort, 'o-', label=f'Cohort {cohort}', markersize=4)\n",
    "    ax.axvline(cohort - 0.5, linestyle=':', alpha=0.5)\n",
    "\n",
    "# Never treated\n",
    "Y_never = Y_obs[cohort_assignment == 0, :].mean(axis=0)\n",
    "ax.plot(range(T_periods), Y_never, 's--', label='Never Treated', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Period')\n",
    "ax.set_ylabel('Outcome')\n",
    "ax.set_title('Staggered Adoption: Mean Outcomes by Cohort')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Right: Cohort-specific effects\n",
    "ax = axes[1]\n",
    "cohorts = list(cohort_results.keys())\n",
    "estimates = [cohort_results[c]['estimate'] for c in cohorts]\n",
    "trues = [cohort_results[c]['true'] for c in cohorts]\n",
    "\n",
    "x = np.arange(len(cohorts))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, estimates, width, label='SDID Estimate', color=COLORS['blue'], alpha=0.7)\n",
    "ax.bar(x + width/2, trues, width, label='True Effect', color=COLORS['red'], alpha=0.7)\n",
    "ax.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Cohort {c}' for c in cohorts])\n",
    "ax.set_ylabel('Treatment Effect')\n",
    "ax.set_title('SDID vs True Effects by Cohort')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: Why use placebo variance instead of bootstrap for SDID inference?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Few treated units**: Bootstrap requires resampling treated units. With\n",
    "   one treated unit (California), bootstrap doesn't apply directly.\n",
    "\n",
    "2. **Weight estimation**: Placebo approach naturally accounts for weight\n",
    "   estimation uncertainty by re-estimating weights for each placebo.\n",
    "\n",
    "3. **Fisher tradition**: Placebo tests have long history in SC literature\n",
    "   (Abadie et al. 2010). SDID extends this tradition.\n",
    "\n",
    "4. **When bootstrap works**:\n",
    "   - Multiple treated units\n",
    "   - Block bootstrap over time\n",
    "   - Wild bootstrap for dependent errors\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: How does SDID handle staggered adoption compared to TWFE?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **TWFE problem**: Standard TWFE uses already-treated units as implicit\n",
    "   controls for later cohorts, causing bias with heterogeneous effects.\n",
    "\n",
    "2. **SDID approach**: Estimate separately for each cohort, using only\n",
    "   not-yet-treated units as controls.\n",
    "\n",
    "3. **Aggregation**: Combine cohort-specific estimates with appropriate\n",
    "   weights (sample size, inverse variance, or equal).\n",
    "\n",
    "4. **Advantages**:\n",
    "   - No forbidden comparisons (treated vs treated)\n",
    "   - Allows heterogeneous effects by cohort\n",
    "   - Transparent about what's being estimated\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: What assumptions are needed for SDID inference to be valid?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Parallel trends (conditional)**: After reweighting, treated and synthetic\n",
    "   control would have parallel trends absent treatment.\n",
    "\n",
    "2. **Exchangeability**: Placebo units are exchangeable with treated unit\n",
    "   under the null. This means:\n",
    "   - Similar data generating process\n",
    "   - No spillovers from actual treated unit\n",
    "\n",
    "3. **No anticipation**: Units don't change behavior before treatment.\n",
    "\n",
    "4. **SUTVA**: No interference between units.\n",
    "\n",
    "5. **For normal CI**: Large enough $N_{co}$ for CLT to apply.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Arkhangelsky, D., Athey, S., Hirshberg, D., Imbens, G., & Wager, S. (2021). \n",
    "      Synthetic Difference-in-Differences. *American Economic Review*, 111(12), 4088-4118.\n",
    "\n",
    "[^2]: Abadie, A., Diamond, A., & Hainmueller, J. (2010). \n",
    "      Synthetic Control Methods for Comparative Case Studies. \n",
    "      *Journal of the American Statistical Association*, 105(490), 493-505.\n",
    "\n",
    "[^3]: Callaway, B., & Sant'Anna, P. H. (2021). \n",
    "      Difference-in-Differences with Multiple Time Periods. \n",
    "      *Journal of Econometrics*, 225(2), 200-230.\n",
    "\n",
    "[^4]: Facure, M. (2022). *Causal Inference for the Brave and True*, Chapter 25."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
