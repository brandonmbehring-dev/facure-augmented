{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03.3 Hypothesis Testing: The Logic of Statistical Significance\n",
    "\n",
    "**Chapter**: 3 - Stats Review  \n",
    "**Section**: 3 - Hypothesis Testing  \n",
    "**Facure Source**: 03-Stats-Review-The-Most-Dangerous-Equation.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [The Null Hypothesis](#11-the-null-hypothesis)\n",
    "   - 1.2 [Proof by Contradiction](#12-proof-by-contradiction)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [Z-Statistics](#21-z-statistics)\n",
    "   - 2.2 [P-Values](#22-p-values)\n",
    "   - 2.3 [Type I and Type II Errors](#23-type-i-and-type-ii-errors)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [Testing Treatment Effects](#31-testing-treatment-effects)\n",
    "   - 3.2 [A/B Testing Framework](#32-ab-testing-framework)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: Hypothesis testing logic is fundamental to A/B testing roles. Understanding p-values correctly is essential for data science interviews at tech companies.\n",
    "\n",
    "### 1.1 The Null Hypothesis\n",
    "\n",
    "Hypothesis testing follows **proof by contradiction**:\n",
    "\n",
    "1. **Assume** the treatment has no effect (null hypothesis, H₀)\n",
    "2. **Compute** how likely our data is under this assumption\n",
    "3. **If very unlikely**, reject the assumption → conclude there IS an effect\n",
    "\n",
    "### 1.2 Proof by Contradiction\n",
    "\n",
    "Facure's analogy: Like proving an alibi in court.\n",
    "\n",
    "- **H₀** (Null): \"The defendant was at the crime scene\" (no alibi)\n",
    "- **Evidence**: Video showing defendant elsewhere at the time\n",
    "- **Conclusion**: If H₀ were true, this evidence would be extremely unlikely → Reject H₀\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- We never \"accept\" the null—we fail to reject it\n",
    "- Statistical significance ≠ practical significance\n",
    "- p < 0.05 is convention, not law\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 Z-Statistics\n",
    "\n",
    "For testing the difference between two means:\n",
    "\n",
    "$$\\boxed{z = \\frac{(\\bar{X}_1 - \\bar{X}_0) - \\Delta_0}{\\sqrt{SE_1^2 + SE_0^2}}}$$\n",
    "\n",
    "where:\n",
    "- $\\bar{X}_1 - \\bar{X}_0$ = observed difference\n",
    "- $\\Delta_0$ = hypothesized difference (usually 0 under H₀)\n",
    "- $\\sqrt{SE_1^2 + SE_0^2}$ = standard error of the difference\n",
    "\n",
    "**Key property**: Under H₀, the z-statistic follows a standard normal distribution.\n",
    "\n",
    "### 2.2 P-Values\n",
    "\n",
    "$$p = P(|Z| > |z_{obs}| \\mid H_0)$$\n",
    "\n",
    "**Interpretation**: The probability of observing data **at least as extreme** as ours, **if the null hypothesis were true**.\n",
    "\n",
    "**Critical misconception**: p-value is NOT P(H₀ is true | data). It's P(data | H₀).\n",
    "\n",
    "**Decision rule**:\n",
    "- If p < α (usually 0.05): Reject H₀ (\"statistically significant\")\n",
    "- If p ≥ α: Fail to reject H₀\n",
    "\n",
    "### 2.3 Type I and Type II Errors\n",
    "\n",
    "| | H₀ True | H₀ False |\n",
    "|---|---------|----------|\n",
    "| **Reject H₀** | Type I Error (α) | Correct |\n",
    "| **Fail to Reject** | Correct | Type II Error (β) |\n",
    "\n",
    "- **Type I Error (α)**: False positive—claiming an effect when there isn't one\n",
    "- **Type II Error (β)**: False negative—missing a real effect\n",
    "- **Power = 1 - β**: Probability of detecting a true effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 Testing Treatment Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Facure's classroom data\n",
    "classroom = load_facure_data('online_classroom.csv')\n",
    "\n",
    "# Split by format\n",
    "online = classroom.query('format_ol == 1')['falsexam']\n",
    "face_to_face = classroom.query('format_ol == 0 and format_blended == 0')['falsexam']\n",
    "\n",
    "# Compute statistics\n",
    "def get_stats(data):\n",
    "    return {\n",
    "        'n': len(data),\n",
    "        'mean': np.mean(data),\n",
    "        'std': np.std(data, ddof=1),\n",
    "        'se': np.std(data, ddof=1) / np.sqrt(len(data))\n",
    "    }\n",
    "\n",
    "stats_online = get_stats(online)\n",
    "stats_f2f = get_stats(face_to_face)\n",
    "\n",
    "print(\"SAMPLE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Group':<15} {'n':>6} {'Mean':>10} {'Std':>10} {'SE':>10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Online':<15} {stats_online['n']:>6} {stats_online['mean']:>10.2f} {stats_online['std']:>10.2f} {stats_online['se']:>10.2f}\")\n",
    "print(f\"{'Face-to-Face':<15} {stats_f2f['n']:>6} {stats_f2f['mean']:>10.2f} {stats_f2f['std']:>10.2f} {stats_f2f['se']:>10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hypothesis test\n",
    "def hypothesis_test(test_data, control_data, h0=0):\n",
    "    \"\"\"Two-sample z-test for difference in means.\"\"\"\n",
    "    # Compute sample statistics\n",
    "    n1, n0 = len(test_data), len(control_data)\n",
    "    mu1, mu0 = np.mean(test_data), np.mean(control_data)\n",
    "    se1 = np.std(test_data, ddof=1) / np.sqrt(n1)\n",
    "    se0 = np.std(control_data, ddof=1) / np.sqrt(n0)\n",
    "    \n",
    "    # Difference and its SE\n",
    "    diff = mu1 - mu0\n",
    "    se_diff = np.sqrt(se1**2 + se0**2)\n",
    "    \n",
    "    # Z-statistic\n",
    "    z = (diff - h0) / se_diff\n",
    "    \n",
    "    # Two-tailed p-value\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "    \n",
    "    return {\n",
    "        'diff': diff,\n",
    "        'se_diff': se_diff,\n",
    "        'z': z,\n",
    "        'p_value': p_value,\n",
    "        'ci_lower': diff - 1.96 * se_diff,\n",
    "        'ci_upper': diff + 1.96 * se_diff,\n",
    "    }\n",
    "\n",
    "result = hypothesis_test(online, face_to_face)\n",
    "\n",
    "print(\"HYPOTHESIS TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"H₀: μ_online - μ_f2f = 0 (no difference)\")\n",
    "print(f\"H₁: μ_online - μ_f2f ≠ 0 (there is a difference)\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Observed difference: {result['diff']:.2f}\")\n",
    "print(f\"SE of difference:    {result['se_diff']:.2f}\")\n",
    "print(f\"Z-statistic:         {result['z']:.3f}\")\n",
    "print(f\"P-value:             {result['p_value']:.4f}\")\n",
    "print(f\"95% CI:              [{result['ci_lower']:.2f}, {result['ci_upper']:.2f}]\")\n",
    "print(\"-\"*60)\n",
    "if result['p_value'] < 0.05:\n",
    "    print(\"Conclusion: REJECT H₀ at α = 0.05\")\n",
    "    print(\"Online students performed significantly WORSE.\")\n",
    "else:\n",
    "    print(\"Conclusion: FAIL TO REJECT H₀ at α = 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the test\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Plot the null distribution\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = stats.norm.pdf(x)\n",
    "ax.plot(x, y, color=TUFTE_PALETTE['secondary'], linewidth=2, label='H₀ distribution')\n",
    "\n",
    "# Shade rejection regions\n",
    "ax.fill_between(x[x <= -1.96], y[x <= -1.96], alpha=0.3, \n",
    "                color=TUFTE_PALETTE['control'], label='Rejection region (α=0.05)')\n",
    "ax.fill_between(x[x >= 1.96], y[x >= 1.96], alpha=0.3, \n",
    "                color=TUFTE_PALETTE['control'])\n",
    "\n",
    "# Mark the observed z-statistic\n",
    "ax.axvline(result['z'], color=TUFTE_PALETTE['treatment'], linewidth=2, \n",
    "           linestyle='--', label=f'Observed z = {result[\"z\"]:.2f}')\n",
    "\n",
    "ax.set_xlabel('Z-statistic')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Hypothesis Test: Is Online Learning Different?\\n(Two-tailed test, α = 0.05)')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThe observed z = {result['z']:.2f} falls in the rejection region.\")\n",
    "print(f\"P-value = {result['p_value']:.4f} < 0.05, so we reject H₀.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 A/B Testing Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AB_test(test, control, confidence=0.95, h0=0):\n",
    "    \"\"\"\n",
    "    Complete A/B test with all relevant statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test : pd.Series or np.array\n",
    "        Test group outcomes\n",
    "    control : pd.Series or np.array\n",
    "        Control group outcomes\n",
    "    confidence : float\n",
    "        Confidence level (default 0.95)\n",
    "    h0 : float\n",
    "        Null hypothesis value for difference (default 0)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : Complete test results\n",
    "    \"\"\"\n",
    "    # Sample statistics\n",
    "    n_test, n_control = len(test), len(control)\n",
    "    mu_test, mu_control = np.mean(test), np.mean(control)\n",
    "    se_test = np.std(test, ddof=1) / np.sqrt(n_test)\n",
    "    se_control = np.std(control, ddof=1) / np.sqrt(n_control)\n",
    "    \n",
    "    # Difference statistics\n",
    "    diff = mu_test - mu_control\n",
    "    se_diff = np.sqrt(se_test**2 + se_control**2)\n",
    "    \n",
    "    # Critical value\n",
    "    z_crit = stats.norm.ppf(1 - (1 - confidence) / 2)\n",
    "    \n",
    "    # Test statistic and p-value\n",
    "    z = (diff - h0) / se_diff\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((\n",
    "        (n_test - 1) * np.var(test, ddof=1) + \n",
    "        (n_control - 1) * np.var(control, ddof=1)\n",
    "    ) / (n_test + n_control - 2))\n",
    "    cohens_d = diff / pooled_std\n",
    "    \n",
    "    return {\n",
    "        # Sample info\n",
    "        'n_test': n_test,\n",
    "        'n_control': n_control,\n",
    "        'mu_test': mu_test,\n",
    "        'mu_control': mu_control,\n",
    "        'se_test': se_test,\n",
    "        'se_control': se_control,\n",
    "        # Test results\n",
    "        'difference': diff,\n",
    "        'se_diff': se_diff,\n",
    "        'z_stat': z,\n",
    "        'p_value': p_value,\n",
    "        'ci_lower': diff - z_crit * se_diff,\n",
    "        'ci_upper': diff + z_crit * se_diff,\n",
    "        # Effect size\n",
    "        'cohens_d': cohens_d,\n",
    "        # Decision\n",
    "        'significant': p_value < (1 - confidence),\n",
    "    }\n",
    "\n",
    "# Run the A/B test\n",
    "ab_result = AB_test(online, face_to_face)\n",
    "\n",
    "print(\"A/B TEST REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Group (Online):    n={ab_result['n_test']}, μ={ab_result['mu_test']:.2f}, SE={ab_result['se_test']:.2f}\")\n",
    "print(f\"Control Group (F2F):    n={ab_result['n_control']}, μ={ab_result['mu_control']:.2f}, SE={ab_result['se_control']:.2f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Difference:             {ab_result['difference']:.2f}\")\n",
    "print(f\"95% CI:                 [{ab_result['ci_lower']:.2f}, {ab_result['ci_upper']:.2f}]\")\n",
    "print(f\"Z-statistic:            {ab_result['z_stat']:.3f}\")\n",
    "print(f\"P-value:                {ab_result['p_value']:.4f}\")\n",
    "print(f\"Cohen's d:              {ab_result['cohens_d']:.3f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Significant at α=0.05:  {'YES' if ab_result['significant'] else 'NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Method 1: Manual z-test (as shown above)\n",
    "z = (diff - h0) / se_diff\n",
    "p_value = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "\n",
    "# Method 2: Using scipy's ttest_ind\n",
    "t_stat, p_value = stats.ttest_ind(test, control)\n",
    "\n",
    "# Method 3: Using statsmodels\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "t_stat, p_value, df = ttest_ind(test, control)\n",
    "\n",
    "# In practice: Use causal_inference_mastery\n",
    "from causal_inference.rct import simple_ate\n",
    "result = simple_ate(outcome, treatment)\n",
    "print(f\"ATE = {result.ate:.3f}, p = {result.p_value:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "**Q1 (Google, Uber)**: *\"What is a p-value?\"*\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "A p-value is the probability of observing data at least as extreme as what we observed, **assuming the null hypothesis is true**.\n",
    "\n",
    "Mathematically: p = P(|T| ≥ |t_obs| | H₀)\n",
    "\n",
    "**Critical misconceptions to avoid**:\n",
    "- NOT P(H₀ is true | data) — that would require Bayes' theorem\n",
    "- NOT the probability of making an error\n",
    "- NOT the probability the result is due to chance\n",
    "\n",
    "A small p-value means: \"If there were no effect, data this extreme would be rare.\"\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q2 (Meta, Netflix)**: *\"What's the difference between Type I and Type II errors? Which is worse?\"*\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "**Type I Error (α)**: False positive—rejecting H₀ when it's true\n",
    "- Example: Claiming a drug works when it doesn't\n",
    "\n",
    "**Type II Error (β)**: False negative—failing to reject H₀ when it's false\n",
    "- Example: Missing a drug that actually works\n",
    "\n",
    "**Which is worse?** Depends on context:\n",
    "- Medical trial: Type I is worse (harmful treatment deployed)\n",
    "- A/B testing a button color: Type II is worse (missed opportunity)\n",
    "- Criminal trial: Type I is worse (convicting innocent)\n",
    "\n",
    "There's a trade-off: Reducing α increases β (and vice versa). Power analysis helps balance.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q3 (Stripe, Airbnb)**: *\"Your A/B test shows p = 0.048. The business wants to ship the feature. What do you say?\"*\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "Considerations beyond p < 0.05:\n",
    "\n",
    "1. **Effect size**: Is the improvement meaningful? A 0.1% conversion increase might not justify engineering effort.\n",
    "\n",
    "2. **Multiple comparisons**: If you tested 20 metrics, expect 1 false positive at α=0.05. Was this the primary metric?\n",
    "\n",
    "3. **Test duration**: Did you stop early? Peeking inflates false positives.\n",
    "\n",
    "4. **Sample ratio mismatch**: Were groups actually balanced?\n",
    "\n",
    "5. **Business context**: Is 0.048 robust enough for the decision's cost?\n",
    "\n",
    "My recommendation: \"p = 0.048 suggests the effect is likely real, but I'd want to see the effect size, check for multiple testing issues, and possibly run a confirmation test before shipping.\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 3.\n",
    "\n",
    "[^2]: Fisher, R. A. (1925). *Statistical Methods for Research Workers*. Oliver and Boyd.\n",
    "\n",
    "[^3]: Neyman, J., & Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. *Phil. Trans. R. Soc. A*, 231, 289-337.\n",
    "\n",
    "[^4]: Wasserstein, R. L., & Lazar, N. A. (2016). The ASA's statement on p-values: context, process, and purpose. *The American Statistician*, 70(2), 129-133."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
