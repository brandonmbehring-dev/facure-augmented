{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03.4 Practical Significance: When to Trust Your Results\n",
    "\n",
    "**Chapter**: 3 - Stats Review  \n",
    "**Section**: 4 - Practical Significance  \n",
    "**Facure Source**: 03-Stats-Review-The-Most-Dangerous-Equation.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [Statistical vs Practical](#11-statistical-vs-practical)\n",
    "   - 1.2 [The Multiple Testing Problem](#12-the-multiple-testing-problem)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [Effect Sizes](#21-effect-sizes)\n",
    "   - 2.2 [Multiple Comparison Corrections](#22-multiple-comparison-corrections)\n",
    "   - 2.3 [Power Analysis](#23-power-analysis)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [Effect Size Calculation](#31-effect-size-calculation)\n",
    "   - 3.2 [Multiple Testing Simulation](#32-multiple-testing-simulation)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: Understanding when NOT to act on statistical significance is crucial. This separates data scientists who ship real value from those who chase noise.\n",
    "\n",
    "### 1.1 Statistical vs Practical\n",
    "\n",
    "Facure's key insight: **Statistical significance tells you IF an effect exists. Effect size tells you if you should CARE.**\n",
    "\n",
    "Example:\n",
    "- A/B test with 10 million users\n",
    "- Conversion: 2.001% vs 2.000%\n",
    "- p-value: 0.001 (highly significant!)\n",
    "- Effect: +0.001 percentage points\n",
    "\n",
    "**Statistically significant? Yes. Worth shipping? Probably not.**\n",
    "\n",
    "### 1.2 The Multiple Testing Problem\n",
    "\n",
    "If you test 20 metrics at α = 0.05, you **expect** 1 false positive.\n",
    "\n",
    "$$P(\\text{at least one false positive}) = 1 - (1-0.05)^{20} = 0.64$$\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- Effect size (Cohen's d, lift %) matters as much as p-value\n",
    "- Multiple testing inflates false positives\n",
    "- Pre-registration of primary metrics prevents p-hacking\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 Effect Sizes\n",
    "\n",
    "**Cohen's d** (for continuous outcomes):\n",
    "\n",
    "$$\\boxed{d = \\frac{\\bar{X}_1 - \\bar{X}_0}{s_{pooled}}}$$\n",
    "\n",
    "where $s_{pooled} = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_0-1)s_0^2}{n_1 + n_0 - 2}}$\n",
    "\n",
    "**Interpretation**:\n",
    "| d | Interpretation |\n",
    "|---|----------------|\n",
    "| 0.2 | Small |\n",
    "| 0.5 | Medium |\n",
    "| 0.8 | Large |\n",
    "\n",
    "**Relative lift** (for conversion rates):\n",
    "\n",
    "$$\\text{Lift} = \\frac{p_1 - p_0}{p_0} \\times 100\\%$$\n",
    "\n",
    "### 2.2 Multiple Comparison Corrections\n",
    "\n",
    "**Bonferroni correction**:\n",
    "$$\\alpha_{adj} = \\frac{\\alpha}{m}$$\n",
    "\n",
    "where $m$ = number of tests. Conservative but simple.\n",
    "\n",
    "**Benjamini-Hochberg (FDR)**:\n",
    "1. Rank p-values: $p_{(1)} \\leq p_{(2)} \\leq \\cdots \\leq p_{(m)}$\n",
    "2. Find largest $k$ where $p_{(k)} \\leq \\frac{k}{m}\\alpha$\n",
    "3. Reject all $H_{(1)}, \\ldots, H_{(k)}$\n",
    "\n",
    "Controls FDR (expected proportion of false discoveries among rejections).\n",
    "\n",
    "### 2.3 Power Analysis\n",
    "\n",
    "**Power** = P(reject H₀ | H₀ is false) = 1 - β\n",
    "\n",
    "**Required sample size** (two-sample, equal groups):\n",
    "\n",
    "$$n = 2\\left(\\frac{z_{\\alpha/2} + z_\\beta}{d}\\right)^2$$\n",
    "\n",
    "For 80% power at α = 0.05 detecting d = 0.5:\n",
    "$$n = 2\\left(\\frac{1.96 + 0.84}{0.5}\\right)^2 \\approx 64 \\text{ per group}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 Effect Size Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classroom data\n",
    "classroom = load_facure_data('online_classroom.csv')\n",
    "\n",
    "online = classroom.query('format_ol == 1')['falsexam']\n",
    "face_to_face = classroom.query('format_ol == 0 and format_blended == 0')['falsexam']\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size.\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1 + n2 - 2))\n",
    "    \n",
    "    d = (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "    return d\n",
    "\n",
    "d = cohens_d(online, face_to_face)\n",
    "\n",
    "print(\"EFFECT SIZE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mean (Online):        {np.mean(online):.2f}\")\n",
    "print(f\"Mean (Face-to-Face):  {np.mean(face_to_face):.2f}\")\n",
    "print(f\"Raw difference:       {np.mean(online) - np.mean(face_to_face):.2f}\")\n",
    "print(f\"Cohen's d:            {d:.3f}\")\n",
    "print(\"-\"*50)\n",
    "if abs(d) < 0.2:\n",
    "    print(\"Interpretation: Negligible effect\")\n",
    "elif abs(d) < 0.5:\n",
    "    print(\"Interpretation: Small effect\")\n",
    "elif abs(d) < 0.8:\n",
    "    print(\"Interpretation: Medium effect\")\n",
    "else:\n",
    "    print(\"Interpretation: Large effect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the \"big data\" problem\n",
    "np.random.seed(42)\n",
    "\n",
    "# Tiny true effect\n",
    "true_effect = 0.01  # 0.01 points difference\n",
    "\n",
    "results = []\n",
    "sample_sizes = [100, 1000, 10000, 100000, 1000000]\n",
    "\n",
    "for n in sample_sizes:\n",
    "    control = np.random.normal(50, 10, n)\n",
    "    treatment = np.random.normal(50 + true_effect, 10, n)\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(treatment, control)\n",
    "    d = cohens_d(treatment, control)\n",
    "    \n",
    "    results.append({\n",
    "        'n': n,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05,\n",
    "        'cohens_d': d,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"THE BIG DATA PROBLEM\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True effect: {true_effect} (negligible)\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'N':>10} {'P-value':>12} {'Significant':>12} {'Cohen\\'s d':>12}\")\n",
    "print(\"-\"*60)\n",
    "for _, row in results_df.iterrows():\n",
    "    sig_str = 'YES' if row['significant'] else 'no'\n",
    "    print(f\"{row['n']:>10,} {row['p_value']:>12.6f} {sig_str:>12} {row['cohens_d']:>12.4f}\")\n",
    "print(\"-\"*60)\n",
    "print(\"\\nNote: With enough data, even meaningless effects become 'significant'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multiple Testing Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the multiple testing problem\n",
    "np.random.seed(42)\n",
    "\n",
    "n_metrics = 20\n",
    "n_simulations = 1000\n",
    "n_samples = 100\n",
    "\n",
    "# No true effect for any metric\n",
    "false_positives = []\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    p_values = []\n",
    "    for _ in range(n_metrics):\n",
    "        control = np.random.normal(0, 1, n_samples)\n",
    "        treatment = np.random.normal(0, 1, n_samples)  # Same distribution!\n",
    "        _, p = stats.ttest_ind(treatment, control)\n",
    "        p_values.append(p)\n",
    "    \n",
    "    # Count false positives (any p < 0.05 when ALL nulls are true)\n",
    "    n_false_positives = sum(p < 0.05 for p in p_values)\n",
    "    false_positives.append(n_false_positives)\n",
    "\n",
    "# Probability of at least one false positive\n",
    "p_at_least_one = np.mean([fp > 0 for fp in false_positives])\n",
    "\n",
    "print(\"MULTIPLE TESTING SIMULATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of metrics tested: {n_metrics}\")\n",
    "print(f\"True effects:             NONE (all nulls are true)\")\n",
    "print(f\"Number of simulations:    {n_simulations}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"P(at least 1 false positive): {p_at_least_one:.1%}\")\n",
    "print(f\"Expected from theory:         {1 - (0.95)**n_metrics:.1%}\")\n",
    "print(f\"Average false positives:      {np.mean(false_positives):.2f}\")\n",
    "print(f\"Expected from theory:         {n_metrics * 0.05:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate corrections\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate p-values (some real effects, some null)\n",
    "n_tests = 20\n",
    "n_true_effects = 5  # First 5 have real effects\n",
    "\n",
    "p_values = []\n",
    "for i in range(n_tests):\n",
    "    if i < n_true_effects:\n",
    "        # Real effect (d ≈ 0.5)\n",
    "        control = np.random.normal(0, 1, 100)\n",
    "        treatment = np.random.normal(0.5, 1, 100)\n",
    "    else:\n",
    "        # No effect\n",
    "        control = np.random.normal(0, 1, 100)\n",
    "        treatment = np.random.normal(0, 1, 100)\n",
    "    \n",
    "    _, p = stats.ttest_ind(treatment, control)\n",
    "    p_values.append(p)\n",
    "\n",
    "p_values = np.array(p_values)\n",
    "\n",
    "# Apply corrections\n",
    "alpha = 0.05\n",
    "\n",
    "# Uncorrected\n",
    "sig_uncorrected = p_values < alpha\n",
    "\n",
    "# Bonferroni\n",
    "bonferroni_alpha = alpha / n_tests\n",
    "sig_bonferroni = p_values < bonferroni_alpha\n",
    "\n",
    "# Benjamini-Hochberg (FDR)\n",
    "from scipy.stats import false_discovery_control\n",
    "# Manual BH implementation\n",
    "sorted_idx = np.argsort(p_values)\n",
    "sorted_p = p_values[sorted_idx]\n",
    "bh_threshold = np.arange(1, n_tests+1) / n_tests * alpha\n",
    "bh_accept = sorted_p <= bh_threshold\n",
    "# Find the largest k where p_(k) <= k/m * alpha\n",
    "if bh_accept.any():\n",
    "    k = np.max(np.where(bh_accept)[0]) + 1\n",
    "    sig_bh = np.zeros(n_tests, dtype=bool)\n",
    "    sig_bh[sorted_idx[:k]] = True\n",
    "else:\n",
    "    sig_bh = np.zeros(n_tests, dtype=bool)\n",
    "\n",
    "print(\"MULTIPLE COMPARISON CORRECTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tests:          {n_tests}\")\n",
    "print(f\"True effects:         {n_true_effects} (tests 1-5)\")\n",
    "print(f\"Null effects:         {n_tests - n_true_effects} (tests 6-20)\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Method':<20} {'Rejections':>12} {'True Pos':>10} {'False Pos':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for name, sig in [('Uncorrected', sig_uncorrected), \n",
    "                   ('Bonferroni', sig_bonferroni),\n",
    "                   ('Benjamini-Hochberg', sig_bh)]:\n",
    "    true_pos = sig[:n_true_effects].sum()\n",
    "    false_pos = sig[n_true_effects:].sum()\n",
    "    print(f\"{name:<20} {sig.sum():>12} {true_pos:>10} {false_pos:>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize p-value distribution\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Sort by p-value for visualization\n",
    "sorted_idx = np.argsort(p_values)\n",
    "sorted_p = p_values[sorted_idx]\n",
    "\n",
    "colors = [TUFTE_PALETTE['treatment'] if i < n_true_effects else TUFTE_PALETTE['secondary'] \n",
    "          for i in sorted_idx]\n",
    "\n",
    "bars = ax.bar(range(n_tests), sorted_p, color=colors, alpha=0.7)\n",
    "\n",
    "# Thresholds\n",
    "ax.axhline(0.05, color=TUFTE_PALETTE['control'], linestyle='--', \n",
    "           label=f'α = 0.05 (uncorrected)')\n",
    "ax.axhline(bonferroni_alpha, color='black', linestyle=':', \n",
    "           label=f'Bonferroni = {bonferroni_alpha:.4f}')\n",
    "\n",
    "# BH line\n",
    "bh_line = np.arange(1, n_tests+1) / n_tests * alpha\n",
    "ax.plot(range(n_tests), bh_line, color=TUFTE_PALETTE['effect'], \n",
    "        linestyle='-', label='BH threshold')\n",
    "\n",
    "ax.set_xlabel('Test (sorted by p-value)')\n",
    "ax.set_ylabel('P-value')\n",
    "ax.set_title('Multiple Testing: P-values vs Thresholds\\n(Blue = true effect, Gray = null)')\n",
    "ax.legend(frameon=False, loc='upper left')\n",
    "ax.set_ylim(0, 0.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Effect size\n",
    "def cohens_d(group1, group2):\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "    return (group1.mean() - group2.mean()) / pooled_std\n",
    "\n",
    "# Multiple testing corrections\n",
    "reject, p_adj, _, _ = multipletests(p_values, method='bonferroni')\n",
    "reject, p_adj, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "\n",
    "# Power analysis\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "power = TTestIndPower()\n",
    "n = power.solve_power(effect_size=0.5, alpha=0.05, power=0.8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "**Q1 (Netflix, Spotify)**: *\"What's the difference between statistical and practical significance?\"*\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "**Statistical significance**: p < α (usually 0.05). Answers: \"Is the effect real?\"\n",
    "\n",
    "**Practical significance**: Effect size is meaningful. Answers: \"Should we care?\"\n",
    "\n",
    "Key insight: With large n, even tiny effects become statistically significant. A 0.001% conversion lift with p < 0.001 might not justify the engineering cost to ship.\n",
    "\n",
    "**In practice**: Report both p-value AND effect size (Cohen's d, lift %). Let stakeholders judge practical significance based on business context.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q2 (Uber, Lyft)**: *\"You ran 20 A/B tests and 3 were significant at p < 0.05. How do you interpret this?\"*\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "**Multiple testing problem**: At α = 0.05, we expect 1 false positive per 20 tests (20 × 0.05 = 1).\n",
    "\n",
    "**Finding 3 significant results** could mean:\n",
    "- 2-3 real effects + 0-1 false positives, OR\n",
    "- 1 real effect + 2 false positives\n",
    "\n",
    "**What to do**:\n",
    "1. Apply correction (Bonferroni: α_adj = 0.05/20 = 0.0025)\n",
    "2. Check which of the 3 survive correction\n",
    "3. For borderline cases, consider effect sizes and pre-registration status\n",
    "4. Consider replication for important findings\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q3 (Stripe, Airbnb)**: *\"How would you design an experiment to have 80% power to detect a 5% lift in conversion?\"*\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "\n",
    "**Power analysis framework**:\n",
    "\n",
    "1. **Define parameters**:\n",
    "   - Baseline conversion: p₀ (e.g., 10%)\n",
    "   - Minimum detectable effect: 5% lift → p₁ = 10.5%\n",
    "   - α = 0.05, power = 0.80\n",
    "\n",
    "2. **Calculate effect size**:\n",
    "   - Cohen's h = 2*arcsin(√p₁) - 2*arcsin(√p₀) ≈ 0.016\n",
    "   - This is a very small effect!\n",
    "\n",
    "3. **Required sample size**:\n",
    "   - n ≈ 2 × ((1.96 + 0.84) / 0.016)² ≈ 61,000 per group\n",
    "   - Total: ~122,000 users\n",
    "\n",
    "4. **Practical considerations**:\n",
    "   - How long to accumulate 122k users?\n",
    "   - Is 5% lift worth the wait?\n",
    "   - Consider sequential testing to stop early\n",
    "\n",
    "**Code**: `from statsmodels.stats.power import NormalIndPower`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 3.\n",
    "\n",
    "[^2]: Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences*. Lawrence Erlbaum.\n",
    "\n",
    "[^3]: Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. *JRSS B*, 57(1), 289-300.\n",
    "\n",
    "[^4]: Kohavi, R., Tang, D., & Xu, Y. (2020). *Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing*. Cambridge University Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
