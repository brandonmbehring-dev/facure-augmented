{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 18.2 Estimating Heterogeneity\n",
    "\n",
    "**Chapter**: 18 - Heterogeneous Treatment Effects  \n",
    "**Section**: 2 - Estimating Heterogeneity  \n",
    "**Facure Source**: 18-Heterogeneous-Treatment-Effects-and-Personalization.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-16\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [Predicting the Unobservable](#11-predicting-the-unobservable)\n",
    "   - 1.2 [Interaction Terms](#12-interaction-terms)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [The Derivative Approach](#21-the-derivative-approach)\n",
    "   - 2.2 [Model Specification](#22-model-specification)\n",
    "3. [Implementation](#3-implementation)\n",
    "   - 3.1 [Building CATE Models](#31-building-cate-models)\n",
    "   - 3.2 [Sensitivity Predictions](#32-sensitivity-predictions)\n",
    "4. [Numeric Demonstration](#4-numeric-demonstration)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(123)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: Understanding how to predict individual treatment sensitivity — even though it's unobservable — is key to many industry applications of causal inference.\n",
    "\n",
    "### 1.1 Predicting the Unobservable\n",
    "\n",
    "We want to predict $\\frac{\\partial Y_i}{\\partial T_i}$ — individual sensitivity to treatment. But this is **unobservable** (we can't see the same unit under different treatments).\n",
    "\n",
    "**Key insight**: We don't need to OBSERVE sensitivity to PREDICT it!\n",
    "\n",
    "If we have a model that predicts $\\hat{Y}$ from $X$ and $T$, we can compute:\n",
    "\n",
    "$$\\widehat{\\frac{\\partial Y}{\\partial T}} = \\frac{\\hat{Y}(T + \\epsilon) - \\hat{Y}(T)}{\\epsilon}$$\n",
    "\n",
    "This is a numerical approximation of the derivative — our predicted sensitivity.\n",
    "\n",
    "### 1.2 Interaction Terms\n",
    "\n",
    "Consider a linear model:\n",
    "\n",
    "$$Y_i = \\beta_0 + \\beta_1 T_i + \\beta_2 X_i + \\epsilon_i$$\n",
    "\n",
    "Taking the derivative:\n",
    "\n",
    "$$\\frac{\\partial Y_i}{\\partial T_i} = \\beta_1$$\n",
    "\n",
    "Everyone has the same sensitivity! This is the **ATE** model.\n",
    "\n",
    "Now add an **interaction**:\n",
    "\n",
    "$$Y_i = \\beta_0 + \\beta_1 T_i + \\beta_2 X_i + \\beta_3 (T_i \\cdot X_i) + \\epsilon_i$$\n",
    "\n",
    "Taking the derivative:\n",
    "\n",
    "$$\\frac{\\partial Y_i}{\\partial T_i} = \\beta_1 + \\beta_3 X_i$$\n",
    "\n",
    "Now sensitivity varies with $X$! This is a **CATE** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ice cream sales data\n",
    "prices_rnd = load_facure_data('ice_cream_sales_rnd.csv')\n",
    "\n",
    "# Split into train/test for model comparison\n",
    "train, test = train_test_split(prices_rnd, test_size=0.25, random_state=123)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining set: {len(train):,} observations\")\n",
    "print(f\"Test set: {len(test):,} observations\")\n",
    "print(f\"\\nOutcome: sales\")\n",
    "print(f\"Treatment: price\")\n",
    "print(f\"Features: temp, weekday, cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 The Derivative Approach\n",
    "\n",
    "For any outcome model $\\mu(X, T)$, we can define **sensitivity** as:\n",
    "\n",
    "$$\\tau(x) = \\frac{\\partial \\mu(x, t)}{\\partial t}$$\n",
    "\n",
    "**For linear models**, this derivative has a closed form:\n",
    "\n",
    "| Model | Sensitivity |\n",
    "|-------|-------------|\n",
    "| $Y = \\beta_0 + \\beta_1 T + \\beta_2 X$ | $\\beta_1$ (constant) |\n",
    "| $Y = \\beta_0 + \\beta_1 T + \\beta_3 TX$ | $\\beta_1 + \\beta_3 X$ (linear in X) |\n",
    "| $Y = f(T, X)$ (nonlinear) | Numerical approximation |\n",
    "\n",
    "**Numerical approximation** (general):\n",
    "\n",
    "$$\\hat{\\tau}(x) = \\frac{\\hat{\\mu}(x, t + h) - \\hat{\\mu}(x, t)}{h}$$\n",
    "\n",
    "With $h$ small (e.g., 0.01 or 1).\n",
    "\n",
    "### 2.2 Model Specification\n",
    "\n",
    "We'll build three models of increasing complexity:\n",
    "\n",
    "**Model 1 (ATE)**: No interactions\n",
    "$$\\text{sales} = \\beta_0 + \\beta_1 \\text{price} + \\beta_2 \\text{temp} + \\beta_3 \\text{weekday} + \\beta_4 \\text{cost} + \\epsilon$$\n",
    "\n",
    "**Model 2 (CATE with temp)**: One interaction\n",
    "$$\\text{sales} = \\beta_0 + \\beta_1 \\text{price} + \\beta_2 \\text{price} \\times \\text{temp} + \\text{controls} + \\epsilon$$\n",
    "\n",
    "**Model 3 (Full CATE)**: All interactions\n",
    "$$\\text{sales} = \\beta_0 + \\beta_1 \\text{price} + \\beta_2 \\text{price} \\times \\text{temp} + \\beta_3 \\text{price} \\times \\text{cost} + \\beta_4 \\text{price} \\times \\text{weekday} + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementation\n",
    "\n",
    "### 3.1 Building CATE Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: ATE model (constant effect)\n",
    "m1 = smf.ols(\"sales ~ price + temp + C(weekday) + cost\", data=train).fit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL 1: ATE (No Interactions)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nsales = β₀ + β₁·price + β₂·temp + weekday_dummies + β₄·cost\")\n",
    "print(f\"\\nPrice coefficient: {m1.params['price']:.4f}\")\n",
    "print(f\"\\nPredicted sensitivity: {m1.params['price']:.2f} for ALL days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: CATE with temperature interaction\n",
    "m2 = smf.ols(\"sales ~ price*temp + C(weekday) + cost\", data=train).fit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL 2: CATE (Price × Temperature)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPrice coefficient (base): {m2.params['price']:.4f}\")\n",
    "print(f\"Price × temp interaction: {m2.params['price:temp']:.4f}\")\n",
    "print(f\"\\nSensitivity = {m2.params['price']:.2f} + {m2.params['price:temp']:.4f} × temp\")\n",
    "print(f\"\\nExamples:\")\n",
    "print(f\"  At 20°C: {m2.params['price'] + m2.params['price:temp']*20:.2f}\")\n",
    "print(f\"  At 25°C: {m2.params['price'] + m2.params['price:temp']*25:.2f}\")\n",
    "print(f\"  At 30°C: {m2.params['price'] + m2.params['price:temp']*30:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Full CATE with all interactions\n",
    "m3 = smf.ols(\"sales ~ price*cost + price*C(weekday) + price*temp\", data=train).fit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL 3: Full CATE (All Interactions)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPrice coefficient (base): {m3.params['price']:.4f}\")\n",
    "print(f\"Price × temp: {m3.params['price:temp']:.4f}\")\n",
    "print(f\"Price × cost: {m3.params['price:cost']:.4f}\")\n",
    "print(f\"\\nSensitivity varies with temp, cost, AND weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 3.2 Sensitivity Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_sensitivity(model, df, t_col=\"price\", h=1):\n",
    "    \"\"\"\n",
    "    Predict sensitivity using numerical derivative approximation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : fitted statsmodels model\n",
    "    df : DataFrame with features\n",
    "    t_col : name of treatment column\n",
    "    h : step size for numerical derivative (default 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with pred_sens column added\n",
    "    \"\"\"\n",
    "    df_plus = df.copy()\n",
    "    df_plus[t_col] = df[t_col] + h\n",
    "    return df.assign(\n",
    "        pred_sens=(model.predict(df_plus) - model.predict(df)) / h\n",
    "    )\n",
    "\n",
    "# Make predictions on test set\n",
    "test_m1 = pred_sensitivity(m1, test)\n",
    "test_m2 = pred_sensitivity(m2, test)\n",
    "test_m3 = pred_sensitivity(m3, test)\n",
    "\n",
    "print(\"Sensitivity predictions on test set:\")\n",
    "print(f\"\\nModel 1 (ATE):       mean={test_m1['pred_sens'].mean():.3f}, std={test_m1['pred_sens'].std():.3f}\")\n",
    "print(f\"Model 2 (temp):      mean={test_m2['pred_sens'].mean():.3f}, std={test_m2['pred_sens'].std():.3f}\")\n",
    "print(f\"Model 3 (full):      mean={test_m3['pred_sens'].mean():.3f}, std={test_m3['pred_sens'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "sample = test_m3.sample(5, random_state=1)[['temp', 'weekday', 'cost', 'price', 'sales', 'pred_sens']]\n",
    "print(\"Sample predictions (Model 3):\")\n",
    "print(sample.round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\npred_sens = predicted change in sales per $1 price increase\")\n",
    "print(\"Negative values = sales decrease when price increases (expected)\")\n",
    "print(\"More negative = more price sensitive\")\n",
    "print(\"Less negative (or positive) = less price sensitive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Numeric Demonstration\n",
    "\n",
    "Let's visualize how the models differ in their heterogeneity predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sensitivity predictions across models\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Panel 1: Distribution comparison\n",
    "ax = axes[0]\n",
    "ax.hist(test_m1['pred_sens'], bins=20, alpha=0.7, label='M1 (ATE)', color=COLORS['blue'], edgecolor='white')\n",
    "ax.hist(test_m3['pred_sens'], bins=40, alpha=0.7, label='M3 (Full CATE)', color=COLORS['green'], edgecolor='white')\n",
    "ax.axvline(test_m1['pred_sens'].mean(), color=COLORS['blue'], ls='--', lw=2)\n",
    "ax.axvline(test_m3['pred_sens'].mean(), color=COLORS['green'], ls='--', lw=2)\n",
    "ax.set_xlabel('Predicted Sensitivity')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('(a) ATE vs Full CATE', fontweight='bold')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: M1 predictions (constant)\n",
    "ax = axes[1]\n",
    "ax.scatter(test_m1['temp'], test_m1['pred_sens'], alpha=0.5, s=15, c=COLORS['blue'])\n",
    "ax.set_xlabel('Temperature (°C)')\n",
    "ax.set_ylabel('Predicted Sensitivity')\n",
    "ax.set_title('(b) Model 1: Same for All', fontweight='bold')\n",
    "ax.set_ylim(-15, 5)\n",
    "\n",
    "# Panel 3: M3 predictions (heterogeneous)\n",
    "ax = axes[2]\n",
    "scatter = ax.scatter(test_m3['temp'], test_m3['pred_sens'], alpha=0.5, s=15, \n",
    "                     c=test_m3['cost'], cmap='RdYlGn')\n",
    "ax.set_xlabel('Temperature (°C)')\n",
    "ax.set_ylabel('Predicted Sensitivity')\n",
    "ax.set_title('(c) Model 3: Varies by Features', fontweight='bold')\n",
    "ax.set_ylim(-15, 5)\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Cost')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation:\")\n",
    "print(\"  - Model 1 gives the SAME prediction for everyone (vertical line)\")\n",
    "print(\"  - Model 3 predictions vary with temperature AND cost (colored spread)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model 2 and model 3\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Sensitivity vs Temperature (both models)\n",
    "ax = axes[0]\n",
    "ax.scatter(test_m2['temp'], test_m2['pred_sens'], alpha=0.4, s=15, \n",
    "           c=COLORS['blue'], label='M2 (temp only)')\n",
    "ax.scatter(test_m3['temp'], test_m3['pred_sens'], alpha=0.4, s=15, \n",
    "           c=COLORS['green'], label='M3 (all interactions)')\n",
    "\n",
    "# Add trend lines\n",
    "z2 = np.polyfit(test_m2['temp'], test_m2['pred_sens'], 1)\n",
    "z3 = np.polyfit(test_m3['temp'], test_m3['pred_sens'], 1)\n",
    "temp_range = np.linspace(test['temp'].min(), test['temp'].max(), 100)\n",
    "ax.plot(temp_range, np.poly1d(z2)(temp_range), c=COLORS['blue'], lw=2, ls='-')\n",
    "ax.plot(temp_range, np.poly1d(z3)(temp_range), c=COLORS['green'], lw=2, ls='-')\n",
    "\n",
    "ax.axhline(0, color='black', ls=':', lw=1)\n",
    "ax.set_xlabel('Temperature (°C)')\n",
    "ax.set_ylabel('Predicted Sensitivity')\n",
    "ax.set_title('(a) Temperature Relationship', fontweight='bold')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: Prediction correlation\n",
    "ax = axes[1]\n",
    "ax.scatter(test_m2['pred_sens'], test_m3['pred_sens'], alpha=0.4, s=15, \n",
    "           c=TUFTE_PALETTE['primary'])\n",
    "ax.plot([-15, 5], [-15, 5], 'k--', lw=1.5, label='45° line')\n",
    "ax.set_xlabel('M2 Predictions')\n",
    "ax.set_ylabel('M3 Predictions')\n",
    "ax.set_title('(b) Model Correlation', fontweight='bold')\n",
    "\n",
    "corr = np.corrcoef(test_m2['pred_sens'], test_m3['pred_sens'])[0, 1]\n",
    "ax.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax.transAxes, fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrelation between M2 and M3 predictions: {corr:.3f}\")\n",
    "print(\"Model 3 adds variation from cost and weekday interactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sensitivity bands for personalization\n",
    "test_m3['sens_band'] = pd.qcut(test_m3['pred_sens'], q=3, \n",
    "                               labels=['High Sensitivity', 'Medium', 'Low Sensitivity'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENSITIVITY BANDS (Model 3)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = test_m3.groupby('sens_band').agg({\n",
    "    'pred_sens': ['mean', 'min', 'max'],\n",
    "    'temp': 'mean',\n",
    "    'cost': 'mean',\n",
    "    'sales': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nHigh sensitivity band: cooler days, higher cost\")\n",
    "print(\"  -> These days: keep prices LOW\")\n",
    "print(\"\\nLow sensitivity band: hotter days, lower cost\")\n",
    "print(\"  -> These days: can RAISE prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bands with price-sales relationship\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Use seaborn for regression plots by band\n",
    "for i, band in enumerate(['High Sensitivity', 'Low Sensitivity']):\n",
    "    subset = test_m3[test_m3['sens_band'] == band]\n",
    "    color = COLORS['red'] if band == 'High Sensitivity' else COLORS['green']\n",
    "    \n",
    "    # Scatter\n",
    "    ax.scatter(subset['price'], subset['sales'], alpha=0.3, s=20, c=color)\n",
    "    \n",
    "    # Regression line\n",
    "    z = np.polyfit(subset['price'], subset['sales'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(subset['price'].min(), subset['price'].max(), 100)\n",
    "    ax.plot(x_range, p(x_range), c=color, lw=2.5, label=f'{band} (slope={z[0]:.1f})')\n",
    "\n",
    "ax.set_xlabel('Price', fontsize=11)\n",
    "ax.set_ylabel('Sales', fontsize=11)\n",
    "ax.set_title('Price-Sales Relationship by Sensitivity Band', fontweight='bold')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisual confirms: High sensitivity band has steeper negative slope!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "```\n",
    "Key Takeaways ───────────────────────────────────────────\n",
    "\n",
    "1. Sensitivity is predicted, not observed\n",
    "   - Use numerical derivative: [f(T+h) - f(T)] / h\n",
    "   - Works with any model that predicts E[Y|X,T]\n",
    "\n",
    "2. Interaction terms create heterogeneity\n",
    "   - No interactions: τ(x) = β₁ (constant)\n",
    "   - With interactions: τ(x) = β₁ + β₃·x (varies)\n",
    "\n",
    "3. Model comparison\n",
    "   - M1 (ATE): same prediction for everyone\n",
    "   - M2 (temp): varies with temperature\n",
    "   - M3 (full): varies with temp, cost, weekday\n",
    "   \n",
    "4. Practical use: Create sensitivity bands\n",
    "   - Segment by predicted CATE\n",
    "   - Personalize treatment (pricing) by segment\n",
    "─────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Meta E5, DS)**: *\"How do you predict treatment effect heterogeneity using linear regression?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key technique: Interaction terms**\n",
    "\n",
    "Start with base model (constant effect):\n",
    "$$Y = \\beta_0 + \\beta_1 T + \\beta_2 X + \\epsilon$$\n",
    "\n",
    "Add interaction:\n",
    "$$Y = \\beta_0 + \\beta_1 T + \\beta_2 X + \\beta_3 (T \\cdot X) + \\epsilon$$\n",
    "\n",
    "Now CATE varies with X:\n",
    "$$\\tau(x) = \\frac{\\partial Y}{\\partial T} = \\beta_1 + \\beta_3 x$$\n",
    "\n",
    "**For prediction**:\n",
    "\n",
    "1. Fit the interaction model on training data\n",
    "2. For new unit with features $x$:\n",
    "   - Predict $\\hat{Y}(x, T=1)$ and $\\hat{Y}(x, T=0)$\n",
    "   - CATE = difference\n",
    "   - Or use derivative formula directly: $\\hat{\\beta}_1 + \\hat{\\beta}_3 x$\n",
    "\n",
    "**Numerical approximation** (general):\n",
    "$$\\hat{\\tau}(x) = \\frac{\\hat{Y}(x, T+h) - \\hat{Y}(x, T)}{h}$$\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Amazon L6, Econ)**: *\"What are the limitations of using linear regression for CATE estimation?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Limitations**:\n",
    "\n",
    "1. **Linearity assumption**: Heterogeneity must be linear in X\n",
    "   - $\\tau(x) = \\beta_1 + \\beta_3 x$ assumes linear relationship\n",
    "   - May miss complex nonlinear heterogeneity\n",
    "\n",
    "2. **Feature engineering burden**: Must specify interactions manually\n",
    "   - With many features, combinatorial explosion\n",
    "   - May miss important interactions\n",
    "\n",
    "3. **Regularization issues**: In high-dimensional settings\n",
    "   - Regularization can shrink treatment effects toward zero\n",
    "   - Especially problematic if treatment is weak predictor\n",
    "\n",
    "4. **No uncertainty quantification** for CATE\n",
    "   - Standard errors are for coefficients, not for CATE(x)\n",
    "   - Need bootstrap or other methods\n",
    "\n",
    "**Alternatives**:\n",
    "- Meta-learners (S, T, X, R) can use flexible ML models\n",
    "- Causal forests capture nonlinear heterogeneity\n",
    "- Bayesian methods provide uncertainty quantification\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Google L5, Quant)**: *\"How do you interpret the coefficient on an interaction term T×X?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "In the model:\n",
    "$$Y = \\beta_0 + \\beta_1 T + \\beta_2 X + \\beta_3 (T \\cdot X) + \\epsilon$$\n",
    "\n",
    "**Interpretation of β₃**:\n",
    "\n",
    "1. **As treatment effect modifier**:\n",
    "   - For each unit increase in X, the treatment effect changes by β₃\n",
    "   - If β₃ > 0: higher X → larger treatment effect\n",
    "   - If β₃ < 0: higher X → smaller treatment effect\n",
    "\n",
    "2. **As conditional treatment effect**:\n",
    "   - When X = 0: treatment effect = β₁\n",
    "   - When X = 1: treatment effect = β₁ + β₃\n",
    "   - When X = x: treatment effect = β₁ + β₃·x\n",
    "\n",
    "3. **Marginal effect of X**:\n",
    "   - When T = 0: marginal effect of X on Y is β₂\n",
    "   - When T = 1: marginal effect of X on Y is β₂ + β₃\n",
    "\n",
    "**Example**: Price × Temperature interaction\n",
    "- β₁ = -5 (base price effect at temp=0)\n",
    "- β₃ = 0.1 (interaction)\n",
    "- At 20°C: price effect = -5 + 0.1×20 = -3\n",
    "- At 30°C: price effect = -5 + 0.1×30 = -2\n",
    "\n",
    "Interpretation: Each 10°C increase reduces price sensitivity by 1 unit.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 18: \"Heterogeneous Treatment Effects and Personalization.\"\n",
    "\n",
    "[^2]: Angrist, J. D. and Pischke, J. S. (2008). *Mostly Harmless Econometrics*. Princeton University Press. Chapter 4: \"Instrumental Variables.\"\n",
    "\n",
    "[^3]: Athey, S. and Imbens, G. W. (2016). Recursive partitioning for heterogeneous causal effects. *PNAS*, 113(27), 7353-7360.\n",
    "\n",
    "[^4]: Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. *PNAS*, 116(10), 4156-4165.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [03. Causal vs Predictive Models](./03_causal_vs_predictive.ipynb) — Comparing CATE models with predictive models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
