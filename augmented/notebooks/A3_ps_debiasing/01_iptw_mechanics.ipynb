{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing with Propensity Score: IPTW Mechanics\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Appendix A3 | Notebook 1 of 2**\n",
    "\n",
    "This notebook covers the mechanics of inverse probability of treatment\n",
    "weighting (IPTW) for debiasing observational data, with comparison\n",
    "between stored and estimated propensity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from augmented.common import *\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### The Email Marketing Problem\n",
    "\n",
    "**Scenario**: Financial company testing marketing emails for new advisory service.\n",
    "\n",
    "- Three emails: `em1`, `em2`, `em3` with different targeting\n",
    "- Assignment based on customer characteristics (age, income, investments)\n",
    "- Outcome: Whether customer converts to new service\n",
    "\n",
    "**Key insight**: Marketing team stored the assignment probabilities!\n",
    "\n",
    "### Why Propensity Score Debiasing?\n",
    "\n",
    "**Alternative to orthogonalization**:\n",
    "- Orthogonalization: Residualize both T and Y on X\n",
    "- PS debiasing: Reweight sample by inverse treatment probability\n",
    "\n",
    "**When to prefer PS**:\n",
    "1. **Stored probabilities**: No model needed → no estimation error\n",
    "2. **Binary/categorical treatment**: PS best defined for discrete T\n",
    "3. **Balance diagnostics**: Easy to check if debiasing worked\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "**Weight by inverse probability of what you got**:\n",
    "\n",
    "- If treated: weight = 1/P(T=1|X)\n",
    "- If control: weight = 1/P(T=0|X) = 1/(1-P(T=1|X))\n",
    "\n",
    "**Why this works**:\n",
    "- High-propensity treated units are downweighted (common)\n",
    "- Low-propensity treated units are upweighted (rare, like controls)\n",
    "- Creates \"pseudo-population\" where T ⊥ X\n",
    "\n",
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "IPTW upsamples:\n",
    "- Treated units that look like controls (low PS)\n",
    "- Control units that look like treated (high PS)\n",
    "\n",
    "These are exactly the units needed for counterfactual estimation!\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Propensity Score Definition\n",
    "\n",
    "For binary treatment:\n",
    "$$e(X) = P(T=1|X)$$\n",
    "\n",
    "For categorical treatment with K levels:\n",
    "$$e_k(X) = P(T=k|X) \\quad \\text{for } k = 0, 1, ..., K$$\n",
    "\n",
    "### 2.2 IPTW Weights\n",
    "\n",
    "**Binary treatment**:\n",
    "$$W_i = \\frac{T_i}{e(X_i)} + \\frac{1-T_i}{1-e(X_i)}$$\n",
    "\n",
    "**Categorical treatment**:\n",
    "$$W_i = \\sum_{k=0}^{K} \\frac{\\mathbf{1}\\{T_i=k\\}}{e_k(X_i)}$$\n",
    "\n",
    "### 2.3 Identification Result\n",
    "\n",
    "**Theorem (Horvitz-Thompson)**:\n",
    "\n",
    "Under conditional ignorability $(Y(0), Y(1)) \\perp T | X$ and positivity $0 < e(X) < 1$:\n",
    "\n",
    "$$E\\left[\\frac{Y \\cdot T}{e(X)}\\right] = E[Y(1)]$$\n",
    "\n",
    "$$E\\left[\\frac{Y \\cdot (1-T)}{1-e(X)}\\right] = E[Y(0)]$$\n",
    "\n",
    "**Proof sketch**:\n",
    "\n",
    "\\begin{align}\n",
    "E\\left[\\frac{Y \\cdot T}{e(X)}\\right] &= E\\left[E\\left[\\frac{Y \\cdot T}{e(X)} \\Bigg| X\\right]\\right] \\\\\n",
    "&= E\\left[\\frac{1}{e(X)} E[Y \\cdot T | X]\\right] \\\\\n",
    "&= E\\left[\\frac{1}{e(X)} E[Y | T=1, X] \\cdot P(T=1|X)\\right] \\\\\n",
    "&= E\\left[\\frac{1}{e(X)} E[Y(1) | X] \\cdot e(X)\\right] \\\\\n",
    "&= E[E[Y(1)|X]] = E[Y(1)]\n",
    "\\end{align}\n",
    "\n",
    "### 2.4 ATE Estimator\n",
    "\n",
    "$$\\hat{\\tau}_{IPTW} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{Y_i T_i}{e(X_i)} - \\frac{1}{n}\\sum_{i=1}^{n} \\frac{Y_i (1-T_i)}{1-e(X_i)}$$\n",
    "\n",
    "### 2.5 Normalized Weights\n",
    "\n",
    "**Hajek estimator** (normalized IPTW):\n",
    "\n",
    "$$\\hat{\\tau}_{Hajek} = \\frac{\\sum_i Y_i T_i / e(X_i)}{\\sum_i T_i / e(X_i)} - \\frac{\\sum_i Y_i (1-T_i) / (1-e(X_i))}{\\sum_i (1-T_i) / (1-e(X_i))}$$\n",
    "\n",
    "- More stable (weights sum to n)\n",
    "- Slightly biased but lower variance\n",
    "- Recommended in practice\n",
    "\n",
    "```\n",
    "★ Key Result ──────────────────────────────────────────────────\n",
    "IPTW achieves balance: In the pseudo-population,\n",
    "\n",
    "$$E[X | T=1] \\approx E[X | T=0]$$\n",
    "\n",
    "Treatment becomes \"as good as random\" after weighting.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Email Marketing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or simulate email marketing data\n",
    "try:\n",
    "    email = load_facure_data('invest_email')\n",
    "    print(\"Loaded invest_email.csv from Facure data\")\n",
    "except:\n",
    "    # Simulate data\n",
    "    print(\"Generating simulated email data...\")\n",
    "    np.random.seed(42)\n",
    "    n = 5000\n",
    "    \n",
    "    # Customer characteristics\n",
    "    age = np.random.uniform(25, 55, n)\n",
    "    income = np.random.exponential(5000, n)\n",
    "    insurance = np.random.exponential(20000, n)\n",
    "    invested = np.random.exponential(10000, n)\n",
    "    \n",
    "    # Propensity scores (stored from experiment design)\n",
    "    # em1 targets mass market (lower income, less invested)\n",
    "    em1_ps = 1 / (1 + np.exp(0.001*income + 0.0001*invested - 3))\n",
    "    em1_ps = np.clip(em1_ps, 0.02, 0.98)\n",
    "    \n",
    "    # Treatment assignment based on stored PS\n",
    "    em1 = np.random.binomial(1, em1_ps)\n",
    "    \n",
    "    # Outcome: conversion (depends on age, slightly on treatment)\n",
    "    # True treatment effect = 0.05\n",
    "    convert_prob = 1 / (1 + np.exp(-0.1*age + 0.0001*income - 0.05*em1 - 2))\n",
    "    converted = np.random.binomial(1, convert_prob)\n",
    "    \n",
    "    email = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'income': income,\n",
    "        'insurance': insurance,\n",
    "        'invested': invested,\n",
    "        'em1_ps': em1_ps,\n",
    "        'em1': em1,\n",
    "        'converted': converted\n",
    "    })\n",
    "\n",
    "print(f\"\\nData shape: {email.shape}\")\n",
    "email.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define confounders\n",
    "confounders = ['age', 'income', 'insurance', 'invested']\n",
    "\n",
    "# Check confounding: correlations with treatment and outcome\n",
    "print(\"Correlations with treatment (em1) and outcome (converted):\")\n",
    "print(\"-\" * 50)\n",
    "corr_df = email[confounders + ['em1', 'converted']].corr()[['em1', 'converted']]\n",
    "print(corr_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confounding: distribution by treatment status\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for i, var in enumerate(confounders):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    for em1_val, color, label in [(0, COLORS['gray'], 'Control (em1=0)'), \n",
    "                                   (1, COLORS['blue'], 'Treated (em1=1)')]:\n",
    "        subset = email[email['em1'] == em1_val][var]\n",
    "        ax.hist(subset, bins=30, alpha=0.5, color=color, label=label, density=True)\n",
    "    \n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{var} by Treatment Status')\n",
    "    ax.legend()\n",
    "    apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Covariate Imbalance Before Weighting', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Estimate (Biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive difference in means\n",
    "treated_mean = email[email['em1'] == 1]['converted'].mean()\n",
    "control_mean = email[email['em1'] == 0]['converted'].mean()\n",
    "naive_ate = treated_mean - control_mean\n",
    "\n",
    "print(\"Naive Analysis (Ignoring Confounding):\")\n",
    "print(f\"  Treated conversion rate: {treated_mean:.3f}\")\n",
    "print(f\"  Control conversion rate: {control_mean:.3f}\")\n",
    "print(f\"  Naive ATE: {naive_ate:.4f}\")\n",
    "print(\"\\n  Warning: This is biased due to confounding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### IPTW with Stored Propensity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iptw_weights(treatment, propensity_score):\n",
    "    \"\"\"\n",
    "    Compute IPTW weights for binary treatment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    treatment : array\n",
    "        Binary treatment indicator (0 or 1)\n",
    "    propensity_score : array\n",
    "        Probability of treatment given covariates\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        IPTW weights\n",
    "    \"\"\"\n",
    "    weights = np.where(\n",
    "        treatment == 1,\n",
    "        1 / propensity_score,\n",
    "        1 / (1 - propensity_score)\n",
    "    )\n",
    "    return weights\n",
    "\n",
    "\n",
    "def iptw_ate(outcome, treatment, propensity_score, normalized=True):\n",
    "    \"\"\"\n",
    "    Estimate ATE using IPTW.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    outcome : array\n",
    "        Outcome variable\n",
    "    treatment : array\n",
    "        Binary treatment indicator\n",
    "    propensity_score : array\n",
    "        Propensity scores\n",
    "    normalized : bool\n",
    "        If True, use Hajek estimator (normalized weights)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        ate, mu1, mu0\n",
    "    \"\"\"\n",
    "    T = np.asarray(treatment)\n",
    "    Y = np.asarray(outcome)\n",
    "    e = np.asarray(propensity_score)\n",
    "    \n",
    "    if normalized:\n",
    "        # Hajek estimator\n",
    "        mu1 = np.sum(Y * T / e) / np.sum(T / e)\n",
    "        mu0 = np.sum(Y * (1 - T) / (1 - e)) / np.sum((1 - T) / (1 - e))\n",
    "    else:\n",
    "        # Horvitz-Thompson estimator\n",
    "        n = len(Y)\n",
    "        mu1 = np.sum(Y * T / e) / n\n",
    "        mu0 = np.sum(Y * (1 - T) / (1 - e)) / n\n",
    "    \n",
    "    return {\n",
    "        'ate': mu1 - mu0,\n",
    "        'mu1': mu1,\n",
    "        'mu0': mu0\n",
    "    }\n",
    "\n",
    "# IPTW with stored propensity scores\n",
    "result_stored = iptw_ate(\n",
    "    email['converted'], \n",
    "    email['em1'], \n",
    "    email['em1_ps'],\n",
    "    normalized=True\n",
    ")\n",
    "\n",
    "print(\"IPTW with Stored Propensity Scores:\")\n",
    "print(f\"  E[Y(1)]: {result_stored['mu1']:.4f}\")\n",
    "print(f\"  E[Y(0)]: {result_stored['mu0']:.4f}\")\n",
    "print(f\"  ATE: {result_stored['ate']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Approach (Create Debiased Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_debiased_dataset(data, treatment_col, ps_col, n_samples=10000, seed=42):\n",
    "    \"\"\"\n",
    "    Create debiased dataset by resampling with IPTW weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Original dataset\n",
    "    treatment_col : str\n",
    "        Treatment column name\n",
    "    ps_col : str\n",
    "        Propensity score column name\n",
    "    n_samples : int\n",
    "        Number of samples to draw\n",
    "    seed : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Resampled dataset\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Compute weights\n",
    "    weights = compute_iptw_weights(data[treatment_col], data[ps_col])\n",
    "    \n",
    "    # Normalize weights for sampling\n",
    "    weights_normalized = weights / weights.sum()\n",
    "    \n",
    "    # Resample with replacement\n",
    "    indices = np.random.choice(\n",
    "        len(data), size=n_samples, replace=True, p=weights_normalized\n",
    "    )\n",
    "    \n",
    "    return data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "# Create debiased dataset\n",
    "email_debiased = create_debiased_dataset(email, 'em1', 'em1_ps', n_samples=10000)\n",
    "\n",
    "print(f\"Original dataset: {len(email)} rows\")\n",
    "print(f\"Debiased dataset: {len(email_debiased)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check balance in debiased dataset\n",
    "print(\"Correlations BEFORE debiasing:\")\n",
    "print(email[confounders + ['em1']].corr()['em1'].round(3))\n",
    "\n",
    "print(\"\\nCorrelations AFTER debiasing:\")\n",
    "print(email_debiased[confounders + ['em1']].corr()['em1'].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize balance improvement\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for i, var in enumerate(confounders):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    for em1_val, color, label in [(0, COLORS['gray'], 'Control (em1=0)'), \n",
    "                                   (1, COLORS['blue'], 'Treated (em1=1)')]:\n",
    "        subset = email_debiased[email_debiased['em1'] == em1_val][var]\n",
    "        ax.hist(subset, bins=30, alpha=0.5, color=color, label=label, density=True)\n",
    "    \n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{var} by Treatment Status')\n",
    "    ax.legend()\n",
    "    apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Covariate Balance AFTER Weighting', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPTW with Estimated Propensity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def estimate_propensity_score(X, T, n_folds=5, seed=42):\n",
    "    \"\"\"\n",
    "    Estimate propensity scores using calibrated classifier with cross-validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array or DataFrame\n",
    "        Covariates\n",
    "    T : array\n",
    "        Binary treatment\n",
    "    n_folds : int\n",
    "        Number of cross-validation folds\n",
    "    seed : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Estimated propensity scores\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Calibrated classifier\n",
    "    base_clf = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=3, min_samples_leaf=40, random_state=seed\n",
    "    )\n",
    "    clf = CalibratedClassifierCV(base_clf, cv=3)\n",
    "    \n",
    "    # Cross-validated predictions\n",
    "    ps_estimated = cross_val_predict(clf, X, T, cv=n_folds, method='predict_proba')[:, 1]\n",
    "    \n",
    "    return ps_estimated\n",
    "\n",
    "# Estimate propensity scores\n",
    "X = email[confounders]\n",
    "T = email['em1']\n",
    "\n",
    "ps_estimated = estimate_propensity_score(X, T)\n",
    "\n",
    "# Compare stored vs estimated\n",
    "email_compare = email.assign(ps_estimated=ps_estimated)\n",
    "\n",
    "print(\"Propensity Score Comparison:\")\n",
    "print(f\"  Correlation (stored vs estimated): {np.corrcoef(email['em1_ps'], ps_estimated)[0,1]:.3f}\")\n",
    "print(f\"  Mean absolute difference: {np.mean(np.abs(email['em1_ps'] - ps_estimated)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PS comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "ax = axes[0]\n",
    "ax.scatter(email['em1_ps'], ps_estimated, alpha=0.3, s=10)\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax.set_xlabel('Stored Propensity Score')\n",
    "ax.set_ylabel('Estimated Propensity Score')\n",
    "ax.set_title('Stored vs Estimated PS')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Calibration curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "ax = axes[1]\n",
    "prob_true, prob_pred = calibration_curve(T, ps_estimated, n_bins=10)\n",
    "ax.plot(prob_pred, prob_true, 'o-', color=COLORS['blue'], label='Calibrated RF')\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration Curve')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPTW with estimated PS\n",
    "result_estimated = iptw_ate(\n",
    "    email['converted'], \n",
    "    email['em1'], \n",
    "    ps_estimated,\n",
    "    normalized=True\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "print(\"Comparison of IPTW Estimates:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Naive (no adjustment):        {naive_ate:.4f}\")\n",
    "print(f\"IPTW with stored PS:          {result_stored['ate']:.4f}\")\n",
    "print(f\"IPTW with estimated PS:       {result_estimated['ate']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check balance with estimated PS\n",
    "email_debiased_est = create_debiased_dataset(\n",
    "    email.assign(ps_est=ps_estimated), 'em1', 'ps_est', n_samples=10000\n",
    ")\n",
    "\n",
    "print(\"Balance Check (Correlations with em1):\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Original:\")\n",
    "print(email[confounders + ['em1']].corr()['em1'].round(3))\n",
    "print(\"\\nDebiased with Stored PS:\")\n",
    "print(email_debiased[confounders + ['em1']].corr()['em1'].round(3))\n",
    "print(\"\\nDebiased with Estimated PS:\")\n",
    "print(email_debiased_est[confounders + ['em1']].corr()['em1'].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Key Takeaway ────────────────────────────────────────────────\n",
    "Stored vs Estimated Propensity Scores:\n",
    "\n",
    "| Aspect | Stored PS | Estimated PS |\n",
    "|--------|-----------|---------------|\n",
    "| Accuracy | Exact (no error) | Model-dependent |\n",
    "| Balance | Near-perfect | Residual imbalance |\n",
    "| Requirements | Experiment design | Observational data OK |\n",
    "| Risk | None | Model misspecification |\n",
    "\n",
    "**Best practice**: Store PS during experiment whenever possible!\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: What is the intuition behind IPTW?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Core intuition**: Create a pseudo-population where treatment is random.\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "1. **Treated units with low PS** (look like controls)\n",
    "   - These are rare → upweight them\n",
    "   - They represent counterfactual Y(1) for control-like population\n",
    "\n",
    "2. **Control units with high PS** (look like treated)\n",
    "   - These are rare → upweight them\n",
    "   - They represent counterfactual Y(0) for treated-like population\n",
    "\n",
    "**Mathematical intuition**:\n",
    "$$E\\left[\\frac{Y \\cdot T}{e(X)}\\right] = E\\left[Y(1)\\right]$$\n",
    "\n",
    "The 1/e(X) weighting \"undoes\" the selection bias in who got treated.\n",
    "\n",
    "**Analogy**: Survey weighting\n",
    "- If urban residents are oversampled 3x, weight them by 1/3\n",
    "- Similarly, if high-PS units are overtreated, weight by 1/PS\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: When should you use IPTW vs orthogonalization (DML)?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Prefer IPTW when**:\n",
    "\n",
    "1. **Stored propensity scores available**\n",
    "   - No estimation error\n",
    "   - Exact balance achievable\n",
    "\n",
    "2. **Binary or categorical treatment**\n",
    "   - PS well-defined for discrete T\n",
    "   - Continuous T requires different approach\n",
    "\n",
    "3. **Balance diagnostics important**\n",
    "   - Easy to check: plot PS distributions\n",
    "   - Clear positivity violations visible\n",
    "\n",
    "**Prefer orthogonalization/DML when**:\n",
    "\n",
    "1. **Continuous treatment**\n",
    "   - PS not well-defined\n",
    "   - Residualization natural\n",
    "\n",
    "2. **Estimating PS is difficult**\n",
    "   - High-dimensional X\n",
    "   - Complex treatment mechanism\n",
    "\n",
    "3. **Doubly robust properties needed**\n",
    "   - DML robust to either nuisance model error\n",
    "   - IPTW requires correct PS\n",
    "\n",
    "**Both can be combined**: AIPW/DR estimator uses both PS and outcome regression.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: What's the difference between Horvitz-Thompson and Hajek estimators?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Horvitz-Thompson** (unnormalized):\n",
    "$$\\hat{\\mu}_1^{HT} = \\frac{1}{n}\\sum_i \\frac{Y_i T_i}{e(X_i)}$$\n",
    "\n",
    "- Unbiased\n",
    "- Weights don't sum to n\n",
    "- Can be unstable with extreme PS\n",
    "\n",
    "**Hajek** (normalized):\n",
    "$$\\hat{\\mu}_1^{H} = \\frac{\\sum_i Y_i T_i / e(X_i)}{\\sum_i T_i / e(X_i)}$$\n",
    "\n",
    "- Slightly biased\n",
    "- Weights sum to n\n",
    "- More stable (ratio of two consistent estimators)\n",
    "\n",
    "**Practical recommendation**:\n",
    "- Use Hajek in practice (lower variance)\n",
    "- Bias negligible for reasonable n\n",
    "- Horvitz-Thompson for theoretical derivations\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: What are the assumptions for IPTW to work?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**1. Conditional Ignorability (Unconfoundedness)**:\n",
    "$$(Y(0), Y(1)) \\perp T | X$$\n",
    "- No unmeasured confounders\n",
    "- All selection into treatment explained by X\n",
    "\n",
    "**2. Positivity (Common Support)**:\n",
    "$$0 < e(X) < 1 \\quad \\text{for all } X$$\n",
    "- Everyone has positive probability of both T=0 and T=1\n",
    "- Violations → infinite weights → instability\n",
    "\n",
    "**3. SUTVA**:\n",
    "- No interference between units\n",
    "- Treatment effect doesn't depend on others' treatment\n",
    "\n",
    "**4. Correct PS specification** (if estimated):\n",
    "- Model must capture true treatment mechanism\n",
    "- Calibration important (probabilities must be accurate)\n",
    "\n",
    "**Testing assumptions**:\n",
    "- Positivity: Plot PS distributions by T\n",
    "- Balance: Check covariate overlap after weighting\n",
    "- Cannot test unconfoundedness directly\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: Why should you store propensity scores during experiments?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Benefits of stored PS**:\n",
    "\n",
    "1. **No estimation error**\n",
    "   - True PS known by design\n",
    "   - No model misspecification risk\n",
    "\n",
    "2. **Perfect balance achievable**\n",
    "   - Weights exactly correct\n",
    "   - Correlations → 0 in pseudo-population\n",
    "\n",
    "3. **Valid inference**\n",
    "   - SE formulas exact\n",
    "   - No additional variance from PS estimation\n",
    "\n",
    "4. **Audit trail**\n",
    "   - Document randomization process\n",
    "   - Reproducibility\n",
    "\n",
    "**Practical implementation**:\n",
    "\n",
    "```python\n",
    "# During experiment design\n",
    "def assign_treatment(customer):\n",
    "    ps = compute_propensity(customer)\n",
    "    treatment = np.random.binomial(1, ps)\n",
    "    return treatment, ps  # Store BOTH!\n",
    "\n",
    "# Store in database\n",
    "INSERT INTO experiments (customer_id, treatment, propensity_score, ...)\n",
    "```\n",
    "\n",
    "**Cost of not storing**:\n",
    "- Must estimate PS from data\n",
    "- Estimation error → residual confounding\n",
    "- More complex analysis\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Horvitz, D. G., & Thompson, D. J. (1952). A Generalization of Sampling Without \n",
    "      Replacement from a Finite Universe. *Journal of the American Statistical Association*.\n",
    "\n",
    "[^2]: Rosenbaum, P. R., & Rubin, D. B. (1983). The Central Role of the Propensity Score \n",
    "      in Observational Studies for Causal Effects. *Biometrika*.\n",
    "\n",
    "[^3]: Imbens, G. W. (2004). Nonparametric Estimation of Average Treatment Effects Under \n",
    "      Exogeneity: A Review. *Review of Economics and Statistics*.\n",
    "\n",
    "[^4]: Facure, M. (2022). *Causal Inference for the Brave and True*, Appendix: \n",
    "      Debiasing with Propensity Score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
