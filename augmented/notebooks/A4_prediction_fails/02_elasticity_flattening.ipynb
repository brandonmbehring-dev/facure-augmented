{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Prediction Fails: Elasticity Flattening\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Appendix A4 | Notebook 2 of 3**\n",
    "\n",
    "This notebook provides the technical depth on why ML partitioning flattens\n",
    "treatment response curves, including DAG-based explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from augmented.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "# ML imports\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### The Optimization Landscape\n",
    "\n",
    "Consider a typical pricing or coupon problem:\n",
    "\n",
    "1. **Response curve**: As treatment $T$ increases, outcome $Y$ first rises, then falls\n",
    "2. **Optimal treatment**: Find $T^*$ where $\\frac{\\partial Y}{\\partial T} = 0$\n",
    "3. **Personalization**: Different customers may have different optima\n",
    "\n",
    "### What Prediction Does\n",
    "\n",
    "When we partition by predicted $\\hat{Y}$:\n",
    "\n",
    "1. **Goal**: Group similar customers for personalized treatment\n",
    "2. **Reality**: Groups customers with similar *outcome levels*\n",
    "3. **Problem**: Same outcome level ≠ same treatment response\n",
    "\n",
    "### The Flattening Effect\n",
    "\n",
    "Within prediction partitions:\n",
    "- $Y$ variance is minimized (that's what good prediction means!)\n",
    "- $\\frac{\\partial Y}{\\partial T}$ becomes unobservable\n",
    "- Response curves appear flat\n",
    "\n",
    "> **Key insight**: Prediction \"explains away\" the variation we need to see treatment effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 The Response Function\n",
    "\n",
    "Let the true response function be:\n",
    "\n",
    "$$Y = f(T, X) + \\epsilon$$\n",
    "\n",
    "For pricing/coupons, often quadratic:\n",
    "\n",
    "$$Y = \\alpha + \\beta T - \\gamma T^2 + \\delta' X + \\epsilon$$\n",
    "\n",
    "The treatment effect (elasticity):\n",
    "\n",
    "$$\\frac{\\partial Y}{\\partial T} = \\beta - 2\\gamma T$$\n",
    "\n",
    "### 2.2 Optimization Requires the Derivative\n",
    "\n",
    "The optimal treatment solves:\n",
    "\n",
    "$$T^* = \\arg\\max_T E[Y|T, X] \\Rightarrow \\frac{\\partial E[Y|T,X]}{\\partial T} = 0$$\n",
    "\n",
    "For quadratic case: $T^* = \\frac{\\beta}{2\\gamma}$\n",
    "\n",
    "**Key requirement**: We need to estimate $\\frac{\\partial Y}{\\partial T}$, not $Y$ itself.\n",
    "\n",
    "### 2.3 How Prediction Partitioning Fails\n",
    "\n",
    "**Step 1**: Fit model $\\hat{Y} = M(T, X)$\n",
    "\n",
    "**Step 2**: Partition data by $\\hat{Y}$ into bands $B_1, ..., B_K$\n",
    "\n",
    "**Step 3**: Within each band $B_k$:\n",
    "$$\\text{Var}(Y | Y \\in B_k) \\approx \\text{small}$$\n",
    "\n",
    "(This is what good prediction means: correct prediction → low residual variance)\n",
    "\n",
    "**Step 4**: If $Y$ barely varies, we cannot estimate:\n",
    "$$\\frac{\\partial Y}{\\partial T} \\Big|_{B_k} \\approx 0$$\n",
    "\n",
    "Even though the true elasticity may be large!\n",
    "\n",
    "### 2.4 DAG Perspective\n",
    "\n",
    "**Original DAG**:\n",
    "```\n",
    "    X\n",
    "   / \\\n",
    "  v   v\n",
    "  T → Y\n",
    "```\n",
    "\n",
    "**With prediction model**:\n",
    "```\n",
    "    X\n",
    "   / \\\n",
    "  v   v\n",
    "  T → Y ← (learned by M)\n",
    "        ↓\n",
    "       M(X,T)\n",
    "```\n",
    "\n",
    "**Conditioning on $M(X,T)$**: Blocks part of the $T \\to Y$ path!\n",
    "\n",
    "This is the \"bad control\" problem: we're conditioning on a descendant of $Y$ that's also influenced by $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Synthetic Data with Known Response Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quadratic_response(n=5000, seed=42):\n",
    "    \"\"\"Generate data with known quadratic response to treatment.\n",
    "    \n",
    "    True DGP: Y = 100 + 20*T - 1.5*T^2 + 5*X + noise\n",
    "    Optimal T = 20/(2*1.5) ≈ 6.67\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Confounder\n",
    "    X = np.random.normal(0, 1, n)\n",
    "    \n",
    "    # Treatment (confounded by X)\n",
    "    T = 5 + 2*X + np.random.normal(0, 2, n)\n",
    "    T = np.clip(T, 0, 15)  # Realistic range\n",
    "    \n",
    "    # Outcome with quadratic treatment effect\n",
    "    Y = 100 + 20*T - 1.5*T**2 + 5*X + np.random.normal(0, 10, n)\n",
    "    \n",
    "    return pd.DataFrame({'T': T, 'X': X, 'Y': Y})\n",
    "\n",
    "\n",
    "data = generate_quadratic_response()\n",
    "\n",
    "print(\"Data summary:\")\n",
    "print(data.describe().round(2))\n",
    "print(f\"\\nTrue optimal T: {20 / (2 * 1.5):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the true response curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Response curve\n",
    "ax = axes[0]\n",
    "T_grid = np.linspace(0, 15, 100)\n",
    "Y_true = 100 + 20*T_grid - 1.5*T_grid**2  # No X effect, mean response\n",
    "\n",
    "ax.plot(T_grid, Y_true, 'b-', linewidth=3, label='True E[Y|T] (at X=0)')\n",
    "ax.scatter(data['T'], data['Y'], alpha=0.1, s=10, color='gray', label='Data')\n",
    "ax.axvline(20/(2*1.5), color=COLORS['red'], linestyle='--', label=f'Optimal T = {20/(2*1.5):.2f}')\n",
    "\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Outcome Y')\n",
    "ax.set_title('True Quadratic Response Curve')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Derivative (elasticity)\n",
    "ax = axes[1]\n",
    "dY_dT = 20 - 2*1.5*T_grid  # True derivative\n",
    "\n",
    "ax.plot(T_grid, dY_dT, 'g-', linewidth=3, label='True ∂Y/∂T')\n",
    "ax.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.axvline(20/(2*1.5), color=COLORS['red'], linestyle='--', label='Optimal (derivative = 0)')\n",
    "ax.fill_between(T_grid, dY_dT, 0, where=(dY_dT > 0), alpha=0.2, color='green', label='Increase T')\n",
    "ax.fill_between(T_grid, dY_dT, 0, where=(dY_dT < 0), alpha=0.2, color='red', label='Decrease T')\n",
    "\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('∂Y/∂T (Elasticity)')\n",
    "ax.set_title('Treatment Effect (Derivative)')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning by Predicted Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit predictive model\n",
    "model = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "model.fit(data[['T', 'X']], data['Y'])\n",
    "\n",
    "# Create prediction bands\n",
    "data['Y_hat'] = model.predict(data[['T', 'X']])\n",
    "data['band'] = pd.qcut(data['Y_hat'], q=5, labels=['B1', 'B2', 'B3', 'B4', 'B5'])\n",
    "\n",
    "print(f\"Model R² (train): {model.score(data[['T', 'X']], data['Y']):.3f}\")\n",
    "print(f\"\\nBand distribution:\")\n",
    "print(data['band'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Y variance reduction within bands\n",
    "overall_var = data['Y'].var()\n",
    "band_vars = data.groupby('band')['Y'].var()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "bars = ax.bar(band_vars.index, band_vars.values, color=COLORS['blue'], alpha=0.7)\n",
    "ax.axhline(overall_var, color=COLORS['red'], linestyle='--', linewidth=2, \n",
    "           label=f'Overall Var(Y) = {overall_var:.0f}')\n",
    "\n",
    "ax.set_xlabel('Prediction Band')\n",
    "ax.set_ylabel('Var(Y) within band')\n",
    "ax.set_title('Variance Reduction: Within-Band Variance << Overall Variance')\n",
    "ax.legend()\n",
    "\n",
    "# Annotate\n",
    "for bar, val in zip(bars, band_vars.values):\n",
    "    ax.annotate(f'{val:.0f}', (bar.get_x() + bar.get_width()/2, val),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "apply_tufte_style(ax)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVariance reduction: {100*(1 - band_vars.mean()/overall_var):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Flattening Effect: Within-Band Response Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate response curve within each band\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Overall response (top left)\n",
    "ax = axes[0, 0]\n",
    "t_bins = pd.cut(data['T'], bins=10)\n",
    "overall_response = data.groupby(t_bins)['Y'].mean()\n",
    "t_mids = [interval.mid for interval in overall_response.index]\n",
    "\n",
    "ax.plot(t_mids, overall_response.values, 'o-', color=COLORS['blue'], \n",
    "        linewidth=2, markersize=8, label='Data')\n",
    "ax.plot(T_grid, 100 + 20*T_grid - 1.5*T_grid**2, '--', color=COLORS['red'],\n",
    "        linewidth=2, label='True curve')\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Mean Y')\n",
    "ax.set_title('OVERALL Response Curve\\n(Clear curvature)')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Within-band responses\n",
    "for idx, band in enumerate(['B1', 'B2', 'B3', 'B4', 'B5']):\n",
    "    row, col = (idx + 1) // 3, (idx + 1) % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    band_data = data[data['band'] == band]\n",
    "    t_bins_band = pd.cut(band_data['T'], bins=8)\n",
    "    band_response = band_data.groupby(t_bins_band)['Y'].mean()\n",
    "    t_mids_band = [interval.mid for interval in band_response.index if not pd.isna(band_response[interval])]\n",
    "    y_vals = [band_response[interval] for interval in band_response.index if not pd.isna(band_response[interval])]\n",
    "    \n",
    "    if len(t_mids_band) > 1:\n",
    "        ax.plot(t_mids_band, y_vals, 'o-', color=COLORS['orange'],\n",
    "                linewidth=2, markersize=6)\n",
    "        \n",
    "        # Fit linear trend to show flatness\n",
    "        if len(t_mids_band) >= 3:\n",
    "            slope = np.polyfit(t_mids_band, y_vals, 1)[0]\n",
    "            ax.set_title(f'Band {band}\\n(Slope: {slope:.2f})')\n",
    "        else:\n",
    "            ax.set_title(f'Band {band}')\n",
    "    else:\n",
    "        ax.set_title(f'Band {band}\\n(Insufficient data)')\n",
    "    \n",
    "    ax.set_xlabel('Treatment T')\n",
    "    ax.set_ylabel('Mean Y')\n",
    "    apply_tufte_style(ax)\n",
    "\n",
    "plt.suptitle('Within-Band Response Curves Are FLATTENED', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify flattening: estimated elasticity overall vs within bands\n",
    "def estimate_elasticity(df):\n",
    "    \"\"\"Simple OLS elasticity estimate.\"\"\"\n",
    "    if df['T'].std() < 0.1 or len(df) < 20:\n",
    "        return np.nan\n",
    "    cov = np.cov(df['T'], df['Y'])[0, 1]\n",
    "    var = df['T'].var()\n",
    "    return cov / var\n",
    "\n",
    "\n",
    "# True elasticity at mean T\n",
    "mean_T = data['T'].mean()\n",
    "true_elasticity = 20 - 2*1.5*mean_T\n",
    "\n",
    "# Overall estimated elasticity\n",
    "overall_elasticity = estimate_elasticity(data)\n",
    "\n",
    "# Within-band elasticities\n",
    "band_elasticities = data.groupby('band').apply(estimate_elasticity)\n",
    "\n",
    "print(f\"True elasticity at mean T: {true_elasticity:.2f}\")\n",
    "print(f\"Overall estimated elasticity: {overall_elasticity:.2f}\")\n",
    "print(f\"\\nWithin-band elasticities:\")\n",
    "print(band_elasticities.round(2))\n",
    "print(f\"\\nAverage within-band elasticity: {band_elasticities.mean():.2f}\")\n",
    "print(f\"\\n⚠️ Within-band elasticities are MUCH SMALLER!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "Within prediction bands:\n",
    "1. Y variance shrinks (good prediction → low residual)\n",
    "2. Treatment effect appears smaller than reality\n",
    "3. Cannot identify optimal treatment level\n",
    "\n",
    "The prediction model \"absorbs\" the variation we need to see.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Why Removing T from the Model Doesn't Fix It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model WITHOUT treatment\n",
    "model_no_T = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "model_no_T.fit(data[['X']], data['Y'])  # Only X, not T\n",
    "\n",
    "data['Y_hat_noT'] = model_no_T.predict(data[['X']])\n",
    "data['band_noT'] = pd.qcut(data['Y_hat_noT'], q=5, labels=['B1', 'B2', 'B3', 'B4', 'B5'])\n",
    "\n",
    "print(f\"Model R² without T: {model_no_T.score(data[['X']], data['Y']):.3f}\")\n",
    "print(f\"(Lower R² but still captures X → Y relationship)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within-band elasticities WITHOUT T in model\n",
    "band_elasticities_noT = data.groupby('band_noT').apply(estimate_elasticity)\n",
    "\n",
    "# Compare\n",
    "comparison = pd.DataFrame({\n",
    "    'With T': band_elasticities,\n",
    "    'Without T': band_elasticities_noT\n",
    "})\n",
    "\n",
    "print(\"Within-band elasticities:\")\n",
    "print(comparison.round(2))\n",
    "print(f\"\\nTrue elasticity at mean: {true_elasticity:.2f}\")\n",
    "print(f\"\\nRemoving T helps somewhat, but still flattened!\")\n",
    "print(f\"\\nReason: X → T, so model indirectly learns T through X.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG Explanation\n",
    "\n",
    "**Why removing T from the model doesn't fully help:**\n",
    "\n",
    "```\n",
    "    X ─────┐\n",
    "    │      │\n",
    "    v      v\n",
    "    T ───→ Y\n",
    "    │      ↑\n",
    "    │      │\n",
    "    └──→ M(X)\n",
    "```\n",
    "\n",
    "Since $X \\to T$, the model $M(X)$ can still learn about $T$ through $X$.\n",
    "\n",
    "- If high $X$ leads to high $T$, and high $T$ increases $Y$...\n",
    "- Then $M(X)$ predicts higher $Y$ for high $X$\n",
    "- Even though $M$ never sees $T$ directly\n",
    "\n",
    "**The information flows**: $T \\to Y \\to M(X)$ via the path $X \\to T \\to Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Y_hat from X-only model still correlates with T\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Correlation between X-only prediction and T\n",
    "ax = axes[0]\n",
    "ax.scatter(data['T'], data['Y_hat_noT'], alpha=0.3, s=10, color=COLORS['blue'])\n",
    "corr = data['T'].corr(data['Y_hat_noT'])\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Predicted Y (X-only model)')\n",
    "ax.set_title(f'Prediction (no T) Still Correlates with T\\nCorr = {corr:.3f}')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Show the indirect path\n",
    "ax = axes[1]\n",
    "ax.scatter(data['X'], data['T'], alpha=0.3, s=10, color=COLORS['orange'])\n",
    "corr_xt = data['X'].corr(data['T'])\n",
    "ax.set_xlabel('Confounder X')\n",
    "ax.set_ylabel('Treatment T')\n",
    "ax.set_title(f'X → T Confounding\\nCorr = {corr_xt:.3f}')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.suptitle('Indirect Learning: Model learns T through X', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Only True Fix: Random Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with RANDOM treatment (no confounding)\n",
    "def generate_random_treatment(n=5000, seed=42):\n",
    "    \"\"\"Treatment is independent of X.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = np.random.normal(0, 1, n)\n",
    "    T = np.random.uniform(0, 15, n)  # Random, not confounded!\n",
    "    Y = 100 + 20*T - 1.5*T**2 + 5*X + np.random.normal(0, 10, n)\n",
    "    \n",
    "    return pd.DataFrame({'T': T, 'X': X, 'Y': Y})\n",
    "\n",
    "\n",
    "data_rct = generate_random_treatment()\n",
    "\n",
    "# Fit X-only model and partition\n",
    "model_rct = GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "model_rct.fit(data_rct[['X']], data_rct['Y'])\n",
    "\n",
    "data_rct['Y_hat'] = model_rct.predict(data_rct[['X']])\n",
    "data_rct['band'] = pd.qcut(data_rct['Y_hat'], q=5, labels=['B1', 'B2', 'B3', 'B4', 'B5'])\n",
    "\n",
    "# Check elasticities\n",
    "band_elasticities_rct = data_rct.groupby('band').apply(estimate_elasticity)\n",
    "\n",
    "print(\"With RANDOM treatment (no confounding):\")\n",
    "print(f\"\\nTrue elasticity at mean T: {20 - 2*1.5*data_rct['T'].mean():.2f}\")\n",
    "print(f\"\\nWithin-band elasticities:\")\n",
    "print(band_elasticities_rct.round(2))\n",
    "print(f\"\\nAverage: {band_elasticities_rct.mean():.2f}\")\n",
    "print(f\"\\n✅ Much closer to truth! Random treatment breaks the indirect path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "summary = pd.DataFrame({\n",
    "    'Scenario': ['Confounded + Model with T', 'Confounded + Model without T', 'Random T + Model without T'],\n",
    "    'Avg Within-Band Elasticity': [\n",
    "        band_elasticities.mean(),\n",
    "        band_elasticities_noT.mean(),\n",
    "        band_elasticities_rct.mean()\n",
    "    ],\n",
    "    'True Elasticity': [true_elasticity, true_elasticity, 20 - 2*1.5*data_rct['T'].mean()]\n",
    "})\n",
    "\n",
    "summary['Attenuation'] = summary['Avg Within-Band Elasticity'] / summary['True Elasticity']\n",
    "\n",
    "print(\"Summary: Elasticity Attenuation by Scenario\")\n",
    "print(summary.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Key Takeaway ────────────────────────────────────────────────\n",
    "With confounded data:\n",
    "- Including T in model: Severe attenuation\n",
    "- Excluding T from model: Moderate attenuation (X → T leakage)\n",
    "\n",
    "With randomized treatment:\n",
    "- Prediction partitions preserve treatment effects\n",
    "- Can still identify optimal T within each band\n",
    "\n",
    "Implication: For causal optimization, run experiments or use\n",
    "causal inference methods designed for observational data.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: Explain the \"flattening\" phenomenon mathematically.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Setup**: True model $Y = f(T) + \\epsilon$ with $\\text{Var}(Y) = \\sigma^2_Y$\n",
    "\n",
    "**Prediction partition**: Group by $\\hat{Y}$ into bands where $\\text{Var}(Y|\\text{band}) = \\sigma^2_b$\n",
    "\n",
    "**Good prediction implies**: $\\sigma^2_b << \\sigma^2_Y$\n",
    "\n",
    "**Treatment effect estimation**:\n",
    "$$\\hat{\\beta} = \\frac{\\text{Cov}(T, Y)}{\\text{Var}(T)}$$\n",
    "\n",
    "Within bands: $\\text{Cov}(T, Y|\\text{band}) \\approx 0$ because $Y$ barely varies!\n",
    "\n",
    "**Result**: $\\hat{\\beta}_{\\text{within-band}} \\approx 0$ even when $\\beta_{\\text{true}} \\neq 0$\n",
    "\n",
    "The prediction model \"explains away\" the $T \\to Y$ variation.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Draw the DAG that explains why removing T from the prediction model doesn't fully solve the problem.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "```\n",
    "      X\n",
    "     / \\\n",
    "    v   \\\n",
    "    T    v\n",
    "    |    Y\n",
    "    v   /\n",
    "    Y ←┘\n",
    "    ↓\n",
    "   M(X)\n",
    "```\n",
    "\n",
    "**Key path**: $X \\to T \\to Y$\n",
    "\n",
    "Since $X$ causes $T$, $M(X)$ indirectly captures information about $T$:\n",
    "- $M(X)$ learns: High $X$ → High $Y$\n",
    "- Part of this is: High $X$ → High $T$ → High $Y$\n",
    "- So $M(X)$ implicitly learns about $T$ through $X$\n",
    "\n",
    "**Solution**: Only if $T \\perp\\!\\!\\!\\perp X$ (randomization) does excluding $T$ fully work.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: In what business scenario might the flattening problem lead to significant financial losses?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Scenario**: Discount optimization for e-commerce\n",
    "\n",
    "**What happens**:\n",
    "1. ML model predicts customer LTV accurately (high R²)\n",
    "2. Partition customers by predicted LTV\n",
    "3. Within each partition, test different discount levels\n",
    "4. Observe near-zero differences → conclude \"discounts don't matter\"\n",
    "5. Give minimal/no discounts to everyone\n",
    "\n",
    "**Reality**:\n",
    "- Some customers have high discount sensitivity (would buy more with discount)\n",
    "- Some have low sensitivity (buy regardless)\n",
    "- Prediction partitions mixed these together\n",
    "- True optimal: Discounts to sensitive customers only\n",
    "\n",
    "**Loss calculation**:\n",
    "- Say 30% of customers have elasticity = -2 (10% discount → 20% more purchases)\n",
    "- Giving no discount loses 20% revenue from this segment\n",
    "- If they represent $10M in potential revenue → $2M loss\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: How would you properly design an analysis to find optimal discount levels?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Step 1**: Run randomized experiment\n",
    "- Randomly assign discount levels (0%, 5%, 10%, 15%)\n",
    "- Ensure balance on observable characteristics\n",
    "\n",
    "**Step 2**: Estimate heterogeneous treatment effects\n",
    "- Use CATE estimators (not LTV prediction!)\n",
    "- Options: Causal Forest, X-learner, R-learner\n",
    "- Target: $\\tau(x) = E[Y(t) - Y(0) | X = x]$\n",
    "\n",
    "**Step 3**: Identify optimal treatment rule\n",
    "- For each customer, find $t^*(x) = \\arg\\max_t \\{\\hat{\\tau}(x, t) - \\text{cost}(t)\\}$\n",
    "- Consider business constraints (budget, margin)\n",
    "\n",
    "**Step 4**: Validate with holdout experiment\n",
    "- Deploy optimized vs uniform policy\n",
    "- Measure lift in profit, not just revenue\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: What's the relationship between this problem and the \"bad controls\" concept from causal inference?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Bad controls** = Variables that block or distort causal paths when conditioned on.\n",
    "\n",
    "**Here**: $M(T, X)$ is a function of:\n",
    "1. The treatment $T$\n",
    "2. The outcome $Y$ (learned relationship)\n",
    "\n",
    "Conditioning on $M(T, X)$ is like conditioning on a **collider descendant**.\n",
    "\n",
    "**In DAG terms**:\n",
    "```\n",
    "T → Y ← X\n",
    "    ↓\n",
    "   M(Y)\n",
    "```\n",
    "\n",
    "$M$ is a descendant of $Y$. Conditioning on $M$:\n",
    "- Opens the backdoor path $T \\to Y \\leftarrow X$ through $M$\n",
    "- Blocks the direct effect $T \\to Y$\n",
    "\n",
    "**Result**: Biased/attenuated treatment effect estimates.\n",
    "\n",
    "Same principle: Don't condition on post-treatment variables or their proxies.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Angrist, J. D., & Pischke, J. S. (2008). *Mostly Harmless Econometrics*. \n",
    "      Princeton University Press. Chapter 3: Bad Controls.\n",
    "\n",
    "[^2]: Pearl, J. (2009). *Causality: Models, Reasoning, and Inference*. \n",
    "      Cambridge University Press. Chapter 6: Causal Inference.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, \n",
    "      Appendix: When Prediction Fails.\n",
    "\n",
    "[^4]: Hernán, M. A., & Robins, J. M. (2020). *Causal Inference: What If*. \n",
    "      Chapman & Hall/CRC. Chapter 7: Confounding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
