{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.2 Regression Theory: Frisch-Waugh-Lovell and Partialling Out\n",
    "\n",
    "**Chapter**: 5 - The Unreasonable Effectiveness of Linear Regression  \n",
    "**Section**: 2 - Regression Theory  \n",
    "**Facure Source**: 05-The-Unreasonable-Effectiveness-of-Linear-Regression.ipynb (Regression Theory section)  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [The Best Linear Predictor](#11-the-best-linear-predictor)\n",
    "   - 1.2 [The Partialling Out Interpretation](#12-the-partialling-out-interpretation)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [Assumptions](#21-assumptions)\n",
    "   - 2.2 [The Frisch-Waugh-Lovell Theorem](#22-the-frisch-waugh-lovell-theorem)\n",
    "   - 2.3 [Proof of FWL](#23-proof-of-fwl)\n",
    "   - 2.4 [Implications for Causal Inference](#24-implications-for-causal-inference)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [Verifying FWL Numerically](#31-verifying-fwl-numerically)\n",
    "   - 3.2 [Debiasing via Partialling Out](#32-debiasing-via-partialling-out)\n",
    "4. [Implementation](#4-implementation)\n",
    "   - 4.1 [The Residual Maker Matrix](#41-the-residual-maker-matrix)\n",
    "   - 4.2 [Production Code](#42-production-code)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    ols_summary_table,\n",
    "    compare_coefficients,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: The FWL theorem is a *foundational* concept that appears in technical screens at quantitative firms. Understanding partialling out is essential for explaining Double Machine Learning, control variables, and debiasing strategies.\n",
    "\n",
    "### 1.1 The Best Linear Predictor\n",
    "\n",
    "As Facure explains, regression solves a theoretical best linear prediction problem. Given outcome $Y$ and covariates $X$, we seek parameters $\\beta^*$ that minimize mean squared error:\n",
    "\n",
    "$$\\beta^* = \\underset{\\beta}{\\text{argmin}} \\ E[(Y_i - X_i'\\beta)^2]$$\n",
    "\n",
    "The first-order condition yields the **normal equations**:\n",
    "\n",
    "$$\\beta^* = E[X_i X_i']^{-1} E[X_i Y_i]$$\n",
    "\n",
    "The sample analogue is the familiar OLS estimator:\n",
    "\n",
    "$$\\hat{\\beta} = (X'X)^{-1} X'Y$$\n",
    "\n",
    "### 1.2 The Partialling Out Interpretation\n",
    "\n",
    "Facure's key insight: with multiple regressors, the coefficient on treatment $T$ can be obtained via a two-step procedure:\n",
    "\n",
    "1. Regress $T$ on all other covariates $X$, get residuals $\\tilde{T}$\n",
    "2. The coefficient $\\kappa$ equals:\n",
    "\n",
    "$$\\kappa = \\frac{\\text{Cov}(Y_i, \\tilde{T}_i)}{\\text{Var}(\\tilde{T}_i)}$$\n",
    "\n",
    "This is profound: **the multivariate coefficient equals the bivariate coefficient after removing the linear influence of other variables**.\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- $\\tilde{T}$ is \"as good as random\" conditional on $X$\n",
    "- The residuals are orthogonal to all included covariates by construction\n",
    "- This is the foundation of debiased/double machine learning\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 Assumptions\n",
    "\n",
    "The Frisch-Waugh-Lovell theorem is a *purely algebraic* result. It requires:\n",
    "\n",
    "**A1 (Full Column Rank)**: The design matrix $X = [X_1 \\ X_2]$ has full column rank.\n",
    "\n",
    "This ensures $(X'X)^{-1}$ exists. No distributional assumptions are needed.\n",
    "\n",
    "### 2.2 The Frisch-Waugh-Lovell Theorem\n",
    "\n",
    "**Theorem (Frisch-Waugh-Lovell, 1933)**[^1]\n",
    "\n",
    "Consider the regression:\n",
    "$$Y = X_1 \\beta_1 + X_2 \\beta_2 + \\epsilon$$\n",
    "\n",
    "where $X_1$ is $n \\times k_1$ and $X_2$ is $n \\times k_2$. Then:\n",
    "\n",
    "$$\\hat{\\beta}_1 = (X_1' M_2 X_1)^{-1} X_1' M_2 Y$$\n",
    "\n",
    "where $M_2 = I_n - X_2(X_2'X_2)^{-1}X_2'$ is the **residual maker** (annihilator) matrix that projects onto the space orthogonal to the columns of $X_2$.\n",
    "\n",
    "**Equivalently**: $\\hat{\\beta}_1$ can be obtained by:\n",
    "1. Regress $Y$ on $X_2$, get residuals $\\tilde{Y} = M_2 Y$\n",
    "2. Regress each column of $X_1$ on $X_2$, get residuals $\\tilde{X}_1 = M_2 X_1$\n",
    "3. Regress $\\tilde{Y}$ on $\\tilde{X}_1$: the coefficient is $\\hat{\\beta}_1$\n",
    "\n",
    "### 2.3 Proof of FWL\n",
    "\n",
    "**Proof**: We provide two approaches—the direct algebraic proof and the geometric interpretation.\n",
    "\n",
    "**Part I: Algebraic Proof**\n",
    "\n",
    "Starting from the partitioned normal equations. With $X = [X_1 \\ X_2]$ and $\\beta = [\\beta_1' \\ \\beta_2']'$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\n",
    "\\begin{bmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{bmatrix}\n",
    "= \\begin{bmatrix} X_1'Y \\\\ X_2'Y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "From the second block equation:\n",
    "$$X_2'X_1 \\hat{\\beta}_1 + X_2'X_2 \\hat{\\beta}_2 = X_2'Y$$\n",
    "$$\\hat{\\beta}_2 = (X_2'X_2)^{-1}(X_2'Y - X_2'X_1 \\hat{\\beta}_1)$$\n",
    "\n",
    "Substituting into the first block equation:\n",
    "$$X_1'X_1 \\hat{\\beta}_1 + X_1'X_2 (X_2'X_2)^{-1}(X_2'Y - X_2'X_1 \\hat{\\beta}_1) = X_1'Y$$\n",
    "\n",
    "Rearranging (letting $P_2 = X_2(X_2'X_2)^{-1}X_2'$):\n",
    "$$X_1'X_1 \\hat{\\beta}_1 - X_1'P_2 X_1 \\hat{\\beta}_1 = X_1'Y - X_1'P_2 Y$$\n",
    "$$X_1'(I - P_2)X_1 \\hat{\\beta}_1 = X_1'(I - P_2)Y$$\n",
    "$$X_1' M_2 X_1 \\hat{\\beta}_1 = X_1' M_2 Y$$\n",
    "\n",
    "Therefore:\n",
    "$$\\boxed{\\hat{\\beta}_1 = (X_1' M_2 X_1)^{-1} X_1' M_2 Y}$$\n",
    "\n",
    "Since $M_2$ is symmetric and idempotent ($M_2 = M_2' = M_2^2$):\n",
    "$$\\hat{\\beta}_1 = ((M_2 X_1)' M_2 X_1)^{-1} (M_2 X_1)' M_2 Y = (\\tilde{X}_1' \\tilde{X}_1)^{-1} \\tilde{X}_1' \\tilde{Y}$$\n",
    "\n",
    "This is exactly OLS of $\\tilde{Y}$ on $\\tilde{X}_1$. $\\blacksquare$\n",
    "\n",
    "**Part II: Geometric Interpretation**\n",
    "\n",
    "Let $\\mathcal{C}(X_2)$ denote the column space of $X_2$.\n",
    "\n",
    "- $P_2 = X_2(X_2'X_2)^{-1}X_2'$ projects onto $\\mathcal{C}(X_2)$\n",
    "- $M_2 = I - P_2$ projects onto $\\mathcal{C}(X_2)^\\perp$\n",
    "\n",
    "The FWL theorem says: to find $\\hat{\\beta}_1$, we can work entirely in the orthogonal complement of $\\mathcal{C}(X_2)$. This removes the \"influence\" of $X_2$ from both $Y$ and $X_1$.\n",
    "\n",
    "### 2.4 Implications for Causal Inference\n",
    "\n",
    "**Proposition (Conditional Exogeneity)**: If treatment $T$ satisfies $(Y_0, Y_1) \\perp T | X$, then partialling out $X$ from $T$ creates variation that is \"as good as random\":\n",
    "\n",
    "$$\\tilde{T} = T - E[T|X]$$\n",
    "\n",
    "is uncorrelated with any function of $X$, by construction.\n",
    "\n",
    "This is the foundation of:\n",
    "1. **Control variable adjustment**: Adding confounders to remove omitted variable bias\n",
    "2. **Double Machine Learning**: Using flexible ML to estimate $E[T|X]$ and $E[Y|X]$\n",
    "3. **Neyman orthogonality**: The influence function becomes insensitive to first-stage estimation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 Verifying FWL Numerically\n",
    "\n",
    "We replicate Facure's wage example and verify FWL to machine precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Facure's wage data\n",
    "wage = load_facure_data('wage.csv').dropna()\n",
    "wage['lhwage'] = np.log(wage['wage'] / wage['hours'])\n",
    "\n",
    "# Define variables\n",
    "controls = ['IQ', 'exper', 'tenure', 'age', 'married', 'black',\n",
    "            'south', 'urban', 'sibs', 'brthord', 'meduc', 'feduc']\n",
    "\n",
    "y = wage['lhwage'].values\n",
    "T = wage['educ'].values\n",
    "X = wage[controls].values\n",
    "\n",
    "# Add constant to controls\n",
    "X_const = sm.add_constant(X)\n",
    "\n",
    "print(f\"Sample size: n = {len(y)}\")\n",
    "print(f\"Treatment: education (years)\")\n",
    "print(f\"Outcome: log hourly wage\")\n",
    "print(f\"Controls: {len(controls)} variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Full multivariate regression\n",
    "X_full = np.column_stack([T, X_const])\n",
    "beta_full = np.linalg.lstsq(X_full, y, rcond=None)[0]\n",
    "kappa_full = beta_full[0]\n",
    "\n",
    "print(\"Method 1: Full Regression\")\n",
    "print(f\"  β_educ = {kappa_full:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: FWL - Partialling out via residuals\n",
    "# Step 1: Residualize T on X\n",
    "beta_T = np.linalg.lstsq(X_const, T, rcond=None)[0]\n",
    "T_tilde = T - X_const @ beta_T\n",
    "\n",
    "# Step 2: Residualize Y on X\n",
    "beta_Y = np.linalg.lstsq(X_const, y, rcond=None)[0]\n",
    "Y_tilde = y - X_const @ beta_Y\n",
    "\n",
    "# Step 3: Bivariate regression of Y_tilde on T_tilde\n",
    "kappa_fwl = np.cov(Y_tilde, T_tilde)[0, 1] / np.var(T_tilde, ddof=1)\n",
    "\n",
    "print(\"Method 2: FWL (Partialling Out)\")\n",
    "print(f\"  β_educ = {kappa_fwl:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: FWL using residual maker matrix directly\n",
    "n = len(y)\n",
    "P_X = X_const @ np.linalg.inv(X_const.T @ X_const) @ X_const.T\n",
    "M_X = np.eye(n) - P_X\n",
    "\n",
    "# Apply FWL formula\n",
    "T_mat = T.reshape(-1, 1)\n",
    "kappa_matrix = np.linalg.inv(T_mat.T @ M_X @ T_mat) @ (T_mat.T @ M_X @ y)\n",
    "kappa_matrix = kappa_matrix[0]\n",
    "\n",
    "print(\"Method 3: Residual Maker Matrix\")\n",
    "print(f\"  β_educ = {kappa_matrix:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all methods match to machine precision\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICATION: All methods match?\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "tol = 1e-10\n",
    "match_1_2 = np.isclose(kappa_full, kappa_fwl, rtol=tol)\n",
    "match_1_3 = np.isclose(kappa_full, kappa_matrix, rtol=tol)\n",
    "\n",
    "print(f\"Full vs FWL:    |Δ| = {abs(kappa_full - kappa_fwl):.2e} → {'✓ MATCH' if match_1_2 else '✗ FAIL'}\")\n",
    "print(f\"Full vs Matrix: |Δ| = {abs(kappa_full - kappa_matrix):.2e} → {'✓ MATCH' if match_1_3 else '✗ FAIL'}\")\n",
    "\n",
    "assert match_1_2 and match_1_3, \"FWL verification failed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Debiasing via Partialling Out\n",
    "\n",
    "Demonstrate how partialling out removes confounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate regression (omitting controls) - BIASED\n",
    "T_const = sm.add_constant(T)\n",
    "beta_biased = np.linalg.lstsq(T_const, y, rcond=None)[0]\n",
    "kappa_biased = beta_biased[1]\n",
    "\n",
    "print(\"Omitted Variable Bias Demonstration\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Biased estimate (no controls):  {kappa_biased:.4f}\")\n",
    "print(f\"Debiased estimate (FWL):        {kappa_fwl:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Bias = {kappa_biased - kappa_fwl:.4f}\")\n",
    "print(f\"Bias direction: {'Positive (overestimate)' if kappa_biased > kappa_fwl else 'Negative'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the partialling out process\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Panel 1: Raw relationship (confounded)\n",
    "ax = axes[0]\n",
    "ax.scatter(T, y, alpha=0.3, s=20, c=TUFTE_PALETTE['primary'], label='Raw data')\n",
    "T_grid = np.linspace(T.min(), T.max(), 100)\n",
    "ax.plot(T_grid, beta_biased[0] + beta_biased[1] * T_grid, \n",
    "        c=TUFTE_PALETTE['bias'], lw=2, label=f'β = {kappa_biased:.3f} (biased)')\n",
    "ax.set_xlabel('Education (years)')\n",
    "ax.set_ylabel('Log Hourly Wage')\n",
    "ax.set_title('(a) Raw: Confounded')\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "# Panel 2: Residualized relationship\n",
    "ax = axes[1]\n",
    "ax.scatter(T_tilde, Y_tilde, alpha=0.3, s=20, c=TUFTE_PALETTE['primary'], label='Residuals')\n",
    "T_tilde_grid = np.linspace(T_tilde.min(), T_tilde.max(), 100)\n",
    "ax.plot(T_tilde_grid, kappa_fwl * T_tilde_grid, \n",
    "        c=TUFTE_PALETTE['effect'], lw=2, label=f'β = {kappa_fwl:.3f} (debiased)')\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5)\n",
    "ax.axvline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5)\n",
    "ax.set_xlabel('Education Residual (T̃)')\n",
    "ax.set_ylabel('Wage Residual (Ỹ)')\n",
    "ax.set_title('(b) After Partialling Out: Debiased')\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "# Panel 3: Coefficient comparison\n",
    "ax = axes[2]\n",
    "methods = ['Biased\\n(no controls)', 'Debiased\\n(FWL)']\n",
    "coefs = [kappa_biased, kappa_fwl]\n",
    "colors = [TUFTE_PALETTE['bias'], TUFTE_PALETTE['effect']]\n",
    "bars = ax.bar(methods, coefs, color=colors, width=0.6, edgecolor='white')\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], lw=0.5)\n",
    "\n",
    "# Direct labels\n",
    "for bar, coef in zip(bars, coefs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, coef + 0.002, \n",
    "            f'{coef:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_ylabel('Coefficient on Education')\n",
    "ax.set_title('(c) Bias Reduction')\n",
    "ax.set_ylim(0, 0.07)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "### 4.1 The Residual Maker Matrix\n",
    "\n",
    "The residual maker (annihilator) matrix is fundamental to understanding regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_maker(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the residual maker (annihilator) matrix M = I - P.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Design matrix (n x k).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    M : np.ndarray\n",
    "        Residual maker matrix (n x n) that projects onto C(X)^⊥.\n",
    "        \n",
    "    Properties\n",
    "    ----------\n",
    "    - Symmetric: M = M'\n",
    "    - Idempotent: M² = M\n",
    "    - MX = 0 (annihilates columns of X)\n",
    "    - trace(M) = n - k\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> X = np.array([[1, 2], [1, 3], [1, 5]])\n",
    "    >>> M = residual_maker(X)\n",
    "    >>> np.allclose(M @ X, 0)  # Annihilates X\n",
    "    True\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    P = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "    M = np.eye(n) - P\n",
    "    return M\n",
    "\n",
    "\n",
    "def fwl_coefficient(y: np.ndarray, T: np.ndarray, X: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute treatment coefficient via Frisch-Waugh-Lovell.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Outcome vector (n,).\n",
    "    T : np.ndarray\n",
    "        Treatment vector (n,).\n",
    "    X : np.ndarray\n",
    "        Control matrix (n x k), should include constant.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    kappa : float\n",
    "        Treatment coefficient after partialling out X.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Implements: κ = (T'M_X T)^{-1} T'M_X y\n",
    "    where M_X = I - X(X'X)^{-1}X'\n",
    "    \"\"\"\n",
    "    M = residual_maker(X)\n",
    "    T_tilde = M @ T\n",
    "    Y_tilde = M @ y\n",
    "    \n",
    "    # Bivariate formula on residuals\n",
    "    kappa = (T_tilde @ Y_tilde) / (T_tilde @ T_tilde)\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify properties of residual maker\n",
    "M = residual_maker(X_const)\n",
    "\n",
    "print(\"Residual Maker Properties\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Symmetric (M = M'): {np.allclose(M, M.T)}\")\n",
    "print(f\"Idempotent (M² = M): {np.allclose(M @ M, M)}\")\n",
    "print(f\"Annihilates X (MX ≈ 0): {np.allclose(M @ X_const, 0)}\")\n",
    "print(f\"trace(M) = n - k: {np.trace(M):.1f} vs {n - X_const.shape[1]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Production Code\n",
    "\n",
    "Reference implementation from `causal_inference_mastery`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to production DML implementation\n",
    "# from causal_inference.cate.dml import DoubleMachineLearning\n",
    "\n",
    "# The DML estimator uses FWL/partialling out as its foundation:\n",
    "# 1. Estimate E[Y|X] with ML model → get Y residuals\n",
    "# 2. Estimate E[T|X] with ML model → get T residuals  \n",
    "# 3. Regress Y residuals on T residuals → get θ\n",
    "\n",
    "# This is exactly FWL with flexible first-stage estimation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Google L5, DS)**: *\"Explain the Frisch-Waugh-Lovell theorem and why it matters for causal inference.\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points to hit:**\n",
    "\n",
    "1. **Statement**: In regression $Y = X_1\\beta_1 + X_2\\beta_2 + \\epsilon$, the coefficient $\\hat{\\beta}_1$ equals the coefficient from regressing the residuals of $Y$ on $X_2$ against the residuals of $X_1$ on $X_2$.\n",
    "\n",
    "2. **Intuition**: \"Partialling out\" removes the linear influence of controls. What remains in $\\tilde{T} = T - E[T|X]$ is variation that cannot be predicted by $X$.\n",
    "\n",
    "3. **Causal relevance**: \n",
    "   - If confounders $X$ satisfy conditional ignorability, then $\\tilde{T}$ is \"as good as random\"\n",
    "   - The coefficient captures causal effect after adjustment\n",
    "   - Foundation for Double Machine Learning: use ML to flexibly estimate $E[T|X]$ and $E[Y|X]$\n",
    "\n",
    "4. **Mathematical insight**: The residual maker $M = I - X(X'X)^{-1}X'$ projects onto the orthogonal complement of $\\mathcal{C}(X)$. FWL says we can work entirely in this subspace.\n",
    "\n",
    "**Common follow-up**: \"What happens if we omit a confounder?\"\n",
    "→ Omitted variable bias formula: $\\hat{\\beta}_{short} = \\beta_{long} + \\gamma \\delta$, where $\\gamma$ is effect of omitted on outcome, $\\delta$ is regression of omitted on included.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Meta IC5, Economist)**: *\"A colleague says 'just throw all variables into the regression to control for confounding.' What's wrong with this advice?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Problems with \"kitchen sink\" regression:**\n",
    "\n",
    "1. **Bad controls**: Including post-treatment variables (mediators, colliders) can *introduce* bias\n",
    "   - Collider bias: conditioning on a common effect opens a backdoor path\n",
    "   - Mediator bias: partialling out the mechanism removes part of the effect\n",
    "\n",
    "2. **Multicollinearity**: Adding highly correlated variables inflates standard errors without reducing bias\n",
    "\n",
    "3. **Overfitting in finite samples**: More controls means more parameters, potentially worse MSE\n",
    "\n",
    "4. **Not all confounders are observed**: Even with \"all\" measured variables, unobserved confounders may remain\n",
    "\n",
    "**Better approach:**\n",
    "- Draw a DAG to identify which variables are confounders vs. bad controls\n",
    "- Use the backdoor criterion: condition on variables that block all backdoor paths\n",
    "- Only include pre-treatment confounders\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Two Sigma, Quant)**: *\"Derive the omitted variable bias formula.\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Setup**: True model is $Y = T\\kappa + A\\gamma + \\epsilon$, but we estimate $Y = T\\hat{\\kappa}_{short} + e$.\n",
    "\n",
    "**Derivation**:\n",
    "\n",
    "The short regression coefficient:\n",
    "$$\\hat{\\kappa}_{short} = \\frac{\\text{Cov}(Y, T)}{\\text{Var}(T)}$$\n",
    "\n",
    "Substitute $Y = T\\kappa + A\\gamma + \\epsilon$:\n",
    "$$= \\frac{\\text{Cov}(T\\kappa + A\\gamma + \\epsilon, T)}{\\text{Var}(T)}$$\n",
    "$$= \\kappa + \\gamma \\cdot \\frac{\\text{Cov}(A, T)}{\\text{Var}(T)}$$\n",
    "$$= \\kappa + \\gamma \\cdot \\delta$$\n",
    "\n",
    "where $\\delta = \\frac{\\text{Cov}(A,T)}{\\text{Var}(T)}$ is the coefficient from regressing $A$ on $T$.\n",
    "\n",
    "**Bias** = $\\gamma \\delta$ = (effect of omitted on Y) × (regression of omitted on included)\n",
    "\n",
    "**When is bias zero?**\n",
    "- $\\gamma = 0$: omitted doesn't affect outcome (not a confounder)\n",
    "- $\\delta = 0$: omitted uncorrelated with treatment (randomization!)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Frisch, R. and Waugh, F. V. (1933). Partial Time Regressions as Compared with Individual Trends. *Econometrica*, 1(4), 387-401.\n",
    "\n",
    "[^2]: Lovell, M. C. (1963). Seasonal Adjustment of Economic Time Series and Multiple Regression Analysis. *Journal of the American Statistical Association*, 58(304), 993-1010.\n",
    "\n",
    "[^3]: Angrist, J. D. and Pischke, J.-S. (2009). *Mostly Harmless Econometrics*. Princeton University Press, Chapter 3.\n",
    "\n",
    "[^4]: Chernozhukov, V. et al. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68. [research_kb: `2bc757a2-8fc9-4622-a733-a18814df7a0b`]\n",
    "\n",
    "[^5]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 5.\n",
    "\n",
    "---\n",
    "\n",
    "**Precision Improvement:**\n",
    "- You said: \"Make a notebook about regression theory\"\n",
    "- Concise: \"Build FWL notebook with proof and numerics\"\n",
    "- Precise: `/augmented 05.2 regression_theory --fwl-proof --numeric-demo`\n",
    "- Pattern: [build] [target] [content-flags]"
   ]
  }
 ],
 "metadata": {
  "augmented": {
   "chapter_number": 5,
   "section_number": 2,
   "title": "Regression Theory: Frisch-Waugh-Lovell and Partialling Out",
   "facure_source": "05-The-Unreasonable-Effectiveness-of-Linear-Regression.ipynb",
   "section_types": [
    "table_of_contents",
    "facure_intuition",
    "formal_treatment",
    "numeric_demonstration",
    "implementation",
    "interview_appendix",
    "references"
   ],
   "version": "1.0.0",
   "last_validated": "2026-01-09"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
