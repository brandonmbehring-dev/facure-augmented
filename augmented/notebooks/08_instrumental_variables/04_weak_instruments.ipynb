{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 08.4 Weak Instruments\n",
    "\n",
    "**Chapter**: 8 - Instrumental Variables  \n",
    "**Section**: 4 - Weakness of Instruments  \n",
    "**Facure Source**: 08-Instrumental-Variables.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [Why Weak Instruments Matter](#11-why-weak-instruments-matter)\n",
    "   - 1.2 [Bias Toward OLS](#12-bias-toward-ols)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [Finite-Sample Bias of 2SLS](#21-finite-sample-bias-of-2sls)\n",
    "   - 2.2 [Stock-Yogo Critical Values](#22-stock-yogo-critical-values)\n",
    "   - 2.3 [LIML as Alternative](#23-liml-as-alternative)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [Simulation: Varying Instrument Strength](#31-simulation-varying-instrument-strength)\n",
    "   - 3.2 [Variance Explosion with Weak Instruments](#32-variance-explosion-with-weak-instruments)\n",
    "   - 3.3 [2SLS vs LIML Comparison](#33-2sls-vs-liml-comparison)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    ols_summary_table,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from linearmodels.iv import IV2SLS, IVLIML\n",
    "from scipy import stats\n",
    "\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: Weak instrument detection is a critical skill. The \"F > 10\" rule of thumb is ubiquitous, but understanding *why* and *when* it fails separates experts from practitioners.\n",
    "\n",
    "### 1.1 Why Weak Instruments Matter\n",
    "\n",
    "As Facure explains: *\"When dealing with IV, we need to remember we are estimating the ATE indirectly. Our estimates depend on both the first stage and the second stage. If the impact of the treatment on the outcome is indeed strong, the second stage will also be strong. However, it doesn't matter how strong the second stage is if we have a weak first stage.\"*\n",
    "\n",
    "A **weak first stage** means the instrument has only a very small correlation with the treatment. Therefore, we can't learn much about the treatment from the instrument.\n",
    "\n",
    "### 1.2 Bias Toward OLS\n",
    "\n",
    "**Key insight**: 2SLS is **biased toward OLS** in finite samples!\n",
    "\n",
    "$$\\text{2SLS bias} \\approx \\frac{\\text{OLS bias}}{1 + F}$$\n",
    "\n",
    "where $F$ is the first-stage F-statistic.\n",
    "\n",
    "**Implications:**\n",
    "- If OLS has positive bias, 2SLS will also have positive bias\n",
    "- If OLS has negative bias, 2SLS will also have negative bias\n",
    "- As $F \\to \\infty$, 2SLS bias → 0 (consistency)\n",
    "- As $F \\to 0$, 2SLS bias → OLS bias (worthless!)\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- 2SLS is **consistent** but **biased** in finite samples\n",
    "- Bias increases with more instruments (more paths to OLS)\n",
    "- Just-identified IV (one instrument) is **median-unbiased**\n",
    "- Rule of thumb: F > 10 for \"safe\" zone (Stock, Wright, Yogo 2002)\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 Finite-Sample Bias of 2SLS\n",
    "\n",
    "Consider the structural model:\n",
    "\n",
    "$$y_i = \\rho s_i + u_i \\quad \\text{with } \\text{Cov}(s_i, u_i) = \\sigma_{su}$$\n",
    "\n",
    "and first stage:\n",
    "\n",
    "$$s_i = \\pi' z_i + \\xi_i$$\n",
    "\n",
    "**Approximate bias formula** (Bound, Jaeger, Baker 1995):\n",
    "\n",
    "$$E[\\hat{\\rho}_{2SLS}] - \\rho \\approx \\frac{\\sigma_{u\\xi}/\\sigma^2_{\\xi}}{\\mu^2/\\sigma^2_{\\pi}} \\cdot (K/n)$$\n",
    "\n",
    "where $K$ = number of instruments, $n$ = sample size, $\\mu^2/\\sigma^2_{\\pi}$ is the concentration parameter.\n",
    "\n",
    "**Simplified form**: The relative bias of 2SLS to OLS is approximately:\n",
    "\n",
    "$$\\frac{\\text{Bias}_{2SLS}}{\\text{Bias}_{OLS}} \\approx \\frac{K}{F}$$\n",
    "\n",
    "where $F$ is the first-stage F-statistic on excluded instruments.\n",
    "\n",
    "### 2.2 Stock-Yogo Critical Values\n",
    "\n",
    "Stock and Yogo (2005) provide critical values for the F-statistic based on:\n",
    "\n",
    "| Acceptable Relative Bias | Required F (one instrument) |\n",
    "|--------------------------|----------------------------|\n",
    "| 10% of OLS bias | F > 16.38 |\n",
    "| 15% of OLS bias | F > 8.96 |\n",
    "| 20% of OLS bias | F > 6.66 |\n",
    "| 25% of OLS bias | F > 5.53 |\n",
    "\n",
    "The common \"F > 10\" rule is a convenient approximation for 10-15% relative bias.\n",
    "\n",
    "### 2.3 LIML as Alternative\n",
    "\n",
    "**Limited Information Maximum Likelihood (LIML)** is an alternative to 2SLS that is:\n",
    "\n",
    "1. **Approximately median-unbiased** for over-identified models\n",
    "2. **Same asymptotic distribution** as 2SLS (so equally efficient asymptotically)\n",
    "3. **Less biased** than 2SLS with weak instruments\n",
    "4. **More variable** (higher variance) than 2SLS\n",
    "\n",
    "The LIML estimator minimizes:\n",
    "\n",
    "$$\\hat{\\rho}_{LIML} = \\arg\\min_{\\rho} \\frac{(y - s\\rho)'M_X(y - s\\rho)}{(y - s\\rho)'M_Z(y - s\\rho)}$$\n",
    "\n",
    "where $M_X$ and $M_Z$ are residual-maker matrices.\n",
    "\n",
    "**Practical advice** (Angrist & Pischke):\n",
    "1. Report first-stage F-statistic (bigger is better)\n",
    "2. Compare 2SLS with LIML — if similar, be happy; if different, worry\n",
    "3. Use just-identified IV with strongest single instrument\n",
    "4. Look at reduced form: if you can't see it there, it's probably not there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 Simulation: Varying Instrument Strength\n",
    "\n",
    "This simulation replicates Facure's demonstration of how IV estimates deteriorate with weaker instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facure's simulation setup\n",
    "np.random.seed(42)\n",
    "n = 10000\n",
    "\n",
    "# Generate base data\n",
    "X = np.random.normal(0, 2, n)  # Observable control\n",
    "U = np.random.normal(0, 2, n)  # Unobservable confounder\n",
    "T = np.random.normal(1 + 0.5 * U, 5, n)  # Endogenous treatment\n",
    "Y = np.random.normal(2 + X - 0.5 * U + 2 * T, 5, n)  # Outcome (TRUE EFFECT = 2.0)\n",
    "\n",
    "TRUE_EFFECT = 2.0\n",
    "\n",
    "# Generate instruments with varying strength\n",
    "# Higher std deviation = weaker correlation with T\n",
    "stddevs = np.linspace(0.1, 100, 50)\n",
    "instruments = {f\"Z_{i}\": np.random.normal(T, s, n) for i, s in enumerate(stddevs)}\n",
    "\n",
    "sim_data = pd.DataFrame({'X': X, 'U': U, 'T': T, 'Y': Y}).assign(**instruments)\n",
    "\n",
    "print(f\"Sample size: n = {n:,}\")\n",
    "print(f\"True causal effect: {TRUE_EFFECT}\")\n",
    "print(f\"Number of instrument variants: {len(instruments)}\")\n",
    "\n",
    "# Check correlation structure\n",
    "correlations = sim_data[[f'Z_{i}' for i in range(5)]].corrwith(sim_data['T'])\n",
    "print(f\"\\nCorrelation with T (first 5 instruments):\")\n",
    "print(correlations.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run IV with each instrument and collect results\n",
    "results = []\n",
    "\n",
    "for i, s in enumerate(stddevs):\n",
    "    z_col = f'Z_{i}'\n",
    "    \n",
    "    # Compute correlation\n",
    "    corr_tz = sim_data['T'].corr(sim_data[z_col])\n",
    "    \n",
    "    # Run first stage to get F-statistic\n",
    "    first_stage = smf.ols(f'T ~ X + {z_col}', data=sim_data).fit()\n",
    "    f_stat = first_stage.fvalue  # Joint F, but we want F on excluded instrument\n",
    "    \n",
    "    # Approximate F on excluded instrument\n",
    "    t_stat_z = first_stage.tvalues[z_col]\n",
    "    f_on_z = t_stat_z ** 2\n",
    "    \n",
    "    # Run 2SLS\n",
    "    formula = f'Y ~ 1 + X + [T ~ {z_col}]'\n",
    "    try:\n",
    "        iv_result = IV2SLS.from_formula(formula, sim_data).fit()\n",
    "        ate = iv_result.params['T']\n",
    "        se = iv_result.std_errors['T']\n",
    "    except Exception:\n",
    "        ate = np.nan\n",
    "        se = np.nan\n",
    "    \n",
    "    results.append({\n",
    "        'z_std': s,\n",
    "        'corr': corr_tz,\n",
    "        'f_stat': f_on_z,\n",
    "        'ate': ate,\n",
    "        'se': se,\n",
    "        'bias': ate - TRUE_EFFECT\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: SE by instrument strength\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: SE by correlation\n",
    "ax = axes[0]\n",
    "ax.scatter(results_df['corr'], results_df['se'], \n",
    "           c=TUFTE_PALETTE['treatment'], alpha=0.7, s=30)\n",
    "ax.set_xlabel('Corr(Z, T)')\n",
    "ax.set_ylabel('IV Standard Error')\n",
    "ax.set_title('(a) Variance Explosion with Weak Instruments')\n",
    "ax.axhline(0.1, color=TUFTE_PALETTE['control'], linestyle='--', \n",
    "           label='SE with strong IV', alpha=0.5)\n",
    "\n",
    "# Panel 2: ATE estimate with CI by correlation\n",
    "ax = axes[1]\n",
    "ax.scatter(results_df['corr'], results_df['ate'], \n",
    "           c=TUFTE_PALETTE['treatment'], alpha=0.7, s=30)\n",
    "ax.fill_between(results_df.sort_values('corr')['corr'],\n",
    "                results_df.sort_values('corr')['ate'] - 1.96 * results_df.sort_values('corr')['se'],\n",
    "                results_df.sort_values('corr')['ate'] + 1.96 * results_df.sort_values('corr')['se'],\n",
    "                alpha=0.2, color=TUFTE_PALETTE['treatment'])\n",
    "ax.axhline(TRUE_EFFECT, color='black', linestyle='--', linewidth=2, label='True effect')\n",
    "ax.set_xlabel('Corr(Z, T)')\n",
    "ax.set_ylabel('IV Estimate')\n",
    "ax.set_title('(b) Estimates Become Useless with Weak Instruments')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### 3.2 Variance Explosion with Weak Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize by F-statistic (more intuitive)\nfig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n\n# Filter to reasonable range (exclude extreme values)\nplot_df = results_df[(results_df['f_stat'] < 500) & (results_df['se'] < 100)].copy()\n\n# Panel 1: SE by F-statistic\nax = axes[0]\nax.scatter(plot_df['f_stat'], plot_df['se'], \n           c=TUFTE_PALETTE['treatment'], alpha=0.7, s=30)\nax.axvline(10, color='red', linestyle='--', linewidth=2, label='F = 10 threshold')\nax.set_xlabel('First-Stage F-statistic')\nax.set_ylabel('IV Standard Error')\nax.set_title('(a) SE by Instrument Strength')\nax.legend(frameon=False)\nax.set_xlim(0, 100)\n\n# Panel 2: Bias by F-statistic\nax = axes[1]\nax.scatter(plot_df['f_stat'], plot_df['bias'], \n           c=TUFTE_PALETTE['treatment'], alpha=0.7, s=30)\nax.axhline(0, color='black', linestyle='-', linewidth=1)\nax.axvline(10, color='red', linestyle='--', linewidth=2, label='F = 10 threshold')\nax.set_xlabel('First-Stage F-statistic')\nax.set_ylabel('Bias (Estimate - True)')\nax.set_title('(b) Bias by Instrument Strength')\nax.legend(frameon=False)\nax.set_xlim(0, 100)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics (filter out extreme values)\nstrong = results_df[results_df['f_stat'] > 10]\nweak = results_df[(results_df['f_stat'] <= 10) & (results_df['se'] < 100)]\n\nprint(\"\\n★ Summary:\")\nprint(f\"  Strong instruments (F > 10): {len(strong)} cases\")\nif len(strong) > 0:\n    print(f\"    - Mean SE: {strong['se'].mean():.4f}\")\n    print(f\"    - Mean absolute bias: {strong['bias'].abs().mean():.4f}\")\nprint(f\"  Weak instruments (F ≤ 10): {len(weak)} cases (extreme values filtered)\")\nif len(weak) > 0:\n    print(f\"    - Mean SE: {weak['se'].mean():.4f}\")\n    print(f\"    - Mean absolute bias: {weak['bias'].abs().mean():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 3.3 2SLS vs LIML Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 2SLS vs LIML with weak instruments\n",
    "np.random.seed(123)\n",
    "n = 5000\n",
    "\n",
    "# Generate data with moderately weak instrument\n",
    "U = np.random.normal(0, 1, n)\n",
    "Z = np.random.binomial(1, 0.5, n)\n",
    "T = 0.5 + 0.15 * Z + 0.8 * U + np.random.normal(0, 1, n)  # Weak first stage (coef = 0.15)\n",
    "Y = 1.0 + TRUE_EFFECT * T + 1.2 * U + np.random.normal(0, 1, n)\n",
    "\n",
    "weak_data = pd.DataFrame({'Y': Y, 'T': T, 'Z': Z, 'U': U})\n",
    "\n",
    "# First stage F-statistic\n",
    "first_stage = smf.ols('T ~ Z', data=weak_data).fit()\n",
    "f_stat = first_stage.tvalues['Z'] ** 2\n",
    "\n",
    "print(f\"WEAK INSTRUMENT SCENARIO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True effect: {TRUE_EFFECT}\")\n",
    "print(f\"First-stage coefficient: {first_stage.params['Z']:.4f}\")\n",
    "print(f\"First-stage F-statistic: {f_stat:.2f}\")\n",
    "print(f\"Weak instrument? {f_stat < 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS (biased)\n",
    "ols_result = smf.ols('Y ~ T', data=weak_data).fit()\n",
    "\n",
    "# 2SLS\n",
    "iv2sls_result = IV2SLS.from_formula('Y ~ 1 + [T ~ Z]', weak_data).fit()\n",
    "\n",
    "# LIML\n",
    "ivliml_result = IVLIML.from_formula('Y ~ 1 + [T ~ Z]', weak_data).fit()\n",
    "\n",
    "# Oracle (with U)\n",
    "oracle_result = smf.ols('Y ~ T + U', data=weak_data).fit()\n",
    "\n",
    "print(\"\\nCOMPARISON: OLS vs 2SLS vs LIML\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Method':<15} {'Estimate':>12} {'SE':>12} {'Bias':>12} {'RMSE':>12}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'True':<15} {TRUE_EFFECT:>12.4f} {'-':>12} {0:>12.4f} {'-':>12}\")\n",
    "print(f\"{'OLS':<15} {ols_result.params['T']:>12.4f} {ols_result.bse['T']:>12.4f} \"\n",
    "      f\"{ols_result.params['T'] - TRUE_EFFECT:>12.4f} {'-':>12}\")\n",
    "print(f\"{'2SLS':<15} {iv2sls_result.params['T']:>12.4f} {iv2sls_result.std_errors['T']:>12.4f} \"\n",
    "      f\"{iv2sls_result.params['T'] - TRUE_EFFECT:>12.4f} {'-':>12}\")\n",
    "print(f\"{'LIML':<15} {ivliml_result.params['T']:>12.4f} {ivliml_result.std_errors['T']:>12.4f} \"\n",
    "      f\"{ivliml_result.params['T'] - TRUE_EFFECT:>12.4f} {'-':>12}\")\n",
    "print(f\"{'Oracle':<15} {oracle_result.params['T']:>12.4f} {oracle_result.bse['T']:>12.4f} \"\n",
    "      f\"{oracle_result.params['T'] - TRUE_EFFECT:>12.4f} {'-':>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo: Compare 2SLS vs LIML bias distribution\n",
    "n_sims = 500\n",
    "n_obs = 1000\n",
    "\n",
    "results_2sls = []\n",
    "results_liml = []\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(n_sims):\n",
    "    # Generate data\n",
    "    U = np.random.normal(0, 1, n_obs)\n",
    "    Z = np.random.binomial(1, 0.5, n_obs)\n",
    "    T = 0.5 + 0.15 * Z + 0.8 * U + np.random.normal(0, 1, n_obs)\n",
    "    Y = 1.0 + TRUE_EFFECT * T + 1.2 * U + np.random.normal(0, 1, n_obs)\n",
    "    \n",
    "    mc_data = pd.DataFrame({'Y': Y, 'T': T, 'Z': Z})\n",
    "    \n",
    "    try:\n",
    "        # 2SLS\n",
    "        iv2sls = IV2SLS.from_formula('Y ~ 1 + [T ~ Z]', mc_data).fit()\n",
    "        results_2sls.append(iv2sls.params['T'])\n",
    "        \n",
    "        # LIML\n",
    "        ivliml = IVLIML.from_formula('Y ~ 1 + [T ~ Z]', mc_data).fit()\n",
    "        results_liml.append(ivliml.params['T'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "results_2sls = np.array(results_2sls)\n",
    "results_liml = np.array(results_liml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Monte Carlo results\n",
    "fig, ax = create_tufte_figure(figsize=(10, 5))\n",
    "\n",
    "# Histogram of estimates\n",
    "bins = np.linspace(-5, 10, 50)\n",
    "ax.hist(results_2sls, bins=bins, alpha=0.5, label='2SLS', \n",
    "        color=TUFTE_PALETTE['control'], density=True)\n",
    "ax.hist(results_liml, bins=bins, alpha=0.5, label='LIML', \n",
    "        color=TUFTE_PALETTE['treatment'], density=True)\n",
    "ax.axvline(TRUE_EFFECT, color='black', linestyle='--', linewidth=2, label='True effect')\n",
    "ax.axvline(np.mean(results_2sls), color=TUFTE_PALETTE['control'], linestyle='-', linewidth=2)\n",
    "ax.axvline(np.median(results_liml), color=TUFTE_PALETTE['treatment'], linestyle='-', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Estimated Effect')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Monte Carlo: 2SLS vs LIML with Weak Instrument')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nMONTE CARLO RESULTS (500 simulations)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True effect: {TRUE_EFFECT}\")\n",
    "print(f\"\")\n",
    "print(f\"2SLS:\")\n",
    "print(f\"  Mean:   {np.mean(results_2sls):.4f} (bias: {np.mean(results_2sls) - TRUE_EFFECT:.4f})\")\n",
    "print(f\"  Median: {np.median(results_2sls):.4f}\")\n",
    "print(f\"  SD:     {np.std(results_2sls):.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"LIML:\")\n",
    "print(f\"  Mean:   {np.mean(results_liml):.4f} (bias: {np.mean(results_liml) - TRUE_EFFECT:.4f})\")\n",
    "print(f\"  Median: {np.median(results_liml):.4f}\")\n",
    "print(f\"  SD:     {np.std(results_liml):.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"★ LIML is approximately median-unbiased ({np.median(results_liml):.2f} vs true {TRUE_EFFECT})\")\n",
    "print(f\"★ But LIML has higher variance ({np.std(results_liml):.2f} vs {np.std(results_2sls):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "**Key Findings:**\n",
    "\n",
    "1. **2SLS is biased** toward OLS even with consistent instruments\n",
    "2. **LIML is approximately median-unbiased** — the median is close to the true value\n",
    "3. **LIML has higher variance** — the distribution is more spread out\n",
    "4. **Trade-off**: Less bias (LIML) vs. lower variance (2SLS)\n",
    "5. **Practical advice**: Compare both; if they agree, be confident; if they disagree, worry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "Weak instrument diagnostics in `causal_inference_mastery` at `src/causal_inference/iv/`:\n",
    "\n",
    "```python\n",
    "from causal_inference.iv.two_stage import TwoStageLeastSquares, LIML\n",
    "from causal_inference.iv.diagnostics import (\n",
    "    first_stage_f_test,\n",
    "    weak_instrument_test,\n",
    "    stock_yogo_critical_values\n",
    ")\n",
    "\n",
    "# Fit 2SLS\n",
    "model_2sls = TwoStageLeastSquares(\n",
    "    formula='log_wage ~ [years_of_schooling ~ q4] + year_of_birth + state_of_birth'\n",
    ")\n",
    "result_2sls = model_2sls.fit(data)\n",
    "\n",
    "# Weak instrument test\n",
    "f_test = first_stage_f_test(result_2sls)\n",
    "print(f\"First-stage F: {f_test.statistic:.2f}\")\n",
    "print(f\"Stock-Yogo 10% critical value: {stock_yogo_critical_values(n_instruments=1)['10%']}\")\n",
    "print(f\"Weak instrument? {f_test.statistic < 10}\")\n",
    "\n",
    "# Compare with LIML\n",
    "model_liml = LIML(\n",
    "    formula='log_wage ~ [years_of_schooling ~ q4] + year_of_birth + state_of_birth'\n",
    ")\n",
    "result_liml = model_liml.fit(data)\n",
    "\n",
    "print(f\"2SLS estimate: {result_2sls.params['years_of_schooling']:.4f}\")\n",
    "print(f\"LIML estimate: {result_liml.params['years_of_schooling']:.4f}\")\n",
    "print(f\"Difference: {abs(result_2sls.params['years_of_schooling'] - result_liml.params['years_of_schooling']):.4f}\")\n",
    "```\n",
    "\n",
    "The production code also includes:\n",
    "- Cragg-Donald weak instrument test for multiple instruments\n",
    "- Anderson-Rubin confidence intervals (robust to weak instruments)\n",
    "- Conditional likelihood ratio tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Google L5, Econ)**: *\"What happens to 2SLS estimates when instruments are weak?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Two main problems:**\n",
    "\n",
    "1. **Bias toward OLS**:\n",
    "   - 2SLS is consistent but biased in finite samples\n",
    "   - Bias is approximately $\\frac{\\text{OLS bias}}{1 + F}$\n",
    "   - With weak instruments (low F), bias approaches OLS bias\n",
    "   - More instruments → more bias toward OLS\n",
    "\n",
    "2. **Variance explosion**:\n",
    "   - IV variance is $\\propto 1/\\text{Cov}(Z, T)^2$\n",
    "   - Weak instrument → small covariance → huge variance\n",
    "   - Confidence intervals become uninformative\n",
    "\n",
    "**Rule of thumb**: First-stage F > 10 for \"safe\" zone (Stock, Wright, Yogo 2002).\n",
    "\n",
    "**Remedies**:\n",
    "- Use just-identified IV (median-unbiased)\n",
    "- Use LIML (approximately median-unbiased for over-identified)\n",
    "- Find stronger instruments\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Meta E5, DS)**: *\"Why is the 'F > 10' rule a rule of thumb, not a theorem?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**It's an approximation because:**\n",
    "\n",
    "1. **Depends on acceptable bias**: Stock-Yogo (2005) shows:\n",
    "   - F > 16.38 for 10% relative bias\n",
    "   - F > 8.96 for 15% relative bias\n",
    "   - F > 10 is convenient midpoint\n",
    "\n",
    "2. **Assumes homoskedasticity**: Critical values derived under homoskedastic errors. With heteroskedasticity, need different tests.\n",
    "\n",
    "3. **Single endogenous variable**: With multiple endogenous variables, need Cragg-Donald statistic and different critical values.\n",
    "\n",
    "4. **Number of instruments matters**: More instruments → need higher F.\n",
    "\n",
    "5. **Sample size matters**: Bias formula involves $K/n$, so same F is worse with more instruments.\n",
    "\n",
    "**Better practice**: Use Stock-Yogo critical values appropriate to your setting.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Amazon L6, Quant)**: *\"Explain why LIML is less biased than 2SLS with weak instruments.\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Intuition**:\n",
    "\n",
    "2SLS bias arises because:\n",
    "1. First-stage residuals are correlated with second-stage errors by construction\n",
    "2. In finite samples, this correlation \"leaks\" into estimates\n",
    "3. With weak instruments, first stage has high variance → more leakage\n",
    "\n",
    "**LIML is different**:\n",
    "1. LIML minimizes a ratio of quadratic forms\n",
    "2. It doesn't commit to first-stage estimates before second stage\n",
    "3. Effectively uses a \"shrinkage\" approach that reduces finite-sample bias\n",
    "\n",
    "**Technical result** (Bekker asymptotic sequence):\n",
    "- When $K/n \\to c$ (instruments grow with sample), 2SLS is inconsistent\n",
    "- LIML remains consistent (approximately median-unbiased)\n",
    "\n",
    "**Trade-off**: LIML has higher variance, especially when instruments *are* strong. So:\n",
    "- Strong instruments → 2SLS preferred (lower variance, negligible bias)\n",
    "- Weak instruments → LIML preferred (bias reduction worth variance cost)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Stock, J. H., Wright, J. H., & Yogo, M. (2002). \"A Survey of Weak Instruments and Weak Identification in Generalized Method of Moments.\" *Journal of Business & Economic Statistics*.\n",
    "\n",
    "[^2]: Stock, J. H. & Yogo, M. (2005). \"Testing for Weak Instruments in Linear IV Regression.\" In *Identification and Inference for Econometric Models*. [research_kb: `a2e2d729-3730-4120-b155-28dfe5a7d0a4`]\n",
    "\n",
    "[^3]: Angrist, J. D. & Pischke, J.-S. (2009). *Mostly Harmless Econometrics*, Section 4.6. Princeton University Press. [research_kb: `93737674-d68d-4952-957f-00e26f085088`]\n",
    "\n",
    "[^4]: Bound, J., Jaeger, D. A., & Baker, R. M. (1995). \"Problems with Instrumental Variables Estimation When the Correlation Between the Instruments and the Endogenous Explanatory Variable is Weak.\" *JASA*.\n",
    "\n",
    "[^5]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 8: \"Instrumental Variables.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Precision Improvement:**\n",
    "- You said: \"Build the weak instruments notebook\"\n",
    "- Concise: \"Build 04_weak_instruments\"\n",
    "- Precise: `/augmented 08.4 --weak-iv --stock-yogo --liml`\n",
    "- Pattern: [build] [target] [content-flags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}