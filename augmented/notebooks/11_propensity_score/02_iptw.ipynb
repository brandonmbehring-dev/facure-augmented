{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Inverse Probability of Treatment Weighting (IPTW)\n",
    "\n",
    "**Chapter 11, Section 2**\n",
    "\n",
    "This notebook covers the IPTW estimator: using propensity scores to reweight the sample.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition) - Reweighting to create pseudo-population\n",
    "2. [Formal Treatment](#formal) - IPTW derivation\n",
    "3. [Numeric Demonstration](#numeric) - Growth mindset application\n",
    "4. [Implementation](#implementation) - Stabilized weights and inference\n",
    "5. [Interview Appendix](#interview) - Practice questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set notebook style\n",
    "set_notebook_style()\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "### The Reweighting Idea\n",
    "\n",
    "Instead of matching units, IPTW creates a **pseudo-population** where treatment is independent of covariates.\n",
    "\n",
    "**Problem**: In observational data, $P(T=1|X)$ varies with $X$ (confounding)\n",
    "\n",
    "**Solution**: Weight each observation to make the sample \"look like\" an RCT\n",
    "\n",
    "### How It Works\n",
    "\n",
    "For treated units with high propensity score:\n",
    "- They were \"likely\" to be treated anyway\n",
    "- **Down-weight** them (not representative of all treated units)\n",
    "\n",
    "For treated units with low propensity score:\n",
    "- They were \"unlikely\" to be treated\n",
    "- **Up-weight** them (represent many others who weren't treated)\n",
    "\n",
    "Same logic for control units (flip the weights).\n",
    "\n",
    "### The Weight Formula\n",
    "\n",
    "$$w_i = \\frac{T_i}{e(X_i)} + \\frac{1-T_i}{1-e(X_i)}$$\n",
    "\n",
    "- Treated units: $w_i = 1/e(X_i)$ (inverse of treatment probability)\n",
    "- Control units: $w_i = 1/(1-e(X_i))$ (inverse of control probability)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and estimate propensity scores\n",
    "mindset = load_facure_data(\"learning_mindset.csv\")\n",
    "\n",
    "covariates = [\n",
    "    'success_expect', 'ethnicity', 'gender', 'frst_in_family',\n",
    "    'school_mindset', 'school_achievement', 'school_ethnic_minority',\n",
    "    'school_poverty', 'school_size'\n",
    "]\n",
    "\n",
    "X = mindset[covariates].values\n",
    "T = mindset['intervention'].values\n",
    "Y = mindset['achievement_score'].values\n",
    "\n",
    "# Estimate propensity scores\n",
    "ps_model = LogisticRegression(C=1e6, max_iter=1000, solver='lbfgs')\n",
    "ps_model.fit(X, T)\n",
    "ps = ps_model.predict_proba(X)[:, 1]\n",
    "\n",
    "print(f\"Data: {len(mindset)} students\")\n",
    "print(f\"PS range: [{ps.min():.4f}, {ps.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IPW weights\n",
    "weights = T / ps + (1 - T) / (1 - ps)\n",
    "\n",
    "print(\"IPW WEIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTreated weights (1/e(X)):\")\n",
    "print(f\"  Min:  {weights[T==1].min():.3f}\")\n",
    "print(f\"  Max:  {weights[T==1].max():.3f}\")\n",
    "print(f\"  Mean: {weights[T==1].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nControl weights (1/(1-e(X))):\")\n",
    "print(f\"  Min:  {weights[T==0].min():.3f}\")\n",
    "print(f\"  Max:  {weights[T==0].max():.3f}\")\n",
    "print(f\"  Mean: {weights[T==0].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Formal Treatment\n",
    "\n",
    "### IPTW Estimator\n",
    "\n",
    "**Theorem** (Horvitz-Thompson Estimator):\n",
    "\n",
    "Under conditional ignorability and positivity:\n",
    "\n",
    "$$\\hat{\\tau}_{IPTW} = \\frac{1}{n}\\sum_i \\frac{T_i Y_i}{e(X_i)} - \\frac{1}{n}\\sum_i \\frac{(1-T_i) Y_i}{1-e(X_i)}$$\n",
    "\n",
    "is an unbiased estimator of the ATE.\n",
    "\n",
    "### Derivation\n",
    "\n",
    "**Step 1**: For treated potential outcome:\n",
    "$$E\\left[\\frac{T_i Y_i}{e(X_i)}\\right] = E\\left[E\\left[\\frac{T_i Y_i}{e(X_i)} \\mid X_i\\right]\\right]$$\n",
    "\n",
    "**Step 2**: Conditional on $X_i$:\n",
    "$$E\\left[\\frac{T_i Y_i}{e(X_i)} \\mid X_i\\right] = \\frac{1}{e(X_i)} E[T_i Y_i | X_i]$$\n",
    "\n",
    "**Step 3**: Since $E[T_i | X_i] = e(X_i)$ and by ignorability:\n",
    "$$= \\frac{1}{e(X_i)} \\cdot e(X_i) \\cdot E[Y_i(1) | X_i] = E[Y_i(1) | X_i]$$\n",
    "\n",
    "**Step 4**: Taking outer expectation:\n",
    "$$E\\left[\\frac{T_i Y_i}{e(X_i)}\\right] = E[Y_i(1)]$$\n",
    "\n",
    "Similarly for control: $E\\left[\\frac{(1-T_i) Y_i}{1-e(X_i)}\\right] = E[Y_i(0)]$\n",
    "\n",
    "Therefore:\n",
    "$$E[\\hat{\\tau}_{IPTW}] = E[Y(1)] - E[Y(0)] = \\tau_{ATE}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement basic IPTW\n",
    "def iptw_ate(Y, T, ps):\n",
    "    \"\"\"\n",
    "    Horvitz-Thompson IPTW estimator for ATE.\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    \n",
    "    # Mean of weighted outcomes\n",
    "    mu1 = np.sum(T * Y / ps) / n\n",
    "    mu0 = np.sum((1 - T) * Y / (1 - ps)) / n\n",
    "    \n",
    "    return mu1 - mu0\n",
    "\n",
    "ate_iptw = iptw_ate(Y, T, ps)\n",
    "print(f\"IPTW ATE: {ate_iptw:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hajek (normalized) estimator - more stable\n",
    "def hajek_ate(Y, T, ps):\n",
    "    \"\"\"\n",
    "    Hajek (normalized) IPTW estimator.\n",
    "    Normalizes by sum of weights instead of n.\n",
    "    \"\"\"\n",
    "    # Weighted mean for treated\n",
    "    mu1 = np.sum(T * Y / ps) / np.sum(T / ps)\n",
    "    \n",
    "    # Weighted mean for control\n",
    "    mu0 = np.sum((1 - T) * Y / (1 - ps)) / np.sum((1 - T) / (1 - ps))\n",
    "    \n",
    "    return mu1 - mu0\n",
    "\n",
    "ate_hajek = hajek_ate(Y, T, ps)\n",
    "\n",
    "print(\"IPTW ESTIMATES:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Horvitz-Thompson: {ate_iptw:.4f}\")\n",
    "print(f\"Hajek (normalized): {ate_hajek:.4f}\")\n",
    "print(f\"\\nHajek is generally preferred (more stable with extreme weights)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Numeric Demonstration\n",
    "\n",
    "### Visualizing the Reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distribution\n",
    "fig, axes = create_tufte_figure(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Left: Weight distribution by treatment\n",
    "ax = axes[0]\n",
    "ax.hist(weights[T==0], bins=30, alpha=0.5, color=COLORS['blue'], label='Control', density=True)\n",
    "ax.hist(weights[T==1], bins=30, alpha=0.5, color=COLORS['red'], label='Treated', density=True)\n",
    "set_tufte_title(ax, \"IPW Weight Distribution\")\n",
    "set_tufte_labels(ax, \"Weight\", \"Density\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Right: Weight vs propensity score\n",
    "ax = axes[1]\n",
    "ax.scatter(ps[T==1], weights[T==1], alpha=0.3, s=10, c=COLORS['red'], label='Treated')\n",
    "ax.scatter(ps[T==0], weights[T==0], alpha=0.3, s=10, c=COLORS['blue'], label='Control')\n",
    "set_tufte_title(ax, \"Weight vs Propensity Score\")\n",
    "set_tufte_labels(ax, \"Propensity Score e(X)\", \"IPW Weight\")\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Treated with low PS get high weights (unlikely to be treated).\")\n",
    "print(\"Control with high PS get high weights (unlikely to be control).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check balance after weighting\n",
    "def weighted_mean(x, w, t, treatment_value):\n",
    "    \"\"\"Compute weighted mean for a treatment group.\"\"\"\n",
    "    mask = t == treatment_value\n",
    "    return np.average(x[mask], weights=w[mask])\n",
    "\n",
    "print(\"COVARIATE BALANCE AFTER WEIGHTING:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Covariate':<25} {'Unweighted Diff':<18} {'Weighted Diff'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cov in covariates:\n",
    "    x = mindset[cov].values\n",
    "    \n",
    "    # Unweighted difference\n",
    "    unweighted_diff = x[T==1].mean() - x[T==0].mean()\n",
    "    \n",
    "    # Weighted difference\n",
    "    weighted_treated = weighted_mean(x, weights, T, 1)\n",
    "    weighted_control = weighted_mean(x, weights, T, 0)\n",
    "    weighted_diff = weighted_treated - weighted_control\n",
    "    \n",
    "    print(f\"{cov:<25} {unweighted_diff:+.4f}           {weighted_diff:+.4f}\")\n",
    "\n",
    "print(f\"\\nWeighting dramatically improves covariate balance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Stabilized Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stabilized weights - reduce variance\n",
    "def stabilized_weights(T, ps):\n",
    "    \"\"\"\n",
    "    Stabilized IPW weights.\n",
    "    Multiply standard weights by marginal treatment probability.\n",
    "    \"\"\"\n",
    "    p_t = T.mean()  # Marginal P(T=1)\n",
    "    \n",
    "    # Stabilized weights\n",
    "    sw = T * (p_t / ps) + (1 - T) * ((1 - p_t) / (1 - ps))\n",
    "    \n",
    "    return sw\n",
    "\n",
    "sw = stabilized_weights(T, ps)\n",
    "\n",
    "print(\"STABILIZED vs STANDARD WEIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nStandard weights:\")\n",
    "print(f\"  Mean: {weights.mean():.3f}\")\n",
    "print(f\"  Std:  {weights.std():.3f}\")\n",
    "print(f\"  Max:  {weights.max():.3f}\")\n",
    "\n",
    "print(f\"\\nStabilized weights:\")\n",
    "print(f\"  Mean: {sw.mean():.3f}\")\n",
    "print(f\"  Std:  {sw.std():.3f}\")\n",
    "print(f\"  Max:  {sw.max():.3f}\")\n",
    "\n",
    "print(f\"\\nStabilized weights have mean ~1 and lower variance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Bootstrap standard errors for IPTW\ndef bootstrap_iptw(Y, T, X, n_bootstrap=500, alpha=0.05):\n    \"\"\"\n    Bootstrap confidence intervals for IPTW ATE.\n    \"\"\"\n    n = len(Y)\n    estimates = []\n    \n    for _ in range(n_bootstrap):\n        # Bootstrap sample\n        idx = np.random.choice(n, n, replace=True)\n        Y_boot = Y[idx]\n        T_boot = T[idx]\n        X_boot = X[idx]\n        \n        # Re-estimate propensity scores\n        ps_model = LogisticRegression(C=1e6, max_iter=1000, solver='lbfgs')\n        ps_model.fit(X_boot, T_boot)\n        ps_boot = ps_model.predict_proba(X_boot)[:, 1]\n        \n        # Compute IPTW estimate\n        ate_boot = hajek_ate(Y_boot, T_boot, ps_boot)\n        estimates.append(ate_boot)\n    \n    estimates = np.array(estimates)\n    \n    return {\n        'ate': np.mean(estimates),\n        'se': np.std(estimates),\n        'ci_lower': np.percentile(estimates, 100 * alpha / 2),\n        'ci_upper': np.percentile(estimates, 100 * (1 - alpha / 2))\n    }\n\nprint(\"Computing bootstrap CI (this may take a moment)...\")\nnp.random.seed(42)\nboot_result = bootstrap_iptw(Y, T, X, n_bootstrap=200)\n\nprint(f\"\\nBOOTSTRAP RESULTS:\")\nprint(\"=\" * 50)\nprint(f\"ATE: {boot_result['ate']:.4f}\")\nprint(f\"SE:  {boot_result['se']:.4f}\")\nprint(f\"95% CI: [{boot_result['ci_lower']:.4f}, {boot_result['ci_upper']:.4f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "print(\"\\nFINAL COMPARISON OF ESTIMATES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Naive\n",
    "naive = Y[T==1].mean() - Y[T==0].mean()\n",
    "print(f\"Naive (no adjustment):     {naive:.4f}\")\n",
    "\n",
    "# Regression\n",
    "formula = 'achievement_score ~ intervention + ' + ' + '.join(covariates)\n",
    "mindset_copy = mindset.copy()\n",
    "reg = smf.ols(formula, data=mindset_copy).fit()\n",
    "print(f\"Regression adjusted:       {reg.params['intervention']:.4f} (SE: {reg.bse['intervention']:.4f})\")\n",
    "\n",
    "# IPTW\n",
    "print(f\"IPTW (Hajek):              {ate_hajek:.4f} (SE: {boot_result['se']:.4f})\")\n",
    "\n",
    "print(f\"\\nAll methods give similar estimates when model is well-specified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1: What is the intuition behind inverse probability weighting?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Core intuition**: Create a pseudo-population where treatment is independent of covariates.\n",
    "\n",
    "**How it works**:\n",
    "\n",
    "1. **Treated unit with high PS**: \"Expected\" to be treated → down-weight (represents few others)\n",
    "\n",
    "2. **Treated unit with low PS**: \"Surprising\" to be treated → up-weight (represents many others who weren't treated)\n",
    "\n",
    "3. **Control unit with low PS**: \"Expected\" to be control → down-weight\n",
    "\n",
    "4. **Control unit with high PS**: \"Surprising\" to be control → up-weight\n",
    "\n",
    "**Mathematical intuition**:\n",
    "\n",
    "$$E\\left[\\frac{TY}{e(X)}\\right] = E\\left[\\frac{P(T=1|X) \\cdot Y(1)}{P(T=1|X)}\\right] = E[Y(1)]$$\n",
    "\n",
    "The propensity score in the denominator \"cancels out\" the selection mechanism, recovering the marginal mean.\n",
    "\n",
    "**Analogy**: Survey sampling - if certain subgroups are underrepresented, we up-weight them to recover population statistics.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q2: What are the problems with IPTW and how do you address them?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Problem 1: Extreme Weights**\n",
    "\n",
    "When $e(X) \\approx 0$ or $e(X) \\approx 1$, weights become huge.\n",
    "\n",
    "*Solutions*:\n",
    "- **Stabilized weights**: Multiply by marginal treatment probability\n",
    "- **Weight trimming**: Cap weights at some percentile (e.g., 99th)\n",
    "- **Overlap enforcement**: Exclude units with extreme PS\n",
    "\n",
    "**Problem 2: Model Dependence**\n",
    "\n",
    "IPTW relies on correct PS model specification.\n",
    "\n",
    "*Solutions*:\n",
    "- Check balance after weighting\n",
    "- Try different PS models (logistic, GBM, etc.)\n",
    "- Use doubly robust methods (Chapter 12)\n",
    "\n",
    "**Problem 3: Variance**\n",
    "\n",
    "IPTW can have high variance, especially with extreme weights.\n",
    "\n",
    "*Solutions*:\n",
    "- Use Hajek (normalized) estimator\n",
    "- Stabilized weights\n",
    "- Bootstrap for variance estimation\n",
    "\n",
    "**Problem 4: Positivity Violation**\n",
    "\n",
    "If $e(X) = 0$ or $e(X) = 1$ for some $X$, IPTW is undefined.\n",
    "\n",
    "*Solutions*:\n",
    "- Trim the sample to common support\n",
    "- Report on excluded units\n",
    "- Consider bounds on treatment effect\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q3: What is the difference between Horvitz-Thompson and Hajek estimators?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Horvitz-Thompson (HT)**:\n",
    "\n",
    "$$\\hat{\\mu}_1^{HT} = \\frac{1}{n}\\sum_i \\frac{T_i Y_i}{e(X_i)}$$\n",
    "\n",
    "- Divides by sample size $n$\n",
    "- Unbiased for $E[Y(1)]$\n",
    "- Can give nonsensical results if weights are extreme\n",
    "\n",
    "**Hajek (Normalized)**:\n",
    "\n",
    "$$\\hat{\\mu}_1^{Hajek} = \\frac{\\sum_i T_i Y_i / e(X_i)}{\\sum_i T_i / e(X_i)}$$\n",
    "\n",
    "- Divides by sum of weights\n",
    "- Slightly biased but much lower variance\n",
    "- Always gives reasonable results (weighted average)\n",
    "\n",
    "**When to use each**:\n",
    "- **HT**: When you need unbiasedness and weights are well-behaved\n",
    "- **Hajek**: Almost always preferred in practice (more stable)\n",
    "\n",
    "**Key insight**: The Hajek estimator is essentially a weighted mean, which is always in the range of observed values. The HT estimator can extrapolate beyond this range.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[^1]: Facure, M. (2022). *Causal Inference for the Brave and True*, Chapter 11.\n",
    "\n",
    "[^2]: Horvitz, D. G., & Thompson, D. J. (1952). A generalization of sampling without replacement from a finite universe. *JASA*, 47(260), 663-685.\n",
    "\n",
    "[^3]: Robins, J. M., Rotnitzky, A., & Zhao, L. P. (1994). Estimation of regression coefficients when some regressors are not always observed. *JASA*, 89(427), 846-866.\n",
    "\n",
    "[^4]: Cross-reference: Doubly robust estimation in `12_doubly_robust/01_dr_estimator.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}