{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20.2 Continuous Treatment Extension\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Implementation](#implementation)\n",
    "4. [Numeric Demonstration](#numeric)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand CATE as partial derivative for continuous treatments\n",
    "- Derive the continuous treatment target transformation\n",
    "- Implement price elasticity estimation with ML models\n",
    "- Recognize when ordering effects is sufficient vs. needing exact magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(123)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition <a name=\"intuition\"></a>\n",
    "\n",
    "### From Binary to Continuous Treatment\n",
    "\n",
    "**Binary treatment**: CATE = effect of switching from T=0 to T=1\n",
    "$$\\tau(x) = E[Y(1) - Y(0) | X]$$\n",
    "\n",
    "**Continuous treatment**: CATE = marginal effect of increasing T\n",
    "$$\\tau(x) = E\\left[\\frac{\\partial Y(t)}{\\partial t} \\Big| X\\right]$$\n",
    "\n",
    "### Price Elasticity Example\n",
    "\n",
    "Consider ice cream sales:\n",
    "- **Treatment**: Price (continuous)\n",
    "- **Outcome**: Sales quantity\n",
    "- **CATE**: How much do sales change per unit price increase?\n",
    "\n",
    "This varies by context:\n",
    "- **Hot days**: People less price-sensitive (lower |elasticity|)\n",
    "- **Cold days**: People more price-sensitive (higher |elasticity|)\n",
    "\n",
    "### Local Linear Model Intuition\n",
    "\n",
    "Assume locally linear relationship:\n",
    "$$Y_i = \\alpha + \\beta T_i + e_i \\quad | X_i = x$$\n",
    "\n",
    "The CATE $\\tau(x) = \\beta$ is the slope in this local model.\n",
    "\n",
    "**OLS estimator** for $\\beta$:\n",
    "$$\\hat{\\beta} = \\frac{Cov(Y, T)}{Var(T)} = \\frac{\\sum(Y_i - \\bar{Y})(T_i - \\bar{T})}{\\sum(T_i - \\bar{T})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment <a name=\"formal\"></a>\n",
    "\n",
    "### Target Transformation for Continuous Treatment\n",
    "\n",
    "**Full transformation** (scaled to equal CATE in expectation):\n",
    "$$Y^*_i = (Y_i - \\bar{Y}) \\cdot \\frac{T_i - \\bar{T}}{\\sigma^2_T}$$\n",
    "\n",
    "**Simplified transformation** (for ordering only):\n",
    "$$Y^*_i = (Y_i - \\bar{Y})(T_i - \\bar{T})$$\n",
    "\n",
    "### Proof: $E[Y^* | X] = \\tau(X)$\n",
    "\n",
    "**Define**: $V_i = \\frac{T_i - \\bar{T}}{\\sigma^2_T}$\n",
    "\n",
    "**Key properties** (under random treatment assignment):\n",
    "1. $E[V_i | X=x] = 0$ (centered treatment)\n",
    "2. $E[T_i V_i | X=x] = 1$ (by definition of variance)\n",
    "3. $E[e_i V_i | X=x] = 0$ (independence under RCT)\n",
    "\n",
    "**Derivation**:\n",
    "$$\\begin{aligned}\n",
    "E[Y^*_i | X=x] &= E[(Y_i - \\bar{Y})V_i | X=x] \\\\\n",
    "&= E[(\\alpha + \\beta T_i + e_i - \\bar{Y})V_i | X=x] \\\\\n",
    "&= \\alpha \\cdot E[V_i | X=x] + \\beta \\cdot E[T_i V_i | X=x] + E[e_i V_i | X=x] \\\\\n",
    "&= \\alpha \\cdot 0 + \\beta \\cdot 1 + 0 \\\\\n",
    "&= \\beta = \\tau(x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Non-Random Treatment\n",
    "\n",
    "For observational data, replace $\\bar{T}$ with conditional expectation $M(X_i) = E[T|X]$:\n",
    "\n",
    "$$Y^*_i = (Y_i - \\bar{Y}) \\cdot \\frac{T_i - M(X_i)}{(T_i - M(X_i))^2}$$\n",
    "\n",
    "This ensures $E[V_i|X=x] = 0$ for proper identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementation <a name=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_f_learner_transform(Y: np.ndarray, T: np.ndarray, \n",
    "                                    scale: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    F-learner target transformation for continuous treatment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array-like\n",
    "        Outcome variable\n",
    "    T : array-like\n",
    "        Continuous treatment variable\n",
    "    scale : bool, default False\n",
    "        If True, divide by treatment variance for exact CATE.\n",
    "        If False, only useful for ordering (not magnitude).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Y_star : array\n",
    "        Transformed target where E[Y*|X] \u221d CATE(X)\n",
    "    \"\"\"\n",
    "    Y_centered = Y - Y.mean()\n",
    "    T_centered = T - T.mean()\n",
    "    \n",
    "    if scale:\n",
    "        T_var = T.var()\n",
    "        return Y_centered * T_centered / T_var\n",
    "    else:\n",
    "        return Y_centered * T_centered\n",
    "\n",
    "\n",
    "class ContinuousFLearner:\n",
    "    \"\"\"\n",
    "    F-Learner for continuous treatment effects.\n",
    "    \n",
    "    Estimates marginal effect dY/dT for different covariate profiles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model=None, scale: bool = False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_model : sklearn-compatible regressor, optional\n",
    "            Default: LGBMRegressor with conservative hyperparameters\n",
    "        scale : bool, default False\n",
    "            Whether to scale by treatment variance for exact CATE\n",
    "        \"\"\"\n",
    "        if base_model is None:\n",
    "            base_model = LGBMRegressor(max_depth=3, min_child_samples=300, num_leaves=5, verbose=-1)\n",
    "        self.model = base_model\n",
    "        self.scale = scale\n",
    "        self.T_mean_ = None\n",
    "        self.Y_mean_ = None\n",
    "        self.T_var_ = None\n",
    "        \n",
    "    def fit(self, X, Y, T):\n",
    "        \"\"\"\n",
    "        Fit continuous F-learner.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like (n, p)\n",
    "            Covariates (NOT including treatment)\n",
    "        Y : array-like (n,)\n",
    "            Outcome\n",
    "        T : array-like (n,)\n",
    "            Continuous treatment\n",
    "        \"\"\"\n",
    "        self.T_mean_ = T.mean()\n",
    "        self.Y_mean_ = Y.mean()\n",
    "        self.T_var_ = T.var()\n",
    "        \n",
    "        Y_star = continuous_f_learner_transform(Y, T, scale=self.scale)\n",
    "        self.model.fit(X, Y_star)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict CATE (or proportional to CATE if scale=False).\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "print(\"Continuous F-Learner implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Numeric Demonstration <a name=\"numeric\"></a>\n",
    "\n",
    "### Ice Cream Pricing Experiment\n",
    "\n",
    "Estimate price elasticity (how sales respond to price changes) for different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ice cream sales (randomized prices)\n",
    "prices_rnd = load_facure_data('ice_cream_sales_rnd.csv')\n",
    "print(f\"Dataset shape: {prices_rnd.shape}\")\n",
    "prices_rnd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "y = \"sales\"\n",
    "T = \"price\"\n",
    "X = [\"temp\", \"weekday\", \"cost\"]\n",
    "\n",
    "# Train/test split\n",
    "np.random.seed(123)\n",
    "train, test = train_test_split(prices_rnd, test_size=0.3)\n",
    "print(f\"Train: {train.shape[0]}, Test: {test.shape[0]}\")\n",
    "\n",
    "# Overall elasticity (ATE)\n",
    "overall_elast = np.cov(train[y], train[T])[0,1] / np.var(train[T])\n",
    "print(f\"Overall price elasticity: {overall_elast:.2f} sales per unit price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply continuous target transformation (unscaled - for ordering)\n",
    "Y_star_cont = continuous_f_learner_transform(\n",
    "    train[y].values, \n",
    "    train[T].values, \n",
    "    scale=False\n",
    ")\n",
    "\n",
    "print(\"Target Transformation Statistics:\")\n",
    "print(f\"  Original Y (sales) range: [{train[y].min()}, {train[y].max()}]\")\n",
    "print(f\"  Transformed Y* range: [{Y_star_cont.min():.1f}, {Y_star_cont.max():.1f}]\")\n",
    "print(f\"  Y* mean: {Y_star_cont.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit continuous F-learner\n",
    "cont_f_learner = ContinuousFLearner(scale=False)\n",
    "cont_f_learner.fit(train[X], train[y].values, train[T].values)\n",
    "\n",
    "# Predict elasticity ordering\n",
    "test_pred = test.assign(cate=cont_f_learner.predict(test[X]))\n",
    "\n",
    "print(\"CATE Predictions (proportional to elasticity):\")\n",
    "print(f\"  Range: [{test_pred['cate'].min():.1f}, {test_pred['cate'].max():.1f}]\")\n",
    "print(f\"  Note: More negative = more price-sensitive\")\n",
    "\n",
    "test_pred.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation functions\n",
    "def sensitivity(data, y, t):\n",
    "    \"\"\"Estimate treatment effect (elasticity) via OLS coefficient.\"\"\"\n",
    "    t_bar = data[t].mean()\n",
    "    y_bar = data[y].mean()\n",
    "    cov = np.sum((data[t] - t_bar) * (data[y] - y_bar))\n",
    "    var = np.sum((data[t] - t_bar) ** 2)\n",
    "    return cov / var if var > 0 else 0\n",
    "\n",
    "def cumulative_gain(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    \"\"\"Compute cumulative gain curve.\"\"\"\n",
    "    size = dataset.shape[0]\n",
    "    ordered_df = dataset.sort_values(prediction, ascending=False).reset_index(drop=True)\n",
    "    n_rows = list(range(min_periods, size, size // steps)) + [size]\n",
    "    return np.array([sensitivity(ordered_df.head(rows), y, t) * (rows/size) \n",
    "                     for rows in n_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate with cumulative gain curves\ngain_curve_test = cumulative_gain(test_pred, \"cate\", y=\"sales\", t=\"price\")\n\ntrain_pred = train.assign(cate=cont_f_learner.predict(train[X]))\ngain_curve_train = cumulative_gain(train_pred, \"cate\", y=\"sales\", t=\"price\")\n\n# Baseline elasticity\nbaseline_elast = sensitivity(test, \"sales\", \"price\")\n\n# Plot\nfig, ax = create_tufte_figure(figsize=(10, 6))\n\n# Use separate x axes for each curve to handle different lengths\nx_axis_test = np.linspace(0, 100, len(gain_curve_test))\nx_axis_train = np.linspace(0, 100, len(gain_curve_train))\n\nax.plot(x_axis_test, gain_curve_test, color=COLORS['blue'], linewidth=2, label='Test')\nax.plot(x_axis_train, gain_curve_train, color=COLORS['gray'], linewidth=2, label='Train')\nax.plot([0, 100], [0, baseline_elast], linestyle='--', color='black', \n        linewidth=1.5, label='Random (ATE line)')\n\nax.set_xlabel('% of Days (sorted by predicted elasticity)')\nax.set_ylabel('Cumulative Gain (Elasticity \u00d7 Proportion)')\nax.set_title('Continuous F-Learner: Price Elasticity Estimation')\nax.legend(loc='upper left')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBaseline elasticity: {baseline_elast:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "\n",
    "The continuous F-learner shows:\n",
    "1. **Strong performance** - curve well above diagonal\n",
    "2. **Low overfitting** - train and test curves are close\n",
    "3. **Effective ordering** - successfully identifies high/low elasticity days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine what drives elasticity heterogeneity\n",
    "test_pred['elasticity_band'] = pd.qcut(test_pred['cate'], q=5, \n",
    "                                        labels=['Very Sensitive', 'Sensitive', 'Average', \n",
    "                                                'Insensitive', 'Very Insensitive'])\n",
    "\n",
    "# Characteristics by band\n",
    "band_chars = test_pred.groupby('elasticity_band', observed=True).agg({\n",
    "    'temp': 'mean',\n",
    "    'weekday': 'mean',\n",
    "    'cost': 'mean',\n",
    "    'sales': 'mean',\n",
    "    'price': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "# Actual elasticity by band\n",
    "band_elast = test_pred.groupby('elasticity_band', observed=True).apply(\n",
    "    lambda df: sensitivity(df, 'sales', 'price'),\n",
    "    include_groups=False\n",
    ").rename('actual_elasticity')\n",
    "\n",
    "result = pd.concat([band_chars, band_elast], axis=1)\n",
    "print(\"Characteristics by Predicted Elasticity Band:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize: Temperature vs Elasticity\nfig, axes = create_tufte_figure(ncols=2, figsize=(14, 5))\n\n# Temperature bins\ntest_pred['temp_bin'] = pd.cut(test_pred['temp'], bins=5)\ntemp_elast = test_pred.groupby('temp_bin', observed=True).apply(\n    lambda df: sensitivity(df, 'sales', 'price'),\n    include_groups=False\n)\n\naxes[0].bar(range(len(temp_elast)), temp_elast.values, color=COLORS['blue'], alpha=0.7)\naxes[0].axhline(y=baseline_elast, color='black', linestyle='--', label='Overall')\naxes[0].set_xticks(range(len(temp_elast)))\naxes[0].set_xticklabels([f'{b.left:.0f}-{b.right:.0f}' for b in temp_elast.index], rotation=45)\naxes[0].set_xlabel('Temperature Range')\naxes[0].set_ylabel('Price Elasticity')\naxes[0].set_title('Elasticity by Temperature')\naxes[0].legend()\n\n# Weekday effect\nweekday_elast = test_pred.groupby('weekday').apply(\n    lambda df: sensitivity(df, 'sales', 'price'),\n    include_groups=False\n)\n\naxes[1].bar(range(len(weekday_elast)), weekday_elast.values, color=COLORS['gray'], alpha=0.7)\naxes[1].axhline(y=baseline_elast, color='black', linestyle='--', label='Overall')\naxes[1].set_xticks(range(len(weekday_elast)))\naxes[1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\naxes[1].set_xlabel('Day of Week')\naxes[1].set_ylabel('Price Elasticity')\naxes[1].set_title('Elasticity by Weekday')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInsight: Warmer days \u2192 less price sensitive (elasticity closer to 0)\")\nprint(\"This makes sense: hot weather increases ice cream demand regardless of price.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Implication\n",
    "\n",
    "**Pricing Strategy**:\n",
    "- **Hot days**: Customers less price-sensitive \u2192 can charge higher prices\n",
    "- **Cold days**: Customers more price-sensitive \u2192 need competitive pricing\n",
    "\n",
    "The continuous F-learner successfully identifies this heterogeneity, enabling dynamic pricing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix <a name=\"interview\"></a>\n",
    "\n",
    "### Q1: How does CATE differ between binary and continuous treatments?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Binary treatment**:\n",
    "$$\\tau(x) = E[Y(1) - Y(0) | X = x]$$\n",
    "Interpretation: Effect of switching from untreated to treated.\n",
    "\n",
    "**Continuous treatment**:\n",
    "$$\\tau(x) = E\\left[\\frac{\\partial Y(t)}{\\partial t} \\Big| X = x\\right]$$\n",
    "Interpretation: Marginal effect of increasing treatment by one unit.\n",
    "\n",
    "Key differences:\n",
    "- Binary: \"On vs Off\" comparison\n",
    "- Continuous: Slope/derivative interpretation\n",
    "- Continuous assumes local linearity\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Why might you use the unscaled transformation?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Full transformation** (scaled):\n",
    "$$Y^* = (Y - \\bar{Y}) \\cdot \\frac{T - \\bar{T}}{\\sigma^2_T}$$\n",
    "\n",
    "**Simplified** (unscaled):\n",
    "$$Y^* = (Y - \\bar{Y})(T - \\bar{T})$$\n",
    "\n",
    "Use unscaled when:\n",
    "1. **Only ordering matters**: Rank units by effect magnitude without needing exact values\n",
    "2. **Targeting decisions**: \"Who responds most?\" not \"By how much?\"\n",
    "3. **Avoid division issues**: $\\sigma^2_T$ can be unstable in small samples\n",
    "\n",
    "The unscaled version preserves ordering ($Y^*_{scaled} = Y^*_{unscaled} / \\sigma^2_T$) since division by constant doesn't change rank.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: What assumption is required for the continuous F-learner?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Local linearity assumption**:\n",
    "$$Y_i = \\alpha + \\beta T_i + e_i \\quad | X_i = x$$\n",
    "\n",
    "Within each covariate region $X=x$, the outcome is linear in treatment.\n",
    "\n",
    "**Why needed**:\n",
    "- The transformation estimates the *slope* $\\beta$\n",
    "- If relationship is nonlinear, slope varies with treatment level\n",
    "- Model conflates treatment-level heterogeneity with covariate heterogeneity\n",
    "\n",
    "**When violated**:\n",
    "- Diminishing returns (concave)\n",
    "- Saturation effects\n",
    "- U-shaped or non-monotonic relationships\n",
    "\n",
    "**Mitigation**: Linearization via transformations (log-log for elasticity, etc.)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References <a name=\"references\"></a>\n",
    "\n",
    "1. **Facure, M.** (2022). *Causal Inference for the Brave and True*, Chapter 20.\n",
    "\n",
    "2. **Kennedy, E.H.** (2020). *Optimal doubly robust estimation of heterogeneous causal effects*. arXiv:2004.14497.\n",
    "\n",
    "3. **Nie, X. & Wager, S.** (2021). *Quasi-oracle estimation of heterogeneous treatment effects*. Biometrika."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}