{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20.1 Target Transformation: The F-Learner\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Implementation](#implementation)\n",
    "4. [Numeric Demonstration](#numeric)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the infeasibility problem in CATE estimation\n",
    "- Derive the F-learner target transformation for binary treatments\n",
    "- Implement plug-and-play CATE estimation with any ML model\n",
    "- Recognize the variance cost of simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(123)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition <a name=\"intuition\"></a>\n",
    "\n",
    "### The CATE Estimation Challenge\n",
    "\n",
    "In predictive modeling, we minimize outcome MSE:\n",
    "$$E[(Y_i - \\hat{Y}_i)^2]$$\n",
    "\n",
    "For CATE estimation, we **want** to minimize treatment effect MSE:\n",
    "$$E[(\\tau(x)_i - \\hat{\\tau}(x)_i)^2]$$\n",
    "\n",
    "**Problem**: We never observe $\\tau(x)_i = Y_i(1) - Y_i(0)$. Only one potential outcome is realized.\n",
    "\n",
    "### The Target Transformation Insight\n",
    "\n",
    "**Key idea**: Transform the observable outcome into something that equals CATE *in expectation*.\n",
    "\n",
    "For a 50/50 randomized experiment:\n",
    "$$Y^*_i = 2 Y_i T_i - 2 Y_i (1-T_i)$$\n",
    "\n",
    "- If treated (T=1): $Y^* = 2Y$\n",
    "- If untreated (T=0): $Y^* = -2Y$\n",
    "\n",
    "This seems bizarre - outcomes become negative for control units. But remarkably:\n",
    "$$E[Y^*_i | X_i = x] = \\tau(x)_i$$\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "The transformation \"borrows\" information across treatment groups:\n",
    "- Treated units contribute information about $Y(1)$\n",
    "- Control units contribute information about $Y(0)$\n",
    "- Proper weighting combines them into unbiased CATE estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment <a name=\"formal\"></a>\n",
    "\n",
    "### General Target Transformation (Binary Treatment)\n",
    "\n",
    "For non-equal propensity:\n",
    "$$Y^*_i = Y_i \\cdot \\frac{T_i - e(X_i)}{e(X_i)(1-e(X_i))}$$\n",
    "\n",
    "where $e(X_i) = P(T_i = 1 | X_i)$ is the propensity score.\n",
    "\n",
    "### Proof: $E[Y^* | X] = \\tau(X)$\n",
    "\n",
    "**Setup**: Under conditional independence $(Y(0), Y(1)) \\perp T | X$.\n",
    "\n",
    "**Step 1**: Decompose by treatment status:\n",
    "$$Y_i = Y_i(1) T_i + Y_i(0)(1-T_i)$$\n",
    "\n",
    "**Step 2**: Expand the transformation:\n",
    "$$E[Y^*_i | X=x] = E\\left[Y_i \\cdot \\frac{T_i - e(x)}{e(x)(1-e(x))} \\Big| X=x\\right]$$\n",
    "\n",
    "**Step 3**: Separate treated and control terms:\n",
    "$$= E\\left[Y_i(1) \\cdot \\frac{T_i(1-e(x))}{e(x)(1-e(x))} \\Big| X=x\\right] - E\\left[Y_i(0) \\cdot \\frac{(1-T_i)e(x)}{e(x)(1-e(x))} \\Big| X=x\\right]$$\n",
    "\n",
    "**Step 4**: Apply conditional independence:\n",
    "$$= \\frac{E[Y_i(1)|X=x] \\cdot e(x)}{e(x)} - \\frac{E[Y_i(0)|X=x] \\cdot (1-e(x))}{(1-e(x))}$$\n",
    "\n",
    "**Step 5**: Simplify:\n",
    "$$= E[Y_i(1)|X=x] - E[Y_i(0)|X=x] = \\tau(x)$$\n",
    "\n",
    "### Special Case: RCT with Constant Propensity\n",
    "\n",
    "If $e(X) = p$ for all $X$ (randomized experiment):\n",
    "$$Y^*_i = Y_i \\cdot \\frac{T_i - p}{p(1-p)}$$\n",
    "\n",
    "For 50/50 split ($p = 0.5$):\n",
    "$$Y^*_i = Y_i \\cdot \\frac{T_i - 0.5}{0.25} = 4Y_i(T_i - 0.5) = 2Y_i T_i - 2Y_i(1-T_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementation <a name=\"implementation\"></a>\n",
    "\n",
    "### F-Learner Algorithm\n",
    "\n",
    "1. **Estimate propensity** (if not known): $\\hat{e}(X)$\n",
    "2. **Transform target**: $Y^*_i = Y_i \\cdot \\frac{T_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1-\\hat{e}(X_i))}$\n",
    "3. **Fit any ML model**: $\\hat{\\tau}(X) = M(X)$ to predict $Y^*$\n",
    "4. **Predict CATE**: Model outputs are CATE estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_learner_transform(Y: np.ndarray, T: np.ndarray, ps: float | np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    F-learner target transformation for binary treatment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array-like\n",
    "        Outcome variable\n",
    "    T : array-like\n",
    "        Binary treatment indicator (0 or 1)\n",
    "    ps : float or array-like\n",
    "        Propensity score(s) P(T=1|X)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Y_star : array\n",
    "        Transformed target where E[Y*|X] = CATE(X)\n",
    "    \"\"\"\n",
    "    return Y * (T - ps) / (ps * (1 - ps))\n",
    "\n",
    "\n",
    "class FLearner:\n",
    "    \"\"\"\n",
    "    F-Learner: Plug-and-play CATE estimator via target transformation.\n",
    "    \n",
    "    Named after the \"Feature\" or \"Fundamental\" learner - uses simple\n",
    "    target transformation to convert any regression model into a CATE estimator.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_model : sklearn-compatible regressor, optional\n",
    "            Default: LGBMRegressor with conservative hyperparameters\n",
    "        \"\"\"\n",
    "        if base_model is None:\n",
    "            base_model = LGBMRegressor(max_depth=3, min_child_samples=300, num_leaves=5, verbose=-1)\n",
    "        self.model = base_model\n",
    "        self.ps_ = None\n",
    "        \n",
    "    def fit(self, X, Y, T, ps=None):\n",
    "        \"\"\"\n",
    "        Fit F-learner.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like (n, p)\n",
    "            Covariates\n",
    "        Y : array-like (n,)\n",
    "            Outcome\n",
    "        T : array-like (n,)\n",
    "            Binary treatment\n",
    "        ps : float or array-like, optional\n",
    "            Propensity score. If None, uses marginal P(T=1).\n",
    "        \"\"\"\n",
    "        self.ps_ = ps if ps is not None else T.mean()\n",
    "        Y_star = f_learner_transform(Y, T, self.ps_)\n",
    "        self.model.fit(X, Y_star)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict CATE for new units.\"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "print(\"F-Learner implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Numeric Demonstration <a name=\"numeric\"></a>\n",
    "\n",
    "### Investment Email Experiment\n",
    "\n",
    "An investment firm sends financial education emails to increase customer investments.\n",
    "- **Treatment**: Email sent (binary)\n",
    "- **Outcome**: Converted (invested)\n",
    "- **Goal**: Find which customers respond best to emails (personalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load investment email data\n",
    "email = load_facure_data('invest_email_rnd.csv')\n",
    "print(f\"Dataset shape: {email.shape}\")\n",
    "print(f\"\\nColumns: {email.columns.tolist()}\")\n",
    "email.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "y = \"converted\"\n",
    "T = \"em1\"  # Email 1 treatment\n",
    "X = [\"age\", \"income\", \"insurance\", \"invested\"]\n",
    "\n",
    "# Train/test split\n",
    "np.random.seed(123)\n",
    "train, test = train_test_split(email, test_size=0.4)\n",
    "print(f\"Train: {train.shape[0]}, Test: {test.shape[0]}\")\n",
    "\n",
    "# Treatment probability (known from randomization)\n",
    "ps = train[T].mean()\n",
    "print(f\"Propensity score (treatment probability): {ps:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply target transformation\n",
    "Y_star_train = f_learner_transform(train[y].values, train[T].values, ps)\n",
    "\n",
    "# Examine transformation\n",
    "print(\"Target Transformation Statistics:\")\n",
    "print(f\"  Original Y range: [{train[y].min():.2f}, {train[y].max():.2f}]\")\n",
    "print(f\"  Transformed Y* range: [{Y_star_train.min():.2f}, {Y_star_train.max():.2f}]\")\n",
    "print(f\"  Y* mean (should \u2248 ATE): {Y_star_train.mean():.4f}\")\n",
    "\n",
    "# Compare to simple difference in means\n",
    "simple_ate = train[train[T]==1][y].mean() - train[train[T]==0][y].mean()\n",
    "print(f\"  Simple ATE estimate: {simple_ate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit F-learner\n",
    "f_learner = FLearner()\n",
    "f_learner.fit(train[X], train[y].values, train[T].values, ps=ps)\n",
    "\n",
    "# Predict CATE on test set\n",
    "test_pred = test.assign(cate=f_learner.predict(test[X]))\n",
    "\n",
    "# Show CATE variation\n",
    "print(\"CATE Predictions:\")\n",
    "print(f\"  Mean: {test_pred['cate'].mean():.4f}\")\n",
    "print(f\"  Std:  {test_pred['cate'].std():.4f}\")\n",
    "print(f\"  Range: [{test_pred['cate'].min():.4f}, {test_pred['cate'].max():.4f}]\")\n",
    "\n",
    "test_pred[['age', 'income', 'em1', 'converted', 'cate']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation functions (from Ch 19)\n",
    "def sensitivity(data, y, t):\n",
    "    \"\"\"Estimate treatment effect (elasticity) via OLS coefficient.\"\"\"\n",
    "    t_bar = data[t].mean()\n",
    "    y_bar = data[y].mean()\n",
    "    cov = np.sum((data[t] - t_bar) * (data[y] - y_bar))\n",
    "    var = np.sum((data[t] - t_bar) ** 2)\n",
    "    return cov / var if var > 0 else 0\n",
    "\n",
    "def cumulative_gain(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    \"\"\"Compute cumulative gain curve.\"\"\"\n",
    "    size = dataset.shape[0]\n",
    "    ordered_df = dataset.sort_values(prediction, ascending=False).reset_index(drop=True)\n",
    "    n_rows = list(range(min_periods, size, size // steps)) + [size]\n",
    "    return np.array([sensitivity(ordered_df.head(rows), y, t) * (rows/size) \n",
    "                     for rows in n_rows])\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate F-learner with cumulative gain curves\ngain_curve_test = cumulative_gain(test_pred, \"cate\", y=\"converted\", t=\"em1\")\n\ntrain_pred = train.assign(cate=f_learner.predict(train[X]))\ngain_curve_train = cumulative_gain(train_pred, \"cate\", y=\"converted\", t=\"em1\")\n\n# Baseline ATE\nbaseline_ate = sensitivity(test, \"converted\", \"em1\")\n\n# Plot\nfig, ax = create_tufte_figure(figsize=(10, 6))\n\n# Use separate x axes for each curve to handle different lengths\nx_axis_test = np.linspace(0, 100, len(gain_curve_test))\nx_axis_train = np.linspace(0, 100, len(gain_curve_train))\n\nax.plot(x_axis_test, gain_curve_test, color=COLORS['blue'], linewidth=2, label='Test')\nax.plot(x_axis_train, gain_curve_train, color=COLORS['gray'], linewidth=2, label='Train')\nax.plot([0, 100], [0, baseline_ate], linestyle='--', color='black', \n        linewidth=1.5, label='Random (ATE line)')\n\nax.set_xlabel('% of Population Targeted')\nax.set_ylabel('Cumulative Gain')\nax.set_title('F-Learner Cumulative Gain: Email Campaign')\nax.legend(loc='upper left')\n\n# Add annotation\nax.text(50, baseline_ate * 0.7, 'Gap indicates overfitting\\n(Train >> Test)', \n        fontsize=10, alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nBaseline ATE: {baseline_ate:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The F-learner shows:\n",
    "1. **Better than random** on test set (curve above diagonal)\n",
    "2. **Significant overfitting** (train >> test performance)\n",
    "3. **High variance** is the cost of the simple transformation\n",
    "\n",
    "The transformed target $Y^*$ is a noisy estimate of individual treatment effect - this variance propagates to CATE predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine heterogeneity by CATE quintile\ntest_pred['cate_quintile'] = pd.qcut(test_pred['cate'], q=5, labels=['Q1 (Low)', 'Q2', 'Q3', 'Q4', 'Q5 (High)'])\n\n# Actual effect by quintile\nquintile_effects = test_pred.groupby('cate_quintile', observed=True).apply(\n    lambda df: sensitivity(df, 'converted', 'em1'),\n    include_groups=False\n)\n\nfig, ax = create_tufte_figure(figsize=(10, 6))\n\nbars = ax.bar(range(5), quintile_effects.values, color=COLORS['blue'], alpha=0.7, edgecolor='black')\nax.axhline(y=baseline_ate, color='black', linestyle='--', label=f'ATE = {baseline_ate:.4f}')\n\nax.set_xticks(range(5))\nax.set_xticklabels(quintile_effects.index)\nax.set_xlabel('Predicted CATE Quintile')\nax.set_ylabel('Actual Treatment Effect')\nax.set_title('F-Learner: Predicted vs Actual Effect by Quintile')\nax.legend()\n\n# Add values on bars\nfor i, (bar, val) in enumerate(zip(bars, quintile_effects.values)):\n    ax.text(bar.get_x() + bar.get_width()/2, val + 0.005, f'{val:.3f}', \n            ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nActual effect by predicted CATE quintile:\")\nprint(quintile_effects)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **Monotonicity**: Higher predicted CATE \u2192 higher actual effect (model is calibrated)\n",
    "2. **Effect range**: Q5 (high CATE) has meaningfully higher effect than Q1\n",
    "3. **Targeting value**: By targeting Q4-Q5, we can get above-average returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix <a name=\"interview\"></a>\n",
    "\n",
    "### Q1: What is the F-learner and why is it called \"plug-and-play\"?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**F-Learner** (Feature/Fundamental learner) transforms the outcome variable so any regression model can estimate CATE:\n",
    "\n",
    "$$Y^* = Y \\cdot \\frac{T - e(X)}{e(X)(1-e(X))}$$\n",
    "\n",
    "**Plug-and-play** because:\n",
    "1. Transform target once\n",
    "2. Use any ML model (XGBoost, RF, NN, etc.) to predict $Y^*$\n",
    "3. Model predictions = CATE estimates\n",
    "\n",
    "**Pros**: Simple, any model works, no custom loss function  \n",
    "**Cons**: High variance, only binary treatment, prone to overfitting\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Why does the F-learner have high variance?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "The transformed target $Y^*$ is **individually noisy** even though it's unbiased in expectation:\n",
    "\n",
    "$$Var(Y^*) = Var\\left(Y \\cdot \\frac{T - e(X)}{e(X)(1-e(X))}\\right)$$\n",
    "\n",
    "Key issues:\n",
    "1. **Denominator shrinks** when $e(X) \\approx 0$ or $e(X) \\approx 1$ \u2192 extreme weights\n",
    "2. **Sign flip**: Treated get positive, control get negative transformed outcomes\n",
    "3. **No smoothing**: Each unit's $Y^*$ is a noisy individual CATE estimate\n",
    "\n",
    "This variance transfers directly to CATE predictions, causing overfitting.\n",
    "\n",
    "**Mitigation**:\n",
    "- Large sample sizes (variance decreases with $n$)\n",
    "- Regularized base models\n",
    "- Weight trimming (cap extreme propensities)\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: How does the F-learner relate to IPTW?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "Both use propensity-based weighting:\n",
    "\n",
    "**IPTW** for ATE:\n",
    "$$\\hat{\\tau}_{IPTW} = \\frac{1}{n}\\sum_i \\frac{T_i Y_i}{e(X_i)} - \\frac{1}{n}\\sum_i \\frac{(1-T_i)Y_i}{1-e(X_i)}$$\n",
    "\n",
    "**F-learner** target:\n",
    "$$Y^* = Y \\cdot \\frac{T - e(X)}{e(X)(1-e(X))}$$\n",
    "\n",
    "Connection:\n",
    "- F-learner is like \"per-unit IPTW\" that estimates individual effects\n",
    "- $E[Y^*] = $ ATE (just like IPTW)\n",
    "- $E[Y^*|X] = $ CATE (F-learner predicts conditional version)\n",
    "\n",
    "Both suffer from propensity extremity issues.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References <a name=\"references\"></a>\n",
    "\n",
    "1. **Athey, S. & Imbens, G.W.** (2016). *Machine Learning Methods for Estimating Heterogeneous Causal Effects*. Draft.\n",
    "\n",
    "2. **Gutierrez, P. & G\u00e9rardy, J-Y.** (2017). *Causal Inference and Uplift Modeling: A Review of the Literature*. JMLR Workshop Proceedings.\n",
    "\n",
    "3. **K\u00fcnzel, S.R. et al.** (2019). *Metalearners for Estimating Heterogeneous Treatment Effects using Machine Learning*. PNAS.\n",
    "\n",
    "4. **Facure, M.** (2022). *Causal Inference for the Brave and True*, Chapter 20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}