{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models 101: The Prediction Problem\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Chapter 17 | Notebook 1 of 3**\n",
    "\n",
    "This notebook introduces predictive machine learning from a causal inference perspective,\n",
    "emphasizing the distinction between prediction ($E[Y|X]$) and causal effects ($E[Y_1 - Y_0]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from augmented.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### From Causal Effects to Predictions\n",
    "\n",
    "In Part I of this book, we focused on **causal questions**:\n",
    "- What is the average treatment effect? $E[Y_1 - Y_0]$\n",
    "- Does the treatment cause the outcome?\n",
    "\n",
    "Now we shift to **predictive questions**:\n",
    "- Given features $X$, what outcome do we expect? $E[Y|X]$\n",
    "- Can we forecast future values?\n",
    "\n",
    "> **Key Distinction**: Prediction asks \"what will happen?\" while causation asks \"what would happen if we intervene?\"\n",
    "\n",
    "### Machine Learning = Powerful Prediction\n",
    "\n",
    "Machine learning is fundamentally about **estimating conditional expectations**:\n",
    "\n",
    "$$\\hat{f}(X) \\approx E[Y|X]$$\n",
    "\n",
    "The ML algorithm learns the mapping from inputs $X$ to outputs $Y$ by minimizing prediction error\n",
    "on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Framing: Customer Profitability\n",
    "\n",
    "Consider a customer acquisition problem:\n",
    "\n",
    "1. **Cost of Acquisition (CAC)**: Marketing, onboarding, installation\n",
    "2. **Revenue Stream**: Monthly payments, purchases\n",
    "3. **Maintenance Costs**: Support, service\n",
    "4. **Net Value**: Revenue - Costs\n",
    "\n",
    "**The Prediction Problem**: Given customer features (age, region, income), predict their net value.\n",
    "\n",
    "**The Business Decision**: Engage only with customers predicted to be profitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Prediction vs Causation\n",
    "\n",
    "| Aspect | Prediction | Causation |\n",
    "|--------|------------|----------|\n",
    "| Question | $E[Y|X]$ | $E[Y|do(T)]$ |\n",
    "| Goal | Forecast | Intervention effect |\n",
    "| Data need | $(X, Y)$ pairs | Exogenous variation |\n",
    "| Confounding | Exploited | Controlled |\n",
    "| Example | \"What's this customer's value?\" | \"Will a discount increase purchases?\" |\n",
    "\n",
    "### 2.2 The ML Objective\n",
    "\n",
    "Machine learning minimizes **expected loss** over the data distribution:\n",
    "\n",
    "$$\\hat{f} = \\arg\\min_f E_{X,Y}[L(Y, f(X))]$$\n",
    "\n",
    "For regression with squared loss:\n",
    "\n",
    "$$\\hat{f} = \\arg\\min_f E[(Y - f(X))^2]$$\n",
    "\n",
    "The solution is the **conditional expectation**:\n",
    "\n",
    "$$\\hat{f}(X) = E[Y|X]$$\n",
    "\n",
    "This is why ML \"learns\" the conditional mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bias-Variance Tradeoff\n",
    "\n",
    "For any estimator $\\hat{f}$, the expected squared error decomposes:\n",
    "\n",
    "$$E[(Y - \\hat{f}(X))^2] = \\underbrace{\\text{Bias}^2}_{\\text{systematic error}} + \\underbrace{\\text{Variance}}_{\\text{estimation noise}} + \\underbrace{\\sigma^2}_{\\text{irreducible}}$$\n",
    "\n",
    "Where:\n",
    "- **Bias**: $E[\\hat{f}(X)] - f(X)$ (wrong on average)\n",
    "- **Variance**: $E[(\\hat{f}(X) - E[\\hat{f}(X)])^2]$ (sensitive to training data)\n",
    "\n",
    "**The Tradeoff**:\n",
    "- Simple models → High bias, low variance (underfitting)\n",
    "- Complex models → Low bias, high variance (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction data\n",
    "transactions = load_facure_data(\"customer_transactions.csv\")\n",
    "print(f\"Transaction data shape: {transactions.shape}\")\n",
    "print(f\"Columns: {transactions.columns[:5].tolist()} ... (30 days of transactions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute net value per customer\n",
    "profitable = transactions[['customer_id']].assign(\n",
    "    net_value=transactions.drop(columns='customer_id').sum(axis=1)\n",
    ")\n",
    "\n",
    "# Load customer features and merge\n",
    "customer_features = load_facure_data(\"customer_features.csv\").merge(\n",
    "    profitable, on='customer_id'\n",
    ")\n",
    "\n",
    "print(f\"Customer features shape: {customer_features.shape}\")\n",
    "customer_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of net value\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "ax.hist(customer_features['net_value'], bins=50, color=COLORS['blue'], alpha=0.7, edgecolor='black')\n",
    "ax.axvline(0, color='red', linestyle='--', label='Break-even')\n",
    "ax.axvline(customer_features['net_value'].mean(), color='green', linestyle='-', \n",
    "           label=f\"Mean: {customer_features['net_value'].mean():.1f}\")\n",
    "ax.set_xlabel('Net Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Customer Net Value')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Profitable vs unprofitable\n",
    "ax = axes[1]\n",
    "profitable_pct = (customer_features['net_value'] > 0).mean() * 100\n",
    "ax.bar(['Unprofitable\\n(net < 0)', 'Profitable\\n(net > 0)'], \n",
    "       [100 - profitable_pct, profitable_pct],\n",
    "       color=[COLORS['red'], COLORS['green']], alpha=0.7)\n",
    "ax.set_ylabel('Percentage of Customers')\n",
    "ax.set_title('Customer Profitability Split')\n",
    "for i, v in enumerate([100 - profitable_pct, profitable_pct]):\n",
    "    ax.text(i, v + 1, f'{v:.1f}%', ha='center')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average net value: {customer_features['net_value'].mean():.2f}\")\n",
    "print(f\"Profitable customers: {profitable_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Policy: Engage Everyone\n",
    "\n",
    "If average net value is negative, engaging with all customers loses money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "train, test = train_test_split(customer_features, test_size=0.3, random_state=13)\n",
    "\n",
    "print(f\"Training set: {train.shape[0]} customers\")\n",
    "print(f\"Test set: {test.shape[0]} customers\")\n",
    "print(f\"\\nTraining set average net value: {train['net_value'].mean():.2f}\")\n",
    "print(f\"Test set average net value: {test['net_value'].mean():.2f}\")\n",
    "\n",
    "if train['net_value'].mean() < 0:\n",
    "    print(\"\\n>>> Engaging with ALL customers would lose money on average!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore net value by features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# By income quantile\n",
    "ax = axes[0]\n",
    "train_income = train.assign(income_q=pd.qcut(train['income'], q=10, labels=False))\n",
    "income_means = train_income.groupby('income_q')['net_value'].mean()\n",
    "ax.bar(income_means.index, income_means.values, color=COLORS['blue'], alpha=0.7)\n",
    "ax.axhline(0, color='red', linestyle='--')\n",
    "ax.set_xlabel('Income Decile')\n",
    "ax.set_ylabel('Average Net Value')\n",
    "ax.set_title('Net Value by Income')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# By age quantile\n",
    "ax = axes[1]\n",
    "train_age = train.assign(age_q=pd.qcut(train['age'], q=10, labels=False, duplicates='drop'))\n",
    "age_means = train_age.groupby('age_q')['net_value'].mean()\n",
    "ax.bar(age_means.index, age_means.values, color=COLORS['green'], alpha=0.7)\n",
    "ax.axhline(0, color='red', linestyle='--')\n",
    "ax.set_xlabel('Age Decile')\n",
    "ax.set_ylabel('Average Net Value')\n",
    "ax.set_title('Net Value by Age')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# By region (top 20)\n",
    "ax = axes[2]\n",
    "region_means = train.groupby('region')['net_value'].mean().sort_values()\n",
    "colors = [COLORS['green'] if v > 0 else COLORS['red'] for v in region_means.values]\n",
    "ax.barh(range(len(region_means)), region_means.values, color=colors, alpha=0.7)\n",
    "ax.axvline(0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('Average Net Value')\n",
    "ax.set_ylabel('Region (sorted)')\n",
    "ax.set_title('Net Value by Region')\n",
    "ax.set_yticks([])\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count profitable regions\n",
    "n_profitable_regions = (region_means > 0).sum()\n",
    "print(f\"Profitable regions: {n_profitable_regions} / {len(region_means)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Region Matters Most\n",
    "\n",
    "Some regions have positive average net value while income/age alone don't separate\n",
    "profitable from unprofitable customers.\n",
    "\n",
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "Before building complex ML models, explore single features.\n",
    "Often, simple rules (like \"only engage in certain regions\")\n",
    "capture most of the predictive value.\n",
    "\n",
    "This is the 80/20 rule applied to ML.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Region-Based Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build region policy from training data\n",
    "region_stats = train.groupby('region')['net_value'].agg(['mean', 'count', 'std'])\n",
    "region_stats['lower_bound'] = (\n",
    "    region_stats['mean'] - 1.96 * region_stats['std'] / np.sqrt(region_stats['count'])\n",
    ")\n",
    "\n",
    "# Regions where lower bound of 95% CI > 0\n",
    "profitable_regions = region_stats[region_stats['lower_bound'] > 0].index.tolist()\n",
    "\n",
    "print(f\"Profitable regions (95% CI lower bound > 0): {len(profitable_regions)}\")\n",
    "print(f\"Regions: {profitable_regions[:10]}...\" if len(profitable_regions) > 10 else f\"Regions: {profitable_regions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate region policy on test set\n",
    "test_region_policy = test[test['region'].isin(profitable_regions)]\n",
    "\n",
    "# Total value if we engage with EVERYONE in test\n",
    "total_value_all = test['net_value'].sum()\n",
    "\n",
    "# Total value with region policy\n",
    "total_value_region = test_region_policy['net_value'].sum()\n",
    "\n",
    "# Per-customer value\n",
    "per_customer_all = test['net_value'].mean()\n",
    "per_customer_region = total_value_region / len(test)  # Normalize by total customers\n",
    "\n",
    "print(f\"Test Set Evaluation:\")\n",
    "print(f\"  Engage ALL: {per_customer_all:.2f} per customer (total: {total_value_all:,.0f})\")\n",
    "print(f\"  Region Policy: {per_customer_region:.2f} per customer (total: {total_value_region:,.0f})\")\n",
    "print(f\"\\nImprovement: {per_customer_region - per_customer_all:.2f} per customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Need for ML\n",
    "\n",
    "The region policy works, but:\n",
    "1. We only used ONE feature\n",
    "2. Interactions (high income in marginal regions) are ignored\n",
    "3. Manual exploration doesn't scale to 100+ features\n",
    "\n",
    "ML automates this by learning $E[Y|X_1, X_2, ..., X_p]$ jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: What's the difference between prediction and causal inference?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Prediction** estimates $E[Y|X]$ - what we expect given observed features\n",
    "   - Can use ANY correlated features, including confounders\n",
    "   - Goal: minimize forecast error\n",
    "\n",
    "2. **Causal inference** estimates $E[Y|do(T)]$ - what happens if we intervene\n",
    "   - Must account for confounding\n",
    "   - Goal: identify intervention effect\n",
    "\n",
    "3. **Example**: Umbrella sales predict rain (correlation), but buying umbrellas\n",
    "   doesn't cause rain (no causal effect).\n",
    "\n",
    "4. **When prediction ≠ causation**:\n",
    "   - Confounders boost prediction but bias causal estimates\n",
    "   - Post-treatment variables good for prediction, bad for causation\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Why is the conditional expectation the optimal predictor under squared loss?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Setup**: Minimize $E[(Y - f(X))^2]$ over all functions $f$\n",
    "\n",
    "2. **Derivation**:\n",
    "   - Take derivative w.r.t. $f(x)$: $-2E[Y - f(X)|X=x] = 0$\n",
    "   - Solve: $f(x) = E[Y|X=x]$\n",
    "\n",
    "3. **Intuition**: The conditional mean minimizes average squared distance\n",
    "   to all possible $Y$ values at each $X$.\n",
    "\n",
    "4. **For other losses**:\n",
    "   - Absolute loss → conditional median\n",
    "   - 0-1 loss → conditional mode\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: When should you use a simple single-feature policy vs ML?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Prefer simple policies when:**\n",
    "- Few features with clear predictive power\n",
    "- Interpretability is critical\n",
    "- Data is limited (ML overfits)\n",
    "- Quick deployment needed\n",
    "\n",
    "**Prefer ML when:**\n",
    "- Many features with complex interactions\n",
    "- Large dataset available\n",
    "- Marginal gains are worth the complexity\n",
    "- Prediction accuracy is paramount\n",
    "\n",
    "**Always start simple**: The 80/20 rule applies to ML.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*.\n",
    "\n",
    "[^2]: Mullainathan, S., & Spiess, J. (2017). Machine Learning: An Applied Econometric Approach.\n",
    "      *Journal of Economic Perspectives*, 31(2), 87-106.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, Chapter 17."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
