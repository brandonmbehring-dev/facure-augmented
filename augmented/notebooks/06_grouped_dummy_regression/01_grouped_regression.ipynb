{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped Data and Weighted Regression\n",
    "\n",
    "**Chapter 6, Section 1**\n",
    "\n",
    "This notebook covers regression with grouped (aggregated) data, heteroskedasticity, and weighted least squares (WLS).\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition) - School size variance paradox\n",
    "2. [Formal Treatment](#formal) - Heteroskedasticity and WLS\n",
    "3. [Numeric Demonstration](#numeric) - ENEM and wage examples\n",
    "4. [Implementation](#implementation) - Using `smf.wls()`\n",
    "5. [Interview Appendix](#interview) - Practice questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from augmented.common import *\n",
    "\n",
    "# Set notebook style\n",
    "set_notebook_style()\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Intuition\n",
    "\n",
    "### The School Size Variance Paradox\n",
    "\n",
    "Consider a simple question: which schools have the highest test scores? If we look at raw data, we might find that **small schools dominate both the top AND the bottom** of the rankings.\n",
    "\n",
    "Why? This is not about school quality—it's about **sampling variance**:\n",
    "\n",
    "- A school with 10 students has high variance in its average score (a few outliers drastically change the mean)\n",
    "- A school with 1,000 students has low variance (outliers are averaged out)\n",
    "\n",
    "This is the **school size paradox**: small samples are more extreme, but this extremity is noise, not signal.\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- Small groups → high variance in group means\n",
    "- Large groups → low variance in group means\n",
    "- Treating all groups equally ignores this fundamental difference\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ENEM school score data\n",
    "enem = load_facure_data(\"enem_scores.csv\")\n",
    "print(f\"ENEM data: {len(enem)} school-year observations\")\n",
    "enem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the variance paradox\n",
    "fig, ax = create_tufte_figure(figsize=(10, 6))\n",
    "\n",
    "# Color by school size\n",
    "scatter = ax.scatter(\n",
    "    enem['number_of_students'], \n",
    "    enem['avg_score'],\n",
    "    c=enem['number_of_students'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=30\n",
    ")\n",
    "\n",
    "set_tufte_title(ax, \"School Size Variance Paradox\")\n",
    "set_tufte_labels(ax, \"Number of Students\", \"Average Score\")\n",
    "\n",
    "# Add annotation for variance pattern\n",
    "ax.annotate(\n",
    "    'High variance\\n(small schools)',\n",
    "    xy=(50, 75), fontsize=9, color=COLORS['gray']\n",
    ")\n",
    "ax.annotate(\n",
    "    'Low variance\\n(large schools)',\n",
    "    xy=(400, 62), fontsize=9, color=COLORS['gray']\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Small schools (left) show much wider spread than large schools (right)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Does This Matter for Causal Inference?\n",
    "\n",
    "When we aggregate data (e.g., by education level, region, or time period), we create groups of different sizes. If we treat all group means equally:\n",
    "\n",
    "1. **Noisy small groups get equal weight** to precise large groups\n",
    "2. **OLS becomes inefficient** (estimates still unbiased, but higher variance)\n",
    "3. **Standard errors are wrong** (ignoring heteroskedasticity)\n",
    "\n",
    "The solution: **Weighted Least Squares (WLS)**—give more weight to more precise observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Formal Treatment\n",
    "\n",
    "### Heteroskedasticity\n",
    "\n",
    "**Definition**: The variance of the error term varies across observations.\n",
    "\n",
    "$$\\text{Var}(\\epsilon_i | X_i) = \\sigma_i^2 \\neq \\sigma^2$$\n",
    "\n",
    "In grouped data, this has a specific form. If observation $i$ is the mean of $n_i$ individuals:\n",
    "\n",
    "$$\\text{Var}(\\bar{\\epsilon}_i) = \\frac{\\sigma^2}{n_i}$$\n",
    "\n",
    "Larger groups have smaller variance in their group means.\n",
    "\n",
    "### OLS Under Heteroskedasticity\n",
    "\n",
    "OLS estimates remain **unbiased** but are:\n",
    "- **Inefficient**: Not the minimum variance estimator\n",
    "- **Standard errors are biased**: Usually underestimate uncertainty for small groups\n",
    "\n",
    "### Weighted Least Squares (WLS)\n",
    "\n",
    "WLS minimizes a weighted sum of squared residuals:\n",
    "\n",
    "$$\\hat{\\beta}_{WLS} = \\arg\\min_\\beta \\sum_{i=1}^{n} w_i (Y_i - X_i'\\beta)^2$$\n",
    "\n",
    "The closed-form solution is:\n",
    "\n",
    "$$\\hat{\\beta}_{WLS} = (X'WX)^{-1}X'WY$$\n",
    "\n",
    "where $W = \\text{diag}(w_1, w_2, ..., w_n)$ is the weight matrix.\n",
    "\n",
    "### Optimal Weights\n",
    "\n",
    "For grouped data where observation $i$ represents the mean of $n_i$ individuals:\n",
    "\n",
    "$$w_i = n_i$$\n",
    "\n",
    "This is the **Gauss-Markov optimal** choice: observations with lower variance (larger groups) get higher weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HC (Heteroskedasticity-Consistent) Standard Errors\n",
    "\n",
    "An alternative to WLS is to use OLS but compute **robust standard errors**:\n",
    "\n",
    "| Estimator | Formula | When to Use |\n",
    "|-----------|---------|-------------|\n",
    "| HC0 (White) | $(X'X)^{-1}(\\sum e_i^2 x_i x_i')(X'X)^{-1}$ | n > 500 |\n",
    "| HC1 | HC0 × $\\frac{n}{n-k}$ | 250 < n < 500 |\n",
    "| HC2 | Leverage-adjusted | General use |\n",
    "| **HC3** | $\\frac{e_i^2}{(1-h_{ii})^2}$ | **n < 250 (recommended)** |\n",
    "\n",
    "Reference: `src/causal_inference/iv/vcov.py:72-119`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Numeric Demonstration\n",
    "\n",
    "### Example 1: Wage Data - Individual vs Grouped\n",
    "\n",
    "Let's compare OLS on individual data vs WLS on grouped data. We'll use the wage dataset to estimate the return to education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wage data\n",
    "wage = load_facure_data(\"wage.csv\")\n",
    "print(f\"Wage data: {len(wage)} individuals\")\n",
    "wage[['lhwage', 'educ', 'IQ']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: OLS on individual data\n",
    "model_individual = smf.ols('lhwage ~ educ', data=wage).fit()\n",
    "print(\"=\" * 60)\n",
    "print(\"Method 1: OLS on Individual Data\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Coefficient on education: {model_individual.params['educ']:.6f}\")\n",
    "print(f\"Standard error: {model_individual.bse['educ']:.6f}\")\n",
    "print(f\"R-squared: {model_individual.rsquared:.4f}\")\n",
    "print(f\"N observations: {model_individual.nobs:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by education level\n",
    "grouped_wage = wage.groupby('educ').agg({\n",
    "    'lhwage': 'mean',\n",
    "    'wage': 'count'  # This gives us the count\n",
    "}).reset_index()\n",
    "grouped_wage.columns = ['educ', 'lhwage', 'count']\n",
    "\n",
    "print(f\"Grouped data: {len(grouped_wage)} education levels\")\n",
    "grouped_wage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: OLS on grouped data (WRONG - ignores heteroskedasticity)\n",
    "model_grouped_ols = smf.ols('lhwage ~ educ', data=grouped_wage).fit()\n",
    "print(\"=\" * 60)\n",
    "print(\"Method 2: OLS on Grouped Data (Incorrect)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Coefficient on education: {model_grouped_ols.params['educ']:.6f}\")\n",
    "print(f\"Standard error: {model_grouped_ols.bse['educ']:.6f}\")\n",
    "print(f\"R-squared: {model_grouped_ols.rsquared:.4f}\")\n",
    "print(f\"N observations: {model_grouped_ols.nobs:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: WLS on grouped data (CORRECT - uses group size as weights)\n",
    "model_grouped_wls = smf.wls('lhwage ~ educ', data=grouped_wage, weights=grouped_wage['count']).fit()\n",
    "print(\"=\" * 60)\n",
    "print(\"Method 3: WLS on Grouped Data (Correct)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Coefficient on education: {model_grouped_wls.params['educ']:.6f}\")\n",
    "print(f\"Standard error: {model_grouped_wls.bse['educ']:.6f}\")\n",
    "print(f\"R-squared: {model_grouped_wls.rsquared:.4f}\")\n",
    "print(f\"Sum of weights: {grouped_wage['count'].sum()} (matches individual N)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON: Education Coefficient Estimates\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['Individual OLS', 'Grouped OLS (wrong)', 'Grouped WLS (correct)'],\n",
    "    'Coefficient': [\n",
    "        model_individual.params['educ'],\n",
    "        model_grouped_ols.params['educ'],\n",
    "        model_grouped_wls.params['educ']\n",
    "    ],\n",
    "    'Std Error': [\n",
    "        model_individual.bse['educ'],\n",
    "        model_grouped_ols.bse['educ'],\n",
    "        model_grouped_wls.bse['educ']\n",
    "    ]\n",
    "})\n",
    "comparison['t-stat'] = comparison['Coefficient'] / comparison['Std Error']\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **Coefficients nearly identical**: OLS on individual data ≈ WLS on grouped data\n",
    "2. **Standard errors differ**: Grouped OLS has wrong SEs; WLS accounts for heteroskedasticity\n",
    "3. **Information loss**: Grouped data loses within-group variation information\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "WLS on grouped data recovers the same point estimate as OLS\n",
    "on individual data, but standard errors may differ because\n",
    "grouping loses within-group variance information.\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weighting\n",
    "fig, axes = create_tufte_figure(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Left: OLS treats all points equally\n",
    "ax = axes[0]\n",
    "ax.scatter(grouped_wage['educ'], grouped_wage['lhwage'], s=50, alpha=0.7, c=COLORS['blue'])\n",
    "x_line = np.linspace(grouped_wage['educ'].min(), grouped_wage['educ'].max(), 100)\n",
    "ax.plot(x_line, model_grouped_ols.params['Intercept'] + model_grouped_ols.params['educ'] * x_line,\n",
    "        color=COLORS['red'], linewidth=2, label='OLS fit')\n",
    "set_tufte_title(ax, \"OLS: All Points Equal Weight\")\n",
    "set_tufte_labels(ax, \"Education (years)\", \"Log Hourly Wage\")\n",
    "\n",
    "# Right: WLS weights by group size\n",
    "ax = axes[1]\n",
    "ax.scatter(grouped_wage['educ'], grouped_wage['lhwage'], \n",
    "           s=grouped_wage['count']/5, alpha=0.7, c=COLORS['blue'])\n",
    "ax.plot(x_line, model_grouped_wls.params['Intercept'] + model_grouped_wls.params['educ'] * x_line,\n",
    "        color=COLORS['green'], linewidth=2, label='WLS fit')\n",
    "set_tufte_title(ax, \"WLS: Size = Group Count\")\n",
    "set_tufte_labels(ax, \"Education (years)\", \"Log Hourly Wage\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Multiple Covariates with Grouped Data\n",
    "\n",
    "When we have multiple covariates, we aggregate them as **means** (not sums or standard deviations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by education with multiple covariates\n",
    "grouped_wage_multi = wage.groupby('educ').agg({\n",
    "    'lhwage': 'mean',\n",
    "    'IQ': 'mean',\n",
    "    'wage': 'count'\n",
    "}).reset_index()\n",
    "grouped_wage_multi.columns = ['educ', 'lhwage', 'IQ', 'count']\n",
    "\n",
    "# Individual OLS with two covariates\n",
    "model_ind_multi = smf.ols('lhwage ~ educ + IQ', data=wage).fit()\n",
    "print(\"Individual OLS with education + IQ:\")\n",
    "print(ols_summary_table(model_ind_multi)[['Coefficient', 'Std. Error']])\n",
    "\n",
    "# WLS on grouped data\n",
    "model_wls_multi = smf.wls('lhwage ~ educ + IQ', data=grouped_wage_multi, \n",
    "                          weights=grouped_wage_multi['count']).fit()\n",
    "print(\"\\nGrouped WLS with education + IQ:\")\n",
    "print(ols_summary_table(model_wls_multi)[['Coefficient', 'Std. Error']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "### Using `smf.wls()` in Statsmodels\n",
    "\n",
    "```python\n",
    "# Basic WLS syntax\n",
    "model = smf.wls('outcome ~ treatment + covariates', \n",
    "                data=grouped_data,\n",
    "                weights=grouped_data['group_size']).fit()\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "1. **Weights should be group sizes** (number of observations per group)\n",
    "2. **Aggregate covariates as means** (not sums)\n",
    "3. **Sum of weights** should approximately equal original sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete implementation example\n",
    "def grouped_wls_regression(individual_data, outcome_col, treatment_col, \n",
    "                           group_col, covariate_cols=None):\n",
    "    \"\"\"\n",
    "    Perform WLS regression on grouped data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    individual_data : pd.DataFrame\n",
    "        Individual-level data.\n",
    "    outcome_col : str\n",
    "        Name of outcome variable.\n",
    "    treatment_col : str\n",
    "        Name of treatment variable.\n",
    "    group_col : str\n",
    "        Variable to group by.\n",
    "    covariate_cols : list, optional\n",
    "        Additional covariates to include.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (WLS results, grouped data)\n",
    "    \"\"\"\n",
    "    # Build aggregation dictionary\n",
    "    agg_dict = {outcome_col: 'mean', treatment_col: 'mean'}\n",
    "    if covariate_cols:\n",
    "        for col in covariate_cols:\n",
    "            agg_dict[col] = 'mean'\n",
    "    agg_dict['__count__'] = (outcome_col, 'count')\n",
    "    \n",
    "    # Aggregate\n",
    "    grouped = individual_data.groupby(group_col).agg(\n",
    "        **{k: (v, 'mean') if isinstance(v, str) else v for k, v in agg_dict.items()}\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Build formula\n",
    "    covs = ' + '.join(covariate_cols) if covariate_cols else ''\n",
    "    formula = f'{outcome_col} ~ {treatment_col}'\n",
    "    if covs:\n",
    "        formula += f' + {covs}'\n",
    "    \n",
    "    # Fit WLS\n",
    "    model = smf.wls(formula, data=grouped, weights=grouped['__count__']).fit()\n",
    "    \n",
    "    return model, grouped\n",
    "\n",
    "# Example usage\n",
    "print(\"Function defined. Use: grouped_wls_regression(data, 'y', 'treatment', 'group')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use WLS vs OLS with Robust SE\n",
    "\n",
    "| Situation | Recommendation |\n",
    "|-----------|----------------|\n",
    "| Grouped data with known group sizes | **WLS** with weights = group sizes |\n",
    "| Individual data, heteroskedasticity suspected | **OLS + HC3** robust SEs |\n",
    "| Individual data, heteroskedasticity form known | **WLS** with appropriate weights |\n",
    "| Individual data, homoskedasticity holds | Standard **OLS** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1: When should you use WLS instead of OLS?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "Use WLS when:\n",
    "1. **You have grouped/aggregated data** where each observation represents the mean of a group\n",
    "2. **Group sizes vary** significantly\n",
    "3. **The form of heteroskedasticity is known** (variance proportional to 1/n_i for grouped data)\n",
    "\n",
    "Key insight: WLS gives more weight to more precise observations (larger groups have smaller variance in their means).\n",
    "\n",
    "The optimal weights are the inverse of the error variance: w_i = 1/Var(ε_i). For grouped data where observation i is the mean of n_i individuals, w_i = n_i.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q2: What happens if you run OLS on grouped data without weights?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "Two problems:\n",
    "\n",
    "1. **Inefficiency**: OLS estimates remain unbiased but have higher variance than WLS. You're \"wasting\" information by treating a precise estimate (large group) the same as an imprecise one (small group).\n",
    "\n",
    "2. **Invalid inference**: Standard errors are biased. OLS assumes homoskedasticity, but grouped data has heteroskedasticity (Var(ε̄_i) = σ²/n_i). The SEs won't correctly reflect uncertainty.\n",
    "\n",
    "Solution: Either use WLS with weights = group sizes, or use OLS with heteroskedasticity-robust (HC) standard errors.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q3: A colleague shows you that small schools have the highest AND lowest test scores. What's your explanation?**\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "This is the **school size variance paradox** (also called the \"small sample size fallacy\").\n",
    "\n",
    "It's a statistical artifact, not a real finding about school quality:\n",
    "- The mean of a small sample has high variance: Var(X̄) = σ²/n\n",
    "- With few students, a school's average is strongly influenced by individual outliers\n",
    "- Large schools' averages regress toward the population mean\n",
    "\n",
    "Implications:\n",
    "- Don't rank schools by raw average scores\n",
    "- Use shrinkage estimators (empirical Bayes) that pull small-sample estimates toward the grand mean\n",
    "- When aggregating, weight by sample size\n",
    "\n",
    "This is why Bill Gates Foundation initially (incorrectly) concluded small schools are better—they were looking at the top of the distribution without noticing small schools also dominated the bottom.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[^1]: Facure, M. (2022). *Causal Inference for the Brave and True*, Chapter 6.\n",
    "\n",
    "[^2]: White, H. (1980). \"A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.\" *Econometrica*, 48(4), 817-838.\n",
    "\n",
    "[^3]: Implementation reference: `src/causal_inference/iv/vcov.py:72-119`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
