{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 22.1 Double/Debiased Machine Learning Intuition\n",
    "\n",
    "**Chapter**: 22 - Debiased/Double Machine Learning  \n",
    "**Section**: 1 - The Core Intuition  \n",
    "**Facure Source**: 22-Debiased-Orthogonal-Machine-Learning.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [The Regularization Bias Problem](#11-the-regularization-bias-problem)\n",
    "   - 1.2 [The FWL Connection](#12-the-fwl-connection)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [The Partially Linear Model](#21-the-partially-linear-model)\n",
    "   - 2.2 [Why Naive ML Fails](#22-why-naive-ml-fails)\n",
    "   - 2.3 [The Double/Debiased Idea](#23-the-doubledebiased-idea)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [Ice Cream Sales Data](#31-ice-cream-sales-data)\n",
    "   - 3.2 [Naive ML vs DML](#32-naive-ml-vs-dml)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, smf,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    ols_summary_table,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: DML bridges ML and causal inference—a hot topic at tech companies. Understanding why naive ML fails and how orthogonalization fixes it shows deep understanding of both fields.\n",
    "\n",
    "### 1.1 The Regularization Bias Problem\n",
    "\n",
    "Facure's key observation: **Machine learning models with regularization introduce bias in causal estimates.**\n",
    "\n",
    "**The problem**:\n",
    "- We want to estimate $\\theta$ in: $Y = \\theta T + g(X) + \\varepsilon$\n",
    "- We use ML to estimate $g(X)$ (confounding function)\n",
    "- ML with regularization (Ridge, Lasso, Trees) biases $\\hat{g}(X)$\n",
    "- This bias \"leaks\" into our estimate of $\\theta$!\n",
    "\n",
    "**Why regularization helps prediction but hurts causal inference**:\n",
    "- Regularization shrinks coefficients toward zero → better out-of-sample prediction\n",
    "- But shrinkage = bias → biased nuisance estimation\n",
    "- Biased nuisance → biased treatment effect\n",
    "\n",
    "### 1.2 The FWL Connection\n",
    "\n",
    "Recall the **Frisch-Waugh-Lovell theorem** (Chapter 5):\n",
    "\n",
    "To estimate $\\theta$ in $Y = \\theta T + \\beta X + \\varepsilon$:\n",
    "1. Regress $Y$ on $X$, get residuals $\\tilde{Y}$\n",
    "2. Regress $T$ on $X$, get residuals $\\tilde{T}$\n",
    "3. Regress $\\tilde{Y}$ on $\\tilde{T}$ → coefficient = $\\theta$\n",
    "\n",
    "**DML insight**: Replace OLS residualization with ML residualization!\n",
    "\n",
    "$$\\tilde{Y} = Y - \\hat{m}(X) \\quad \\text{where } \\hat{m}(X) \\text{ is ML prediction}$$\n",
    "$$\\tilde{T} = T - \\hat{e}(X) \\quad \\text{where } \\hat{e}(X) \\text{ is ML prediction}$$\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- FWL = \"partialling out\" with linear regression\n",
    "- DML = \"partialling out\" with flexible ML\n",
    "- Key difference: Need cross-fitting to avoid overfitting bias\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 The Partially Linear Model\n",
    "\n",
    "**Model**:\n",
    "\n",
    "$$Y = \\theta_0 T + g_0(X) + U, \\quad E[U|X, T] = 0$$\n",
    "$$T = m_0(X) + V, \\quad E[V|X] = 0$$\n",
    "\n",
    "where:\n",
    "- $Y$: Outcome\n",
    "- $T$: Treatment (continuous or binary)\n",
    "- $X$: High-dimensional confounders\n",
    "- $\\theta_0$: Causal effect of interest\n",
    "- $g_0(X)$: Unknown nuisance function (confounding)\n",
    "- $m_0(X)$: Propensity/treatment model\n",
    "\n",
    "**Goal**: Estimate $\\theta_0$ while allowing $g_0$ and $m_0$ to be complex, nonlinear functions estimated by ML.\n",
    "\n",
    "### 2.2 Why Naive ML Fails\n",
    "\n",
    "**Naive approach**: Plug in ML estimates directly.\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{naive}} = \\frac{\\sum_i T_i (Y_i - \\hat{g}(X_i))}{\\sum_i T_i^2}$$\n",
    "\n",
    "**Problem**: $\\hat{g}(X)$ is estimated on the same data, creating bias.\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{naive}} - \\theta_0 \\approx \\underbrace{\\text{(estimation error)}}_{O(n^{-1/2})} + \\underbrace{\\text{(regularization bias)} \\cdot \\text{(estimation error)}}_{\\text{can be } O(1)!}$$\n",
    "\n",
    "The cross-product term can be $O(1)$—it doesn't vanish as $n \\to \\infty$!\n",
    "\n",
    "### 2.3 The Double/Debiased Idea\n",
    "\n",
    "**Chernozhukov et al. (2018)** proposed two key innovations:\n",
    "\n",
    "**1. Neyman Orthogonality**: Use a moment condition that is *insensitive* to small errors in nuisance estimation.\n",
    "\n",
    "**Orthogonal moment condition**:\n",
    "\n",
    "$$\\psi(W; \\theta, \\eta) = (Y - g(X) - \\theta T)(T - m(X))$$\n",
    "\n",
    "where $\\eta = (g, m)$ are nuisance functions.\n",
    "\n",
    "**Property**: The derivative of $E[\\psi]$ with respect to nuisance parameters is zero at the truth:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\eta} E[\\psi(W; \\theta_0, \\eta)]\\bigg|_{\\eta = \\eta_0} = 0$$\n",
    "\n",
    "This means small errors in $\\hat{\\eta}$ have *second-order* effects on $\\hat{\\theta}$.\n",
    "\n",
    "**2. Cross-Fitting**: Estimate nuisance functions on one fold, predict on another.\n",
    "\n",
    "**DML Algorithm**:\n",
    "1. Split data into $K$ folds\n",
    "2. For each fold $k$:\n",
    "   - Train $\\hat{g}_{-k}$ on all data except fold $k$\n",
    "   - Train $\\hat{m}_{-k}$ on all data except fold $k$\n",
    "   - Compute residuals for fold $k$: $\\tilde{Y}_k = Y_k - \\hat{g}_{-k}(X_k)$, $\\tilde{T}_k = T_k - \\hat{m}_{-k}(X_k)$\n",
    "3. Estimate: $\\hat{\\theta} = \\frac{\\sum_i \\tilde{T}_i \\tilde{Y}_i}{\\sum_i \\tilde{T}_i^2}$\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- Orthogonality: Makes $\\hat{\\theta}$ insensitive to nuisance errors\n",
    "- Cross-fitting: Prevents overfitting bias from ML\n",
    "- Together: $\\sqrt{n}$-consistent and asymptotically normal\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 Ice Cream Sales Data\n",
    "\n",
    "Facure's example: **Effect of price on ice cream sales**, with temperature as a confounder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ice cream sales data\n",
    "ice_cream = load_facure_data('ice_cream_sales.csv')\n",
    "ice_cream_rnd = load_facure_data('ice_cream_sales_rnd.csv')\n",
    "\n",
    "print(f\"Observational data: {len(ice_cream)} observations\")\n",
    "print(f\"Randomized data: {len(ice_cream_rnd)} observations\")\n",
    "print(f\"\\nVariables:\")\n",
    "print(f\"  - temp: Temperature (confounder)\")\n",
    "print(f\"  - weekday: Day of week\")\n",
    "print(f\"  - cost: Production cost\")\n",
    "print(f\"  - price: Price (treatment)\")\n",
    "print(f\"  - sales: Sales volume (outcome)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "ice_cream.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confounding problem\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Naive correlation (confounded)\n",
    "ax = axes[0]\n",
    "ax.scatter(ice_cream['price'], ice_cream['sales'], \n",
    "           c=ice_cream['temp'], cmap='coolwarm', s=20, alpha=0.5)\n",
    "ax.set_xlabel('Price')\n",
    "ax.set_ylabel('Sales')\n",
    "ax.set_title('(a) Observational: Price vs Sales\\n(color = temperature)')\n",
    "\n",
    "# Add naive regression line\n",
    "naive_model = smf.ols('sales ~ price', data=ice_cream).fit()\n",
    "price_range = np.linspace(ice_cream['price'].min(), ice_cream['price'].max(), 100)\n",
    "ax.plot(price_range, naive_model.predict(pd.DataFrame({'price': price_range})),\n",
    "        color='black', linewidth=2, linestyle='--', label=f'Naive: β={naive_model.params[\"price\"]:.2f}')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: Randomized data (unconfounded)\n",
    "ax = axes[1]\n",
    "ax.scatter(ice_cream_rnd['price'], ice_cream_rnd['sales'], \n",
    "           c=ice_cream_rnd['temp'], cmap='coolwarm', s=20, alpha=0.5)\n",
    "ax.set_xlabel('Price')\n",
    "ax.set_ylabel('Sales')\n",
    "ax.set_title('(b) Randomized: Price vs Sales\\n(color = temperature)')\n",
    "\n",
    "# Add true regression line\n",
    "rnd_model = smf.ols('sales ~ price', data=ice_cream_rnd).fit()\n",
    "ax.plot(price_range, rnd_model.predict(pd.DataFrame({'price': price_range})),\n",
    "        color=TUFTE_PALETTE['effect'], linewidth=2, label=f'True: β={rnd_model.params[\"price\"]:.2f}')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNaive estimate (confounded): {naive_model.params['price']:.4f}\")\n",
    "print(f\"True effect (from RCT): {rnd_model.params['price']:.4f}\")\n",
    "print(f\"Bias: {naive_model.params['price'] - rnd_model.params['price']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "**The confounding mechanism**:\n",
    "- Hot days → Higher ice cream demand → Higher prices\n",
    "- Hot days → More people out → Higher sales (even at same price)\n",
    "- Temperature confounds price-sales relationship\n",
    "- Naive regression shows POSITIVE price-sales relationship (wrong!)\n",
    "\n",
    "### 3.2 Naive ML vs DML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = ice_cream[['temp', 'weekday', 'cost']].values\n",
    "T = ice_cream['price'].values\n",
    "Y = ice_cream['sales'].values\n",
    "\n",
    "# True effect from RCT\n",
    "true_effect = rnd_model.params['price']\n",
    "\n",
    "print(\"COMPARING ESTIMATORS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True effect (from RCT): {true_effect:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Naive OLS (no controls)\n",
    "naive_ols = smf.ols('sales ~ price', data=ice_cream).fit()\n",
    "print(f\"\\n1. NAIVE OLS (no controls):\")\n",
    "print(f\"   Estimate: {naive_ols.params['price']:.4f}\")\n",
    "print(f\"   Bias: {naive_ols.params['price'] - true_effect:.4f}\")\n",
    "print(f\"   ⚠️ Massive positive bias (confounding!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. OLS with controls\n",
    "ols_controls = smf.ols('sales ~ price + temp + weekday + cost', data=ice_cream).fit()\n",
    "print(f\"\\n2. OLS WITH LINEAR CONTROLS:\")\n",
    "print(f\"   Estimate: {ols_controls.params['price']:.4f}\")\n",
    "print(f\"   Bias: {ols_controls.params['price'] - true_effect:.4f}\")\n",
    "print(f\"   ✓ Much better, but assumes linear confounding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# 3. Double/Debiased Machine Learning\ndef dml_partial_linear(Y, T, X, ml_model, n_folds=5):\n    \"\"\"\n    DML for partially linear model.\n    \n    Y = θ*T + g(X) + ε\n    T = m(X) + v\n    \n    Uses cross-fitting to avoid overfitting bias.\n    \"\"\"\n    n = len(Y)\n    \n    # Cross-fitted predictions\n    Y_pred = cross_val_predict(ml_model, X, Y, cv=n_folds)\n    T_pred = cross_val_predict(ml_model, X, T, cv=n_folds)\n    \n    # Residuals\n    Y_res = Y - Y_pred\n    T_res = T - T_pred\n    \n    # DML estimate (orthogonal moment)\n    theta_dml = np.sum(T_res * Y_res) / np.sum(T_res ** 2)\n    \n    # Standard error\n    residuals = Y_res - theta_dml * T_res\n    var_theta = np.sum(residuals**2 * T_res**2) / (np.sum(T_res**2))**2\n    se_dml = np.sqrt(var_theta)\n    \n    return {\n        'theta': theta_dml,\n        'se': se_dml,\n        'Y_residual': Y_res,\n        'T_residual': T_res\n    }\n\n# Run DML with gradient boosting\nml_model = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\ndml_result = dml_partial_linear(Y, T, X, ml_model, n_folds=5)\n\nprint(f\"\\n3. DOUBLE/DEBIASED ML (Gradient Boosting):\")\nprint(f\"   Estimate: {dml_result['theta']:.4f}\")\nprint(f\"   SE: {dml_result['se']:.4f}\")\nprint(f\"   95% CI: [{dml_result['theta'] - 1.96*dml_result['se']:.4f}, {dml_result['theta'] + 1.96*dml_result['se']:.4f}]\")\nprint(f\"   Bias: {dml_result['theta'] - true_effect:.4f}\")\nprint(f\"   ✓ Flexible nonlinear adjustment + valid standard errors\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY: ESTIMATOR COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<35} {'Estimate':>10} {'Bias':>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'True effect (RCT)':<35} {true_effect:>10.4f} {0:>10.4f}\")\n",
    "print(f\"{'Naive OLS (no controls)':<35} {naive_ols.params['price']:>10.4f} {naive_ols.params['price'] - true_effect:>10.4f}\")\n",
    "print(f\"{'OLS with linear controls':<35} {ols_controls.params['price']:>10.4f} {ols_controls.params['price'] - true_effect:>10.4f}\")\n",
    "print(f\"{'DML (Gradient Boosting)':<35} {dml_result['theta']:>10.4f} {dml_result['theta'] - true_effect:>10.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DML residualization\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Residualized outcome vs treatment\n",
    "ax = axes[0]\n",
    "ax.scatter(dml_result['T_residual'], dml_result['Y_residual'], \n",
    "           c=TUFTE_PALETTE['secondary'], s=20, alpha=0.5)\n",
    "\n",
    "# Add regression line\n",
    "t_range = np.linspace(dml_result['T_residual'].min(), dml_result['T_residual'].max(), 100)\n",
    "ax.plot(t_range, dml_result['theta'] * t_range, \n",
    "        color=TUFTE_PALETTE['effect'], linewidth=2.5,\n",
    "        label=f'DML: β = {dml_result[\"theta\"]:.4f}')\n",
    "\n",
    "ax.axhline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.axvline(0, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.set_xlabel('Residualized Treatment (T̃)')\n",
    "ax.set_ylabel('Residualized Outcome (Ỹ)')\n",
    "ax.set_title('(a) DML: Partialled-Out Relationship')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: Comparison bar chart\n",
    "ax = axes[1]\n",
    "methods = ['Naive\\nOLS', 'OLS +\\nControls', 'DML', 'True\\n(RCT)']\n",
    "estimates = [naive_ols.params['price'], ols_controls.params['price'], \n",
    "             dml_result['theta'], true_effect]\n",
    "colors = [TUFTE_PALETTE['secondary'], TUFTE_PALETTE['secondary'], \n",
    "          TUFTE_PALETTE['effect'], TUFTE_PALETTE['treatment']]\n",
    "\n",
    "bars = ax.bar(methods, estimates, color=colors, edgecolor='white', linewidth=1.5)\n",
    "ax.axhline(true_effect, color=TUFTE_PALETTE['treatment'], linestyle='--', \n",
    "           linewidth=1.5, label='True effect')\n",
    "ax.axhline(0, color=TUFTE_PALETTE['spine'], linewidth=1)\n",
    "\n",
    "ax.set_ylabel('Estimated Price Effect')\n",
    "ax.set_title('(b) Estimator Comparison')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, estimates):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, height + 1, \n",
    "            f'{val:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "**Key takeaways**:\n",
    "\n",
    "1. **Naive OLS**: Massive bias from confounding\n",
    "2. **OLS with controls**: Better, but assumes linearity\n",
    "3. **DML**: Flexible ML + valid inference → closest to truth\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- DML = FWL with ML residualization + cross-fitting\n",
    "- Handles complex, nonlinear confounding\n",
    "- Maintains valid statistical inference\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "---\n\n## 4. Production Implementation\n\nThis method is implemented in the `causal_inference_mastery` library:\n\n```python\nfrom causal_inference.cate.dml import DoubleML, DMLResult\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n\n# DML for partially linear model\ndml = DoubleML(\n    model_y=GradientBoostingRegressor(n_estimators=100, max_depth=3),\n    model_t=GradientBoostingRegressor(n_estimators=100, max_depth=3),\n    n_folds=5,\n    n_rep=10,  # Repeated cross-fitting for stability\n    random_state=42\n)\n\nresult = dml.fit(\n    outcome=data['Y'],\n    treatment=data['T'],\n    covariates=data[['X1', 'X2', 'X3']]\n)\n\nprint(f\"DML estimate: {result.theta:.4f}\")\nprint(f\"SE: {result.se:.4f}\")\nprint(f\"95% CI: [{result.ci_lower:.4f}, {result.ci_upper:.4f}]\")\nprint(f\"First-stage R² (Y): {result.r2_y:.3f}\")\nprint(f\"First-stage R² (T): {result.r2_t:.3f}\")\n\n# Also see econml for production use:\n# from econml.dml import LinearDML\n# dml = LinearDML(model_y=..., model_t=..., cv=5)\n# dml.fit(Y, T, X=X)\n# print(dml.effect(X))\n```\n\n**Key differences from notebook demo**:\n- Production code implements doubly robust variant (AIPW)\n- Median aggregation across repeated cross-fits for robustness\n- First-stage diagnostics (R², partial correlations)\n- Supports continuous and binary treatments\n- Cross-validated with Julia implementation\n\n**See also**: `causal_inference_mastery/src/causal_inference/cate/dml.py`\n\n**External reference**: For production HTE applications, consider [econml](https://github.com/py-why/EconML) which provides optimized DML implementations with GPU support."
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "---\n\n## 5. Interview Appendix\n\n### Practice Questions\n\n**Q1 (Meta E5, DS)**: *\"Why can't you just use a flexible ML model to estimate causal effects directly?\"*\n\n<details>\n<summary>Solution</summary>\n\n**Three problems with naive ML for causal inference**:\n\n1. **Regularization bias**:\n   - ML models use regularization (L1, L2, tree pruning)\n   - Regularization = bias toward simpler models\n   - This bias affects the treatment coefficient\n   - Unlike prediction, causal inference requires unbiased point estimates\n\n2. **Overfitting in nuisance estimation**:\n   - ML models fit the training data well (including noise)\n   - When you use same data to estimate treatment effect, noise correlates\n   - Creates bias that doesn't vanish with more data\n\n3. **Invalid inference**:\n   - Standard ML doesn't provide valid confidence intervals for treatment effects\n   - Need special structure (orthogonality) for valid inference\n\n**DML solution**:\n- Orthogonal moments: Makes estimates insensitive to nuisance errors\n- Cross-fitting: Breaks correlation between nuisance estimation and treatment effect\n- Result: $\\sqrt{n}$-consistent, asymptotically normal, valid CIs\n\n</details>\n\n---\n\n**Q2 (Google L5, Quant)**: *\"What is Neyman orthogonality and why does it matter for DML?\"*\n\n<details>\n<summary>Solution</summary>\n\n**Neyman orthogonality**: A moment condition $\\psi(W; \\theta, \\eta)$ is Neyman orthogonal if:\n\n$$\\frac{\\partial}{\\partial \\eta} E[\\psi(W; \\theta_0, \\eta)]\\bigg|_{\\eta = \\eta_0} = 0$$\n\nIn words: The expected moment is *flat* with respect to nuisance parameters at the true values.\n\n**Why it matters**:\n\n1. **First-order insensitivity**:\n   - Small errors in nuisance estimation $\\hat{\\eta} - \\eta_0$ have second-order effects on $\\hat{\\theta}$\n   - $\\hat{\\theta} - \\theta_0 \\approx O(||\\hat{\\eta} - \\eta_0||^2)$ instead of $O(||\\hat{\\eta} - \\eta_0||)$\n\n2. **Slower nuisance convergence is OK**:\n   - ML estimates converge slower than $\\sqrt{n}$ (typically $n^{-1/4}$ for nonparametric)\n   - Orthogonality means: $n^{-1/4} \\times n^{-1/4} = n^{-1/2}$\n   - Treatment effect still has $\\sqrt{n}$ convergence!\n\n3. **Valid inference**:\n   - Asymptotic normality preserved\n   - Standard errors and confidence intervals are valid\n\n**Example moment**:\n$$\\psi = (Y - g(X) - \\theta T)(T - m(X))$$\n\nThis is orthogonal because errors in $g$ are multiplied by $(T - m(X))$ which has mean zero, and vice versa.\n\n</details>\n\n---\n\n**Q3 (Amazon L6, Econ)**: *\"Explain cross-fitting. Why is it necessary for DML?\"*\n\n<details>\n<summary>Solution</summary>\n\n**Cross-fitting**: Use separate data folds for nuisance estimation and treatment effect estimation.\n\n**Algorithm**:\n1. Split data into $K$ folds (typically $K=5$)\n2. For each fold $k$:\n   - Train ML models on folds $\\{1, ..., K\\} \\setminus \\{k\\}$\n   - Predict on fold $k$\n3. Use all cross-fitted predictions to estimate $\\theta$\n\n**Why necessary**:\n\n1. **Overfitting bias**:\n   - Without cross-fitting: $\\hat{g}(X_i)$ is trained on data including $(X_i, Y_i)$\n   - This creates correlation between $\\hat{g}(X_i)$ and $\\varepsilon_i$\n   - Bias doesn't vanish as $n \\to \\infty$\n\n2. **With cross-fitting**:\n   - $\\hat{g}_{-k}(X_i)$ is trained WITHOUT observation $i$\n   - No correlation between prediction and residual\n   - Removes overfitting bias\n\n3. **Sample splitting alternative**:\n   - Could use half for nuisance, half for estimation\n   - But wastes half the data!\n   - Cross-fitting uses all data efficiently\n\n**Key insight**: Cross-fitting is like cross-validation, but for causal inference instead of prediction accuracy.\n\n</details>\n\n---\n\n**Q4 (Two Sigma, Quant)**: *\"How do you tune ML hyperparameters in DML without leaking causal information?\"*\n\n<details>\n<summary>Solution</summary>\n\n**The problem**: Hyperparameter tuning uses the outcome $Y$, which could leak causal information if done naively.\n\n**Safe approaches**:\n\n1. **Nested cross-validation**:\n   - Outer loop: Cross-fitting folds for DML\n   - Inner loop: Hyperparameter tuning within training folds\n   - Guarantees no data leakage\n\n2. **Pre-specified hyperparameters**:\n   - Use defaults or theory-guided choices\n   - Avoids data-driven selection entirely\n   - Simpler but may sacrifice performance\n\n3. **Tune on prediction task only**:\n   - Tune nuisance models to minimize prediction error\n   - NOT to maximize treatment effect significance\n   - Prediction accuracy ≠ causal accuracy, but it's principled\n\n**What NOT to do**:\n- Tune to maximize t-statistic on treatment\n- Use same validation set for tuning and final estimation\n- Cherry-pick model based on \"reasonable\" effect size\n\n**Implementation example**:\n```python\nfrom sklearn.model_selection import GridSearchCV\n\n# Nested CV: tune within each DML fold\nfor k in range(K):\n    train_idx = folds != k\n    \n    # Inner CV for hyperparameter tuning\n    grid_search = GridSearchCV(\n        GradientBoostingRegressor(),\n        param_grid={'max_depth': [3, 5, 7]},\n        cv=3  # Inner folds\n    )\n    grid_search.fit(X[train_idx], Y[train_idx])\n    best_model = grid_search.best_estimator_\n```\n\n**One-liner**: \"Tune within training folds using nested CV—never let test-fold outcomes influence hyperparameters.\"\n\n</details>\n\n---\n\n**Q5 (Citadel, Quant)**: *\"When would you prefer DML over traditional IV or propensity score matching?\"*\n\n<details>\n<summary>Solution</summary>\n\n**DML advantages over IV**:\n\n| Scenario | DML Preferred | IV Preferred |\n|----------|---------------|--------------|\n| No instrument available | ✓ | ✗ |\n| High-dimensional confounders | ✓ | Possible with regularization |\n| Nonlinear confounding | ✓ | Requires correct specification |\n| Unobserved confounders | ✗ | ✓ (with valid instrument) |\n\n**DML advantages over PSM**:\n\n| Scenario | DML Preferred | PSM Preferred |\n|----------|---------------|---------------|\n| Continuous treatment | ✓ | Limited |\n| High-dimensional X | ✓ | Curse of dimensionality |\n| Nonlinear outcome model | ✓ (doubly robust) | Outcome model matters |\n| Extreme propensities | ✓ (regularized) | Trimming needed |\n\n**When to use each**:\n\n1. **Use DML when**:\n   - Selection on observables plausible\n   - Many potential confounders\n   - Flexible functional forms needed\n   - Want valid inference with ML\n\n2. **Use IV when**:\n   - Unobserved confounders are the concern\n   - Valid instrument available\n   - LATE is acceptable\n\n3. **Use PSM when**:\n   - Low-dimensional settings\n   - Interpretability matters (matched pairs)\n   - Balance diagnostics are priority\n\n**DML's unique value**:\n- Combines ML flexibility with valid statistical inference\n- Doubly robust: consistent if EITHER outcome OR propensity model is correct\n- Modern: leverages advances in ML while maintaining econometric rigor\n\n**One-liner**: \"DML is your go-to for selection-on-observables with complex confounding; IV when unobserved confounders dominate; PSM for simple, interpretable settings.\"\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 22: \"Debiased/Orthogonal Machine Learning.\"\n",
    "\n",
    "[^2]: Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^3]: Robinson, P. M. (1988). Root-N-Consistent Semiparametric Regression. *Econometrica*, 56(4), 931-954.\n",
    "\n",
    "[^4]: Neyman, J. (1959). Optimal Asymptotic Tests of Composite Statistical Hypotheses. *Probability and Statistics*, 213-234.\n",
    "\n",
    "[^5]: Athey, S. and Imbens, G. W. (2019). Machine Learning Methods That Economists Should Know About. *Annual Review of Economics*, 11, 685-725.\n",
    "\n",
    "---\n",
    "\n",
    "**Precision Improvement:**\n",
    "- You said: \"Build Chapter 22 DML notebooks\"\n",
    "- Concise: \"Build 01_dml_intuition.ipynb\"\n",
    "- Precise: `/facure_augment 22.1 --dml-intro --fwl-connection --naive-vs-dml`\n",
    "- Pattern: [build] [target] [content-flags]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}