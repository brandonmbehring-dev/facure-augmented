{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.3 Regression for Non-Random Data\n",
    "\n",
    "**Chapter**: 5 - The Unreasonable Effectiveness of Linear Regression  \n",
    "**Section**: 3 - Regression For Non-Random Data  \n",
    "**Facure Source**: 05-The-Unreasonable-Effectiveness-of-Linear-Regression.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-09\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [The Problem with Observational Data](#11-the-problem-with-observational-data)\n",
    "   - 1.2 [Controls as Conditional Randomization](#12-controls-as-conditional-randomization)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [Conditional Ignorability](#21-conditional-ignorability)\n",
    "   - 2.2 [Why Adding Controls Helps](#22-why-adding-controls-helps)\n",
    "3. [Numeric Demonstration](#3-numeric-demonstration)\n",
    "   - 3.1 [The Wage-Education Example](#31-the-wage-education-example)\n",
    "   - 3.2 [Bias Reduction with Controls](#32-bias-reduction-with-controls)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    ols_summary_table,\n",
    "    compare_coefficients,\n",
    "    create_tufte_figure,\n",
    "    TUFTE_PALETTE,\n",
    ")\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "set_notebook_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: Most real-world causal questions involve observational data. Interviewers test whether you understand *when* regression works and *why* controls help.\n",
    "\n",
    "### 1.1 The Problem with Observational Data\n",
    "\n",
    "As Facure emphasizes, randomized experiments are the gold standard but often infeasible:\n",
    "\n",
    "- You can't randomly assign years of education\n",
    "- You can't randomly assign McKinsey consultants to firms\n",
    "- Ethical and practical constraints abound\n",
    "\n",
    "With observational data, **selection bias** creeps in. People who get more education:\n",
    "- May have higher IQ\n",
    "- Likely have wealthier parents\n",
    "- May be more ambitious\n",
    "\n",
    "These factors affect wages *independently* of education itself.\n",
    "\n",
    "### 1.2 Controls as Conditional Randomization\n",
    "\n",
    "Facure's insight: **regression with controls creates a form of conditional randomization**.\n",
    "\n",
    "Recall from Section 02 (FWL Theorem):\n",
    "\n",
    "$$\\hat{\\kappa} = \\frac{\\text{Cov}(Y, \\tilde{T})}{\\text{Var}(\\tilde{T})}$$\n",
    "\n",
    "where $\\tilde{T}$ are residuals from regressing $T$ on controls $X$.\n",
    "\n",
    "**Key insight**: $\\tilde{T}$ is the part of treatment that *cannot be predicted* by the controls. If we've controlled for all confounders, this residual variation is \"as good as random.\"\n",
    "\n",
    "★ Insight ─────────────────────────────────────\n",
    "- Controls \"partial out\" confounding variation\n",
    "- The coefficient reflects treatment effect *holding controls fixed*\n",
    "- This works only if all confounders are observed and included\n",
    "─────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 Conditional Ignorability\n",
    "\n",
    "**Assumption (Conditional Ignorability / Selection on Observables)**:\n",
    "\n",
    "$$(Y_0, Y_1) \\perp T \\mid X$$\n",
    "\n",
    "Given covariates $X$, treatment assignment is independent of potential outcomes.\n",
    "\n",
    "**Interpretation**: Within strata defined by $X$, treatment is \"as good as random.\"\n",
    "\n",
    "**Alternative formulation (Mean Independence)**:\n",
    "\n",
    "$$E[Y_0 | T, X] = E[Y_0 | X]$$\n",
    "\n",
    "The expected potential outcome under control doesn't depend on treatment, once we condition on $X$.\n",
    "\n",
    "### 2.2 Why Adding Controls Helps\n",
    "\n",
    "Consider the linear model:\n",
    "\n",
    "$$Y_i = \\beta_0 + \\kappa T_i + X_i'\\gamma + u_i$$\n",
    "\n",
    "**Proposition**: Under conditional ignorability, if the outcome model is correctly specified, $\\hat{\\kappa}$ is consistent for the ATE.\n",
    "\n",
    "**Proof sketch**:\n",
    "1. By FWL, $\\hat{\\kappa}$ is the coefficient from regressing $\\tilde{Y}$ on $\\tilde{T}$\n",
    "2. $\\tilde{T} = T - E[T|X]$ is orthogonal to $X$ by construction\n",
    "3. Under conditional ignorability, $\\tilde{T}$ is also independent of $Y_0, Y_1$\n",
    "4. Therefore, $\\text{Cov}(\\tilde{Y}, \\tilde{T}) = \\text{Cov}(Y_1 - Y_0, \\tilde{T}) \\cdot E[\\tilde{T}^2] / \\text{Var}(\\tilde{T}) = \\kappa$ $\\blacksquare$\n",
    "\n",
    "**Caution**: This requires:\n",
    "- **No unobserved confounders**: All variables affecting both $T$ and $Y$ are in $X$\n",
    "- **Correct functional form**: Linear relationships (or appropriate transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration\n",
    "\n",
    "### 3.1 The Wage-Education Example\n",
    "\n",
    "Facure's classic example: estimating the return to education using wage data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wage data\n",
    "wage = load_facure_data('wage.csv').dropna()\n",
    "\n",
    "# Create log hourly wage\n",
    "wage['lhwage'] = np.log(wage['wage'] / wage['hours'])\n",
    "\n",
    "print(f\"Sample size: n = {len(wage)}\")\n",
    "print(f\"Treatment: educ (years of education)\")\n",
    "print(f\"Outcome: lhwage (log hourly wage)\")\n",
    "print(f\"\\nEducation summary:\")\n",
    "print(wage['educ'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available controls\n",
    "print(\"Available controls:\")\n",
    "controls = ['IQ', 'exper', 'tenure', 'age', 'married', 'black',\n",
    "            'south', 'urban', 'sibs', 'brthord', 'meduc', 'feduc']\n",
    "for i, c in enumerate(controls, 1):\n",
    "    print(f\"  {i:2d}. {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Simple regression (no controls) - BIASED\n",
    "model_simple = smf.ols('lhwage ~ educ', data=wage).fit()\n",
    "\n",
    "print(\"Model 1: Simple Regression (No Controls)\")\n",
    "print(f\"  β_educ = {model_simple.params['educ']:.4f}\")\n",
    "print(f\"  SE     = {model_simple.bse['educ']:.4f}\")\n",
    "print(f\"  95% CI = [{model_simple.conf_int().loc['educ', 0]:.4f}, {model_simple.conf_int().loc['educ', 1]:.4f}]\")\n",
    "print(f\"\\n  Interpretation: Each year of education associated with {model_simple.params['educ']*100:.1f}% higher wages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Full regression (all controls)\n",
    "formula = 'lhwage ~ educ + ' + ' + '.join(controls)\n",
    "model_full = smf.ols(formula, data=wage).fit()\n",
    "\n",
    "print(\"Model 2: Full Regression (All Controls)\")\n",
    "print(f\"  β_educ = {model_full.params['educ']:.4f}\")\n",
    "print(f\"  SE     = {model_full.bse['educ']:.4f}\")\n",
    "print(f\"  95% CI = [{model_full.conf_int().loc['educ', 0]:.4f}, {model_full.conf_int().loc['educ', 1]:.4f}]\")\n",
    "print(f\"\\n  Interpretation: Each year of education associated with {model_full.params['educ']*100:.1f}% higher wages\")\n",
    "print(f\"                  (holding IQ, experience, etc. constant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bias Reduction with Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients\n",
    "bias = model_simple.params['educ'] - model_full.params['educ']\n",
    "\n",
    "print(\"Bias Analysis\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Simple model (biased):   {model_simple.params['educ']:.4f}\")\n",
    "print(f\"Full model (debiased):   {model_full.params['educ']:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Estimated bias:          {bias:.4f}\")\n",
    "print(f\"Bias as % of estimate:   {100*bias/model_full.params['educ']:.1f}%\")\n",
    "print(f\"\")\n",
    "print(f\"Direction: {'Positive bias (overestimate)' if bias > 0 else 'Negative bias'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how coefficient changes as we add controls incrementally\n",
    "control_sets = [\n",
    "    [],\n",
    "    ['IQ'],\n",
    "    ['IQ', 'exper'],\n",
    "    ['IQ', 'exper', 'tenure'],\n",
    "    ['IQ', 'exper', 'tenure', 'meduc', 'feduc'],\n",
    "    controls,  # All\n",
    "]\n",
    "\n",
    "results = []\n",
    "for ctrls in control_sets:\n",
    "    if len(ctrls) == 0:\n",
    "        formula = 'lhwage ~ educ'\n",
    "        label = 'None'\n",
    "    else:\n",
    "        formula = 'lhwage ~ educ + ' + ' + '.join(ctrls)\n",
    "        label = ', '.join(ctrls[:3]) + ('...' if len(ctrls) > 3 else '')\n",
    "    \n",
    "    model = smf.ols(formula, data=wage).fit()\n",
    "    results.append({\n",
    "        'controls': label,\n",
    "        'n_controls': len(ctrls),\n",
    "        'beta_educ': model.params['educ'],\n",
    "        'se': model.bse['educ'],\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Coefficient on Education as Controls Added\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias reduction\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Panel 1: Coefficient trajectory\n",
    "ax = axes[0]\n",
    "ax.plot(results_df['n_controls'], results_df['beta_educ'], 'o-', \n",
    "        color=TUFTE_PALETTE['effect'], markersize=8, linewidth=2)\n",
    "ax.fill_between(results_df['n_controls'], \n",
    "                results_df['beta_educ'] - 1.96*results_df['se'],\n",
    "                results_df['beta_educ'] + 1.96*results_df['se'],\n",
    "                alpha=0.2, color=TUFTE_PALETTE['effect'])\n",
    "ax.axhline(model_full.params['educ'], ls='--', color=TUFTE_PALETTE['secondary'], \n",
    "           label=f'Full model: {model_full.params[\"educ\"]:.3f}')\n",
    "ax.set_xlabel('Number of Controls')\n",
    "ax.set_ylabel('Coefficient on Education')\n",
    "ax.set_title('(a) Bias Reduction as Controls Added')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: Simple vs Full comparison\n",
    "ax = axes[1]\n",
    "models = ['No Controls\\n(Biased)', 'All Controls\\n(Debiased)']\n",
    "coefs = [model_simple.params['educ'], model_full.params['educ']]\n",
    "ses = [model_simple.bse['educ'], model_full.bse['educ']]\n",
    "colors = [TUFTE_PALETTE['bias'], TUFTE_PALETTE['effect']]\n",
    "\n",
    "bars = ax.bar(models, coefs, yerr=[1.96*s for s in ses], \n",
    "              color=colors, capsize=5, width=0.5, edgecolor='white')\n",
    "\n",
    "for bar, coef in zip(bars, coefs):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, coef + 0.003, \n",
    "            f'{coef:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_ylabel('Coefficient on Education')\n",
    "ax.set_title('(b) Effect of Controlling for Confounders')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify FWL connection: partialling out gives same answer\n",
    "X = wage[controls].values\n",
    "X_const = sm.add_constant(X)\n",
    "T = wage['educ'].values\n",
    "Y = wage['lhwage'].values\n",
    "\n",
    "# Residualize\n",
    "T_resid = T - X_const @ np.linalg.lstsq(X_const, T, rcond=None)[0]\n",
    "Y_resid = Y - X_const @ np.linalg.lstsq(X_const, Y, rcond=None)[0]\n",
    "\n",
    "# Bivariate on residuals\n",
    "kappa_fwl = np.cov(Y_resid, T_resid)[0, 1] / np.var(T_resid, ddof=1)\n",
    "\n",
    "print(\"FWL Verification\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Full regression:   {model_full.params['educ']:.10f}\")\n",
    "print(f\"FWL (partialling): {kappa_fwl:.10f}\")\n",
    "print(f\"Match: {np.isclose(model_full.params['educ'], kappa_fwl, rtol=1e-10)}\")\n",
    "\n",
    "assert np.isclose(model_full.params['educ'], kappa_fwl, rtol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "The `causal_inference_mastery` library provides utilities for covariate adjustment:\n",
    "\n",
    "```python\n",
    "# Outcome regression with controls\n",
    "from causal_inference.observational.outcome_regression import (\n",
    "    outcome_regression_ate,\n",
    "    lin_estimator,  # Covariate-adjusted ATE\n",
    ")\n",
    "\n",
    "# With continuous treatment (like education)\n",
    "result = outcome_regression_ate(\n",
    "    outcome=wage['lhwage'],\n",
    "    treatment=wage['educ'],\n",
    "    covariates=wage[controls],\n",
    "    robust_se=True,\n",
    ")\n",
    "```\n",
    "\n",
    "For more sophisticated approaches (propensity scores, doubly robust), see Chapters 10-12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Interview Appendix\n\n### Practice Questions\n\n**Q1 (Meta E5, Economist)**: *\"When does adding controls help identify causal effects? When does it hurt?\"*\n\n<details>\n<summary>Solution</summary>\n\n**When adding controls HELPS:**\n\n1. **Confounders**: Variables that cause both treatment and outcome\n   - Example: IQ affects both education and wages\n   - Controlling for IQ removes this source of bias\n\n2. **Precision**: Pre-treatment predictors of outcome reduce residual variance\n   - Even if not confounders, they can improve efficiency\n\n**When adding controls HURTS:**\n\n1. **Colliders**: Variables caused by both treatment and outcome\n   - Example: Job type is affected by both education and ability\n   - Controlling for job type induces \"collider bias\"\n\n2. **Mediators**: Variables on the causal path from treatment to outcome\n   - Example: Occupation mediates education → wages\n   - Controlling removes the indirect effect, giving only \"direct\" effect\n\n3. **Post-treatment variables**: Any variable affected by treatment\n   - Can introduce bias or change interpretation\n\n**Rule**: Only control for pre-treatment confounders. Draw a DAG first!\n\n</details>\n\n---\n\n**Q2 (Google L5, DS)**: *\"You have observational data on a marketing campaign. What assumptions do you need to estimate the causal effect using regression?\"*\n\n<details>\n<summary>Solution</summary>\n\n**Key assumptions:**\n\n1. **Conditional ignorability** (Selection on observables):\n   - $(Y_0, Y_1) \\perp T | X$\n   - Treatment assignment is random *conditional on covariates*\n   - Requires: all confounders observed and controlled\n\n2. **Overlap (Positivity)**:\n   - $0 < P(T=1|X) < 1$ for all $X$\n   - Every covariate stratum has both treated and control units\n   - Needed for extrapolation to work\n\n3. **SUTVA (Stable Unit Treatment Value)**:\n   - No interference between units\n   - No hidden treatment versions\n\n4. **Correct model specification**:\n   - Linear relationships (or appropriate transformations)\n   - No omitted interactions\n\n**Practical checks:**\n- Compare covariate distributions between treated/control\n- Check for common support (overlap)\n- Sensitivity analysis for unobserved confounders\n\n</details>\n\n---\n\n**Q3 (Two Sigma, Quant)**: *\"Explain the connection between FWL theorem and controlling for confounders.\"*\n\n<details>\n<summary>Solution</summary>\n\n**Connection:**\n\nFWL says: The coefficient on treatment from a multiple regression equals the coefficient from:\n1. Residualize $T$ on controls $X$: get $\\tilde{T} = T - E[T|X]$\n2. Residualize $Y$ on controls $X$: get $\\tilde{Y} = Y - E[Y|X]$\n3. Regress $\\tilde{Y}$ on $\\tilde{T}$\n\n**Causal interpretation:**\n\n- $\\tilde{T}$ is the part of treatment *not explained by confounders*\n- If conditional ignorability holds, this residual variation is \"as good as random\"\n- The coefficient captures the causal effect because we've removed confounding\n\n**Why it matters:**\n\n1. Provides intuition: \"Control for X\" means \"use only the variation in T that X can't explain\"\n\n2. Foundation for DML: Replace linear regression with ML to estimate $E[T|X]$ and $E[Y|X]$ flexibly\n\n3. Diagnostic: If $R^2$ from regressing $T$ on $X$ is very high, there's little residual variation → imprecise estimates\n\n</details>\n\n---\n\n**Q4 (Netflix L6, DS)**: *\"You're running an A/B test on a recommendation algorithm, but a colleague suggests dropping some control variables because they're correlated with treatment assignment. How do you respond?\"*\n\n<details>\n<summary>Solution</summary>\n\n**Key insight**: In a proper A/B test (RCT), treatment is randomly assigned—so there ARE no confounders by design.\n\n**Response to colleague:**\n\n1. **In an RCT, controls don't affect unbiasedness**:\n   - Random assignment ensures $E[\\hat{\\kappa}] = \\kappa$ regardless of controls\n   - Correlation with treatment is spurious (due to finite sample)\n   - Dropping controls won't introduce bias\n\n2. **BUT controls CAN help with precision**:\n   - Pre-treatment predictors of outcome reduce residual variance\n   - ANCOVA/Lin estimator: include controls + treatment-control interactions\n   - Can substantially reduce standard errors\n\n3. **When to actually drop variables**:\n   - Post-treatment variables (mediators, downstream outcomes)\n   - Variables affected by treatment can bias results\n\n**Recommendation**:\n- Keep pre-treatment controls for efficiency\n- Verify they're truly pre-treatment (measured before randomization)\n- Use Lin (2013) estimator for optimal precision gains\n\n**One-liner**: \"In an RCT, controls don't fix bias (there is none), but they can buy you precision.\"\n\n</details>\n\n---\n\n**Q5 (Citadel, Quant)**: *\"Adding more controls always reduces omitted variable bias. True or false?\"*\n\n<details>\n<summary>Solution</summary>\n\n**FALSE.** This is a common misconception with important exceptions.\n\n**When adding controls INCREASES bias:**\n\n1. **Colliders (M-bias)**:\n   ```\n   A → X ← B\n   A → Y\n   B → T → Y\n   ```\n   - X is caused by both A and B\n   - Controlling for X opens a backdoor path A → X ← B → T → Y\n   - Introduces bias where none existed!\n\n2. **Mediators**:\n   - Control absorbs part of the treatment effect\n   - Estimate becomes \"direct effect only\" (may not be what you want)\n\n3. **Proxy confounders with measurement error**:\n   - Imperfectly measured confounder can increase bias vs. omitting\n   - \"Conditioning on a noisy proxy\" problem\n\n4. **Instrumental variable contamination**:\n   - If you accidentally control for an instrument, you lose identifying variation\n\n**Correct statement**: Adding *confounders* reduces OVB. Adding *non-confounders* may increase it.\n\n**Practical advice**:\n- Draw the DAG FIRST\n- Only control for variables that block backdoor paths\n- Never control for descendants of treatment\n\n**One-liner**: \"Controls aren't vitamins—more isn't always better. Bad controls can poison your estimate.\"\n\n</details>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 5: \"Regression For Non-Random Data.\"\n",
    "\n",
    "[^2]: Angrist, J. D. and Pischke, J.-S. (2009). *Mostly Harmless Econometrics*. Princeton University Press, Chapter 3.\n",
    "\n",
    "[^3]: Imbens, G. W. (2004). Nonparametric Estimation of Average Treatment Effects Under Exogeneity: A Review. *Review of Economics and Statistics*, 86(1), 4-29.\n",
    "\n",
    "[^4]: Rosenbaum, P. R. and Rubin, D. B. (1983). The Central Role of the Propensity Score in Observational Studies for Causal Effects. *Biometrika*, 70(1), 41-55.\n",
    "\n",
    "---\n",
    "\n",
    "**Precision Improvement:**\n",
    "- You said: \"Build the nonrandom data section\"\n",
    "- Concise: \"Build 03_regression_nonrandom_data\"\n",
    "- Precise: `/facure_augment 05.3 --observational --wage-example`\n",
    "- Pattern: [build] [target] [content-flags]"
   ]
  }
 ],
 "metadata": {
  "facure_augment": {
   "chapter_number": 5,
   "section_number": 3,
   "title": "Regression for Non-Random Data",
   "facure_source": "05-The-Unreasonable-Effectiveness-of-Linear-Regression.ipynb",
   "section_types": [
    "table_of_contents",
    "facure_intuition",
    "formal_treatment",
    "numeric_demonstration",
    "implementation",
    "interview_appendix",
    "references"
   ],
   "version": "1.0.0",
   "last_validated": "2026-01-09"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}