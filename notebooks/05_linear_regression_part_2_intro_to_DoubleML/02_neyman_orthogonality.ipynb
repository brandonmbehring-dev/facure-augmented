{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02. Neyman Orthogonality: Why DML Works\n",
    "\n",
    "**Part 2**: Linear Regression → Double Machine Learning Bridge  \n",
    "**Notebook**: 02 - Neyman Orthogonality  \n",
    "**Tier**: C→B (Intuition to Applied) — Visual intuition building to key definition  \n",
    "**Prerequisites**: Notebook 01 (Robinson transformation)  \n",
    "**Forward Reference**: Notebook 05 (DML implementation)\n",
    "\n",
    "---\n",
    "\n",
    "## The Central Question\n",
    "\n",
    "> **If ML models are biased (due to regularization), how can DML produce unbiased treatment effects?**\n",
    "\n",
    "The answer is **Neyman orthogonality** — a property that makes the treatment effect estimate *insensitive* to first-order errors in nuisance estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The Problem: Nuisance Errors Propagate](#1-the-problem-nuisance-errors-propagate)\n",
    "2. [Visual Intuition: Sensitivity Surfaces](#2-visual-intuition-sensitivity-surfaces)\n",
    "3. [Definition: Neyman Orthogonality](#3-definition-neyman-orthogonality)\n",
    "4. [The DML Score: Achieving Orthogonality](#4-the-dml-score-achieving-orthogonality)\n",
    "5. [Numerical Demonstration](#5-numerical-demonstration)\n",
    "6. [Why Rates Matter](#6-why-rates-matter)\n",
    "7. [Key Takeaways](#7-key-takeaways)\n",
    "8. [Interview Question](#8-interview-question)\n",
    "9. [References](#9-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Problem: Nuisance Errors Propagate\n",
    "\n",
    "In the Robinson transformation, we estimate:\n",
    "- $\\hat{m}(X) \\approx E[T|X]$ (propensity/first-stage)\n",
    "- $\\hat{\\ell}(X) \\approx E[Y|X]$ (outcome model)\n",
    "\n",
    "These are **nuisance parameters** — we don't care about them directly, but we need them to estimate $\\tau$.\n",
    "\n",
    "```\n",
    "★ The Core Problem ─────────────────────────────────────────\n",
    "  \n",
    "  ML models are BIASED due to regularization:\n",
    "  - Lasso shrinks coefficients toward zero\n",
    "  - Ridge adds L2 penalty\n",
    "  - Random Forests have finite depth\n",
    "  \n",
    "  If m̂(X) is biased, does this bias τ̂?\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "Let's see what happens with a **naive** approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data where we KNOW the true effect\n",
    "n = 2000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Confounders\n",
    "X = np.random.uniform(-2, 2, (n, 3))\n",
    "\n",
    "# True nuisance functions (nonlinear)\n",
    "m0 = lambda X: X[:, 0]**2 + 0.5 * X[:, 1]  # E[T|X]\n",
    "g0 = lambda X: np.sin(X[:, 0] * np.pi) + X[:, 2]**2  # direct effect of X on Y\n",
    "\n",
    "# Treatment and outcome\n",
    "true_tau = 2.5\n",
    "T = m0(X) + np.random.normal(0, 0.5, n)\n",
    "Y = true_tau * T + g0(X) + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# Also define ℓ₀(X) = τ·m₀(X) + g₀(X) = E[Y|X]\n",
    "l0 = lambda X: true_tau * m0(X) + g0(X)\n",
    "\n",
    "print(f\"True τ = {true_tau}\")\n",
    "print(f\"m₀(X) = X₁² + 0.5·X₂\")\n",
    "print(f\"g₀(X) = sin(πX₁) + X₃²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_estimator(Y, T, X, m_hat, g_hat):\n",
    "    \"\"\"\n",
    "    Naive moment estimator (NOT orthogonal).\n",
    "    \n",
    "    Uses the score: ψ = (T - m̂(X)) · (Y - τT - ĝ(X))\n",
    "    \n",
    "    This is biased when m̂ or ĝ are biased!\n",
    "    \"\"\"\n",
    "    T_resid = T - m_hat\n",
    "    \n",
    "    # Solve for tau: E[(T - m̂) · (Y - τT - ĝ)] = 0\n",
    "    # τ = E[(T - m̂) · (Y - ĝ)] / E[(T - m̂) · T]\n",
    "    numerator = np.mean(T_resid * (Y - g_hat))\n",
    "    denominator = np.mean(T_resid * T)\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def dml_estimator(Y, T, X, m_hat, l_hat):\n",
    "    \"\"\"\n",
    "    DML orthogonal estimator.\n",
    "    \n",
    "    Uses the score: ψ = (T - m̂(X)) · (Y - ℓ̂(X) - τ(T - m̂(X)))\n",
    "    \n",
    "    Key difference: Uses ℓ̂(X) = E[Y|X] instead of ĝ(X)!\n",
    "    \"\"\"\n",
    "    T_resid = T - m_hat\n",
    "    Y_resid = Y - l_hat\n",
    "    \n",
    "    # Simple OLS on residuals\n",
    "    return np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "\n",
    "print(\"Estimators defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With PERFECT nuisance estimation\n",
    "m_perfect = m0(X)\n",
    "g_perfect = g0(X)\n",
    "l_perfect = l0(X)\n",
    "\n",
    "tau_naive_perfect = naive_estimator(Y, T, X, m_perfect, g_perfect)\n",
    "tau_dml_perfect = dml_estimator(Y, T, X, m_perfect, l_perfect)\n",
    "\n",
    "print(\"With PERFECT nuisance estimation:\")\n",
    "print(f\"  Naive:  τ̂ = {tau_naive_perfect:.4f}\")\n",
    "print(f\"  DML:    τ̂ = {tau_dml_perfect:.4f}\")\n",
    "print(f\"  True:   τ  = {true_tau:.4f}\")\n",
    "print(\"\\nBoth work when nuisance is perfect!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now introduce BIASED nuisance estimation\n",
    "# Simulate regularization bias: shrink toward zero\n",
    "\n",
    "def add_bias_to_nuisance(true_func, X, bias_fraction=0.3):\n",
    "    \"\"\"Simulate regularization bias by shrinking toward mean.\"\"\"\n",
    "    true_vals = true_func(X)\n",
    "    mean_val = np.mean(true_vals)\n",
    "    return (1 - bias_fraction) * true_vals + bias_fraction * mean_val\n",
    "\n",
    "# Create biased estimates\n",
    "m_biased = add_bias_to_nuisance(m0, X, bias_fraction=0.3)\n",
    "g_biased = add_bias_to_nuisance(g0, X, bias_fraction=0.3)\n",
    "l_biased = add_bias_to_nuisance(l0, X, bias_fraction=0.3)\n",
    "\n",
    "tau_naive_biased = naive_estimator(Y, T, X, m_biased, g_biased)\n",
    "tau_dml_biased = dml_estimator(Y, T, X, m_biased, l_biased)\n",
    "\n",
    "print(\"With BIASED nuisance estimation (30% shrinkage):\")\n",
    "print(f\"  Naive:  τ̂ = {tau_naive_biased:.4f}  (bias = {tau_naive_biased - true_tau:+.4f})\")\n",
    "print(f\"  DML:    τ̂ = {tau_dml_biased:.4f}  (bias = {tau_dml_biased - true_tau:+.4f})\")\n",
    "print(f\"  True:   τ  = {true_tau:.4f}\")\n",
    "print(\"\\n★ DML is much more robust to nuisance bias!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "**This is the key insight**: The DML estimator barely changes when nuisance is biased, while the naive estimator is severely affected.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Visual Intuition: Sensitivity Surfaces\n",
    "\n",
    "Let's visualize *how* each estimator responds to nuisance perturbations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sensitivity: how does τ̂ change as we perturb nuisance?\n",
    "bias_fractions = np.linspace(0, 0.5, 20)\n",
    "\n",
    "naive_estimates = []\n",
    "dml_estimates = []\n",
    "\n",
    "for bf in bias_fractions:\n",
    "    m_perturbed = add_bias_to_nuisance(m0, X, bias_fraction=bf)\n",
    "    g_perturbed = add_bias_to_nuisance(g0, X, bias_fraction=bf)\n",
    "    l_perturbed = add_bias_to_nuisance(l0, X, bias_fraction=bf)\n",
    "    \n",
    "    naive_estimates.append(naive_estimator(Y, T, X, m_perturbed, g_perturbed))\n",
    "    dml_estimates.append(dml_estimator(Y, T, X, m_perturbed, l_perturbed))\n",
    "\n",
    "naive_estimates = np.array(naive_estimates)\n",
    "dml_estimates = np.array(dml_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sensitivity curves\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: Estimate vs bias level\n",
    "ax = axes[0]\n",
    "ax.plot(bias_fractions * 100, naive_estimates, 'o-', c=COLORS['red'], \n",
    "        lw=2, ms=5, label='Naive (non-orthogonal)')\n",
    "ax.plot(bias_fractions * 100, dml_estimates, 's-', c=COLORS['green'], \n",
    "        lw=2, ms=5, label='DML (orthogonal)')\n",
    "ax.axhline(true_tau, c=COLORS['blue'], ls='--', lw=2, label=f'True τ = {true_tau}')\n",
    "ax.fill_between(bias_fractions * 100, true_tau - 0.05, true_tau + 0.05, \n",
    "                alpha=0.2, color=COLORS['blue'])\n",
    "ax.set_xlabel('Nuisance Bias (%)', fontsize=11)\n",
    "ax.set_ylabel('Estimated τ', fontsize=11)\n",
    "ax.set_title('(a) Sensitivity to Nuisance Bias', fontweight='bold')\n",
    "ax.legend(loc='upper left', frameon=False)\n",
    "ax.set_ylim(2.0, 3.5)\n",
    "\n",
    "# Panel 2: Derivative (slope of sensitivity)\n",
    "ax = axes[1]\n",
    "naive_slope = np.gradient(naive_estimates, bias_fractions)\n",
    "dml_slope = np.gradient(dml_estimates, bias_fractions)\n",
    "\n",
    "ax.plot(bias_fractions * 100, naive_slope, 'o-', c=COLORS['red'], \n",
    "        lw=2, ms=5, label='Naive')\n",
    "ax.plot(bias_fractions * 100, dml_slope, 's-', c=COLORS['green'], \n",
    "        lw=2, ms=5, label='DML')\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=1)\n",
    "ax.set_xlabel('Nuisance Bias (%)', fontsize=11)\n",
    "ax.set_ylabel('∂τ̂/∂(bias)', fontsize=11)\n",
    "ax.set_title('(b) Sensitivity Derivative', fontweight='bold')\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n★ Key Observation:\")\n",
    "print(f\"   - Naive: Steep slope = High sensitivity to bias\")\n",
    "print(f\"   - DML:   Flat near 0 = Low sensitivity (orthogonal!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Definition: Neyman Orthogonality\n",
    "\n",
    "Now we can state the key definition formally:\n",
    "\n",
    "### The Setup\n",
    "\n",
    "- **Data**: $W = (Y, T, X)$\n",
    "- **Target parameter**: $\\tau$ (treatment effect)\n",
    "- **Nuisance parameters**: $\\eta = (m, \\ell)$ where:\n",
    "  - $m(x) = E[T|X=x]$\n",
    "  - $\\ell(x) = E[Y|X=x]$\n",
    "- **Score function**: $\\psi(W; \\tau, \\eta)$ such that $E[\\psi(W; \\tau_0, \\eta_0)] = 0$\n",
    "\n",
    "### Definition 2.1 (Neyman Orthogonality)\n",
    "\n",
    "```\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║                    NEYMAN ORTHOGONALITY                       ║\n",
    "╠═══════════════════════════════════════════════════════════════╣\n",
    "║                                                               ║\n",
    "║  A score function ψ(W; τ, η) satisfies Neyman orthogonality  ║\n",
    "║  at (τ₀, η₀) if:                                             ║\n",
    "║                                                               ║\n",
    "║        ∂                                                      ║\n",
    "║       ──── E[ψ(W; τ₀, η)]       = 0                          ║\n",
    "║        ∂η                  |η=η₀                              ║\n",
    "║                                                               ║\n",
    "║  Interpretation: The expected score is INSENSITIVE to        ║\n",
    "║  first-order perturbations in η around the true value.       ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "### What This Means Intuitively\n",
    "\n",
    "If the score is orthogonal:\n",
    "- Small errors in $\\hat{\\eta}$ have **negligible** effect on $\\hat{\\tau}$\n",
    "- The sensitivity surface is **flat** at the true nuisance values\n",
    "- Bias in $\\hat{\\tau}$ is proportional to $(\\hat{\\eta} - \\eta_0)^2$, not $(\\hat{\\eta} - \\eta_0)$\n",
    "\n",
    "```\n",
    "★ The Key Implication ──────────────────────────────────────\n",
    "  \n",
    "  Non-orthogonal: Bias(τ̂) ∝ ||η̂ - η₀||    (first order)\n",
    "  Orthogonal:     Bias(τ̂) ∝ ||η̂ - η₀||²   (second order!)\n",
    "  \n",
    "  Second-order bias is MUCH smaller for small errors.\n",
    "─────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate first-order vs second-order bias\n",
    "errors = np.linspace(0, 0.5, 100)\n",
    "first_order = errors          # ∝ ||error||\n",
    "second_order = errors**2      # ∝ ||error||²\n",
    "\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(8, 5))\n",
    "\n",
    "ax.plot(errors, first_order, c=COLORS['red'], lw=3, label='First-order: ∝ ||η̂ - η₀||')\n",
    "ax.plot(errors, second_order, c=COLORS['green'], lw=3, label='Second-order: ∝ ||η̂ - η₀||²')\n",
    "ax.fill_between(errors, 0, second_order, alpha=0.2, color=COLORS['green'])\n",
    "\n",
    "# Annotate typical ML error range\n",
    "ax.axvline(0.1, c=TUFTE_PALETTE['spine'], ls='--', lw=1, alpha=0.7)\n",
    "ax.axvline(0.25, c=TUFTE_PALETTE['spine'], ls='--', lw=1, alpha=0.7)\n",
    "ax.annotate('Typical ML\\nerror range', xy=(0.175, 0.12), fontsize=10, ha='center')\n",
    "\n",
    "ax.set_xlabel('Nuisance Error ||η̂ - η₀||', fontsize=11)\n",
    "ax.set_ylabel('Bias in τ̂', fontsize=11)\n",
    "ax.set_title('First-Order vs Second-Order Bias', fontweight='bold')\n",
    "ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute ratio at typical error levels\n",
    "for e in [0.1, 0.2, 0.3]:\n",
    "    print(f\"At error = {e}: First-order/Second-order = {e / e**2:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. The DML Score: Achieving Orthogonality\n",
    "\n",
    "### The Naive Score (NOT Orthogonal)\n",
    "\n",
    "$$\\psi_{naive}(W; \\tau, \\eta) = (T - m(X)) \\cdot (Y - \\tau T - g(X))$$\n",
    "\n",
    "**Problem**: This score depends on $g(X)$, the *direct* effect of $X$ on $Y$. We don't observe $g_0$ — we only observe $\\ell_0 = E[Y|X]$.\n",
    "\n",
    "### The DML Score (Orthogonal)\n",
    "\n",
    "$$\\psi_{DML}(W; \\tau, \\eta) = (T - m(X)) \\cdot (Y - \\ell(X) - \\tau(T - m(X)))$$\n",
    "\n",
    "**Key change**: Use $\\ell(X) = E[Y|X]$ instead of $g(X)$!\n",
    "\n",
    "Under the model:\n",
    "$$\\ell_0(X) = \\tau_0 \\cdot m_0(X) + g_0(X)$$\n",
    "\n",
    "### Why Is DML Orthogonal?\n",
    "\n",
    "Let's verify numerically that the DML score satisfies orthogonality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_sensitivity(Y, T, X, score_type='dml', perturb_which='m', delta=0.01):\n",
    "    \"\"\"\n",
    "    Compute numerical derivative of E[ψ] with respect to nuisance perturbation.\n",
    "    \n",
    "    If orthogonal, this should be ≈ 0 at true nuisance values.\n",
    "    \"\"\"\n",
    "    m_true = m0(X)\n",
    "    l_true = l0(X)\n",
    "    g_true = g0(X)\n",
    "    \n",
    "    def expected_score(m, l, g, tau):\n",
    "        T_resid = T - m\n",
    "        if score_type == 'dml':\n",
    "            score = T_resid * (Y - l - tau * T_resid)\n",
    "        else:  # naive\n",
    "            score = T_resid * (Y - tau * T - g)\n",
    "        return np.mean(score)\n",
    "    \n",
    "    # Compute derivative numerically\n",
    "    if perturb_which == 'm':\n",
    "        score_plus = expected_score(m_true * (1 + delta), l_true, g_true, true_tau)\n",
    "        score_minus = expected_score(m_true * (1 - delta), l_true, g_true, true_tau)\n",
    "    elif perturb_which == 'l':\n",
    "        score_plus = expected_score(m_true, l_true * (1 + delta), g_true, true_tau)\n",
    "        score_minus = expected_score(m_true, l_true * (1 - delta), g_true, true_tau)\n",
    "    else:  # g (for naive only)\n",
    "        score_plus = expected_score(m_true, l_true, g_true * (1 + delta), true_tau)\n",
    "        score_minus = expected_score(m_true, l_true, g_true * (1 - delta), true_tau)\n",
    "    \n",
    "    return (score_plus - score_minus) / (2 * delta)\n",
    "\n",
    "\n",
    "# Check orthogonality\n",
    "print(\"Numerical Orthogonality Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"(Sensitivity ≈ 0 means orthogonal)\")\n",
    "print()\n",
    "print(\"Naive Score:\")\n",
    "print(f\"  ∂E[ψ]/∂m at η₀: {compute_score_sensitivity(Y, T, X, 'naive', 'm'):.6f}\")\n",
    "print(f\"  ∂E[ψ]/∂g at η₀: {compute_score_sensitivity(Y, T, X, 'naive', 'g'):.6f}\")\n",
    "print()\n",
    "print(\"DML Score:\")\n",
    "print(f\"  ∂E[ψ]/∂m at η₀: {compute_score_sensitivity(Y, T, X, 'dml', 'm'):.6f}\")\n",
    "print(f\"  ∂E[ψ]/∂ℓ at η₀: {compute_score_sensitivity(Y, T, X, 'dml', 'l'):.6f}\")\n",
    "print()\n",
    "print(\"★ DML derivatives are ≈ 0 → ORTHOGONAL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Numerical Demonstration\n",
    "\n",
    "Let's run a Monte Carlo simulation to see orthogonality in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "def run_simulation(n_sims=50, n=1000, nuisance_bias=0.2):\n    \"\"\"\n    Compare naive vs DML estimators across many simulations\n    with biased nuisance estimation.\n    \n    Note: n_sims kept small for speed. Increase for precision.\n    \"\"\"\n    naive_results = []\n    dml_results = []\n    \n    true_tau = 2.5\n    \n    for _ in range(n_sims):\n        # Generate data\n        X = np.random.uniform(-2, 2, (n, 3))\n        m_true = X[:, 0]**2 + 0.5 * X[:, 1]\n        g_true = np.sin(X[:, 0] * np.pi) + X[:, 2]**2\n        l_true = true_tau * m_true + g_true\n        \n        T = m_true + np.random.normal(0, 0.5, n)\n        Y = true_tau * T + g_true + np.random.normal(0, 0.5, n)\n        \n        # Biased nuisance (simulating regularization)\n        m_biased = (1 - nuisance_bias) * m_true + nuisance_bias * np.mean(m_true)\n        g_biased = (1 - nuisance_bias) * g_true + nuisance_bias * np.mean(g_true)\n        l_biased = (1 - nuisance_bias) * l_true + nuisance_bias * np.mean(l_true)\n        \n        # Compute estimates\n        tau_naive = naive_estimator(Y, T, X, m_biased, g_biased)\n        tau_dml = dml_estimator(Y, T, X, m_biased, l_biased)\n        \n        naive_results.append(tau_naive)\n        dml_results.append(tau_dml)\n    \n    return np.array(naive_results), np.array(dml_results), true_tau\n\n# Run simulation (n_sims=100 for reasonable precision)\nnaive_estimates, dml_estimates, true_tau = run_simulation(n_sims=100, nuisance_bias=0.25)\n\nprint(\"Monte Carlo Simulation Results (100 replications, 25% nuisance bias)\")\nprint(\"=\" * 60)\nprint(f\"True τ = {true_tau}\")\nprint()\nprint(\"Naive Estimator:\")\nprint(f\"  Mean:   {np.mean(naive_estimates):.4f}\")\nprint(f\"  Bias:   {np.mean(naive_estimates) - true_tau:+.4f}\")\nprint(f\"  SD:     {np.std(naive_estimates):.4f}\")\nprint()\nprint(\"DML Estimator:\")\nprint(f\"  Mean:   {np.mean(dml_estimates):.4f}\")\nprint(f\"  Bias:   {np.mean(dml_estimates) - true_tau:+.4f}\")\nprint(f\"  SD:     {np.std(dml_estimates):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize distributions\nfig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n\n# Panel 1: Histograms\nax = axes[0]\nbins = np.linspace(2.0, 3.5, 40)\nax.hist(naive_estimates, bins=bins, alpha=0.6, color=COLORS['red'], \n        label='Naive', edgecolor='white')\nax.hist(dml_estimates, bins=bins, alpha=0.6, color=COLORS['green'], \n        label='DML', edgecolor='white')\nax.axvline(true_tau, c=COLORS['blue'], ls='--', lw=2.5, label=f'True τ = {true_tau}')\nax.axvline(np.mean(naive_estimates), c=COLORS['red'], ls=':', lw=2)\nax.axvline(np.mean(dml_estimates), c=COLORS['green'], ls=':', lw=2)\nax.set_xlabel('Estimated τ', fontsize=11)\nax.set_ylabel('Frequency', fontsize=11)\nax.set_title('(a) Distribution of Estimates', fontweight='bold')\nax.legend(loc='upper right', frameon=False)\n\n# Panel 2: Bias comparison across different bias levels (fewer points for speed)\nax = axes[1]\nbias_levels = [0.1, 0.2, 0.3, 0.4]  # Reduced from 8 to 4 for speed\nnaive_biases = []\ndml_biases = []\n\nfor bl in bias_levels:\n    n_est, d_est, tt = run_simulation(n_sims=30, nuisance_bias=bl)\n    naive_biases.append(np.mean(n_est) - tt)\n    dml_biases.append(np.mean(d_est) - tt)\n\nax.plot([b*100 for b in bias_levels], naive_biases, 'o-', c=COLORS['red'], \n        lw=2, ms=8, label='Naive')\nax.plot([b*100 for b in bias_levels], dml_biases, 's-', c=COLORS['green'], \n        lw=2, ms=8, label='DML')\nax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=1)\nax.set_xlabel('Nuisance Bias (%)', fontsize=11)\nax.set_ylabel('Bias in τ̂', fontsize=11)\nax.set_title('(b) Bias vs Nuisance Error', fontweight='bold')\nax.legend(loc='upper left', frameon=False)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Why Rates Matter\n",
    "\n",
    "The practical implication of orthogonality involves **convergence rates**:\n",
    "\n",
    "```\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║                    CONVERGENCE RATES                          ║\n",
    "╠═══════════════════════════════════════════════════════════════╣\n",
    "║                                                               ║\n",
    "║  For √n-consistent τ̂, we need nuisance errors small enough:  ║\n",
    "║                                                               ║\n",
    "║  Non-orthogonal:  ||η̂ - η₀|| = o_p(n^{-1/2})                 ║\n",
    "║                   → Requires VERY fast rates (impossible ML) ║\n",
    "║                                                               ║\n",
    "║  Orthogonal:      ||η̂ - η₀|| = o_p(n^{-1/4})                 ║\n",
    "║                   → Allows slower rates (achievable with ML!)║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "### What Can ML Methods Achieve?\n",
    "\n",
    "| Method | Typical Rate | Non-orthogonal? | Orthogonal? |\n",
    "|--------|--------------|-----------------|-------------|\n",
    "| OLS (correct spec) | $n^{-1/2}$ | ✓ Yes | ✓ Yes |\n",
    "| Lasso | $n^{-1/4}$ to $n^{-1/3}$ | ✗ No | ✓ Yes |\n",
    "| Random Forest | $n^{-1/4}$ to $n^{-1/3}$ | ✗ No | ✓ Yes |\n",
    "| Neural Networks | $n^{-1/4}$ to $n^{-1/3}$ | ✗ No | ✓ Yes |\n",
    "\n",
    "**Orthogonality unlocks ML for causal inference!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence rates\n",
    "n_values = np.logspace(2, 5, 50)  # 100 to 100,000\n",
    "\n",
    "rate_fast = n_values**(-0.5)      # n^{-1/2}\n",
    "rate_ml = n_values**(-0.25)       # n^{-1/4}\n",
    "rate_ml_sq = rate_ml**2           # (n^{-1/4})^2 = n^{-1/2}\n",
    "\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.loglog(n_values, rate_fast, c=COLORS['blue'], lw=2.5, label='n^{-1/2} (needed for non-orthogonal)')\n",
    "ax.loglog(n_values, rate_ml, c=COLORS['red'], lw=2.5, ls='--', label='n^{-1/4} (typical ML rate)')\n",
    "ax.loglog(n_values, rate_ml_sq, c=COLORS['green'], lw=2.5, label='(n^{-1/4})² = n^{-1/2} (orthogonal!)')\n",
    "\n",
    "ax.set_xlabel('Sample Size n', fontsize=11)\n",
    "ax.set_ylabel('Error Rate', fontsize=11)\n",
    "ax.set_title('Orthogonality Converts Slow Rates to Fast Rates', fontweight='bold')\n",
    "ax.legend(loc='lower left', frameon=False, fontsize=10)\n",
    "\n",
    "# Annotate\n",
    "ax.annotate('ML achieves n^{-1/4}\\nbut needs n^{-1/2}...', xy=(3000, 0.04), fontsize=10)\n",
    "ax.annotate('Orthogonality:\\n(n^{-1/4})² = n^{-1/2} ✓', xy=(3000, 0.005), fontsize=10, color=COLORS['green'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "```\n",
    "★ Summary ──────────────────────────────────────────────────\n",
    "\n",
    "1. PROBLEM: ML models are biased (regularization). Does this\n",
    "   bias the treatment effect estimate?\n",
    "\n",
    "2. NON-ORTHOGONAL SCORES: Yes! First-order nuisance errors\n",
    "   directly translate to first-order bias in τ̂.\n",
    "\n",
    "3. NEYMAN ORTHOGONALITY: A score property where:\n",
    "   ∂/∂η E[ψ(W; τ₀, η)]|_{η₀} = 0\n",
    "   \n",
    "   → First-order nuisance errors have ZERO first-order effect\n",
    "\n",
    "4. DML SCORE: (T - m̂(X)) · (Y - ℓ̂(X) - τ(T - m̂(X)))\n",
    "   → Uses E[Y|X] instead of direct effect g(X)\n",
    "   → Achieves orthogonality\n",
    "\n",
    "5. RATES: Orthogonality allows n^{-1/4} nuisance rates\n",
    "   to achieve n^{-1/2} target rates.\n",
    "   → UNLOCKS ML FOR CAUSAL INFERENCE\n",
    "\n",
    "6. REMAINING ISSUE: Overfitting when using same data for\n",
    "   nuisance estimation and score evaluation.\n",
    "   → Solved by cross-fitting (Notebook 04)\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Interview Question\n",
    "\n",
    "**Q (Meta IC5, Economist)**: *\"Why does DML work with biased ML estimators? Explain Neyman orthogonality intuitively.\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points to hit:**\n",
    "\n",
    "1. **The problem**: ML estimators (Random Forest, Lasso) are biased due to regularization. In a naive approach, this bias propagates to the treatment effect estimate.\n",
    "\n",
    "2. **Intuition for orthogonality**: Think of the expected score $E[\\psi]$ as a surface over the nuisance parameters. At the true values $\\eta_0$:\n",
    "   - Non-orthogonal: The surface has a non-zero slope. Moving slightly in any direction changes the score.\n",
    "   - Orthogonal: The surface is **flat**. The gradient is zero. Small perturbations in nuisance have no first-order effect.\n",
    "\n",
    "3. **Mathematical statement**: $\\frac{\\partial}{\\partial \\eta} E[\\psi(W; \\tau_0, \\eta)]|_{\\eta_0} = 0$\n",
    "\n",
    "4. **Consequence for bias**:\n",
    "   - Non-orthogonal: $\\text{Bias}(\\hat{\\tau}) \\propto \\|\\hat{\\eta} - \\eta_0\\|$ (first-order)\n",
    "   - Orthogonal: $\\text{Bias}(\\hat{\\tau}) \\propto \\|\\hat{\\eta} - \\eta_0\\|^2$ (second-order, much smaller)\n",
    "\n",
    "5. **Rate implication**: ML methods achieve rates like $n^{-1/4}$. With orthogonality, $(n^{-1/4})^2 = n^{-1/2}$, which is enough for $\\sqrt{n}$-consistency.\n",
    "\n",
    "6. **The DML trick**: Instead of using $g(X)$ (direct X→Y effect), use $\\ell(X) = E[Y|X]$. Under the model, $\\ell_0 = \\tau_0 \\cdot m_0 + g_0$, and this substitution achieves orthogonality.\n",
    "\n",
    "**One-liner**: \"Orthogonality makes the score insensitive to first-order nuisance errors, converting the bias from linear to quadratic in the error rate.\"\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 9. References\n",
    "\n",
    "[^1]: Chernozhukov, V. et al. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^2]: Neyman, J. (1959). Optimal Asymptotic Tests of Composite Statistical Hypotheses. *Probability and Statistics*.\n",
    "\n",
    "[^3]: Bickel, P. J. et al. (1993). *Efficient and Adaptive Estimation for Semiparametric Models*. Johns Hopkins University Press.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [03. The Regularization Problem](./03_regularization_problem.ipynb) — Why naive ML fails and how DML overcomes it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}