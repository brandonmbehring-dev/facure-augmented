{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 04. Cross-Fitting: Breaking the Overfitting Trap\n",
    "\n",
    "**Part 2**: Linear Regression → Double Machine Learning Bridge  \n",
    "**Notebook**: 04 - Cross-Fitting  \n",
    "**Tier**: B (Applied) — Sample splitting mechanics with practical demos  \n",
    "**Prerequisites**: Notebooks 01-03 (Robinson, Orthogonality, Regularization)  \n",
    "**Forward Reference**: Notebook 05 (Full DML implementation)\n",
    "\n",
    "---\n",
    "\n",
    "## The Remaining Problem\n",
    "\n",
    "Even with Neyman orthogonality, one issue remains:\n",
    "\n",
    "> **Using the same data to estimate nuisance and evaluate the score causes overfitting bias.**\n",
    "\n",
    "This notebook explains why and shows how **cross-fitting** solves it.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The Overfitting Problem](#1-the-overfitting-problem)\n",
    "2. [Sample Splitting](#2-sample-splitting)\n",
    "3. [K-Fold Cross-Fitting](#3-k-fold-cross-fitting)\n",
    "4. [Implementation](#4-implementation)\n",
    "5. [Why It Works](#5-why-it-works)\n",
    "6. [Practical Considerations](#6-practical-considerations)\n",
    "7. [Key Takeaways](#7-key-takeaways)\n",
    "8. [Interview Question](#8-interview-question)\n",
    "9. [References](#9-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.base import clone\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Overfitting Problem\n",
    "\n",
    "### Why Same-Sample Prediction Is Dangerous\n",
    "\n",
    "When we fit a flexible model on data and then use **in-sample predictions** on the same data:\n",
    "\n",
    "1. **The model memorizes noise** in the training data\n",
    "2. **Predictions are too good** — they correlate with the actual noise terms\n",
    "3. **Residuals are too small** — genuine variation is absorbed as \"explained\"\n",
    "\n",
    "```\n",
    "★ The Core Issue ─────────────────────────────────────────────\n",
    "  \n",
    "  In Robinson/DML, we compute:\n",
    "    T̃ = T - m̂(X)\n",
    "    Ỹ = Y - ℓ̂(X)\n",
    "  \n",
    "  If m̂ and ℓ̂ overfit to training noise:\n",
    "    - T̃ and Ỹ are smaller than they should be\n",
    "    - Their correlation is distorted\n",
    "    - τ̂ is biased!\n",
    "─────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting in predictions\n",
    "n = 500\n",
    "p = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "X = np.random.randn(n, p)\n",
    "true_m = X[:, 0]**2 + X[:, 1]  # E[T|X]\n",
    "noise = np.random.normal(0, 1, n)\n",
    "T = true_m + noise\n",
    "\n",
    "# Fit Random Forest (very flexible)\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=None, min_samples_leaf=1, random_state=42)\n",
    "rf.fit(X, T)\n",
    "\n",
    "# In-sample predictions\n",
    "T_pred_insample = rf.predict(X)\n",
    "T_resid_insample = T - T_pred_insample\n",
    "\n",
    "# Cross-validated predictions (out-of-sample)\n",
    "T_pred_cv = cross_val_predict(rf, X, T, cv=5)\n",
    "T_resid_cv = T - T_pred_cv\n",
    "\n",
    "print(\"In-Sample vs Cross-Validated Residuals\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True noise variance:        {np.var(noise):.3f}\")\n",
    "print(f\"In-sample residual var:     {np.var(T_resid_insample):.3f}  (should ≈ {np.var(noise):.3f})\")\n",
    "print(f\"Cross-val residual var:     {np.var(T_resid_cv):.3f}  (should ≈ {np.var(noise):.3f})\")\n",
    "print()\n",
    "print(f\"In-sample variance ratio:   {np.var(T_resid_insample)/np.var(noise):.2%}\")\n",
    "print(f\"Cross-val variance ratio:   {np.var(T_resid_cv)/np.var(noise):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the problem\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Panel 1: True noise distribution\n",
    "ax = axes[0]\n",
    "bins = np.linspace(-4, 4, 40)\n",
    "ax.hist(noise, bins=bins, alpha=0.7, color=COLORS['blue'], edgecolor='white', label='True noise')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('(a) True Noise Distribution', fontweight='bold')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 2: In-sample residuals (too small!)\n",
    "ax = axes[1]\n",
    "ax.hist(T_resid_insample, bins=bins, alpha=0.7, color=COLORS['red'], \n",
    "        edgecolor='white', label='In-sample residuals')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_title(f'(b) In-Sample: Var = {np.var(T_resid_insample):.2f} (too small!)', fontweight='bold')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "# Panel 3: CV residuals (correct)\n",
    "ax = axes[2]\n",
    "ax.hist(T_resid_cv, bins=bins, alpha=0.7, color=COLORS['green'], \n",
    "        edgecolor='white', label='Cross-val residuals')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_title(f'(c) Cross-Val: Var = {np.var(T_resid_cv):.2f} (correct!)', fontweight='bold')\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n★ In-sample residuals are TOO SMALL due to overfitting!\")\n",
    "print(\"  The model 'memorizes' training noise, leaving almost nothing in residuals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### How This Biases the Treatment Effect\n",
    "\n",
    "Let's see what happens to $\\hat{\\tau}$ when we use in-sample vs cross-validated residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full causal DGP\n",
    "n = 1000\n",
    "p = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.randn(n, p)\n",
    "m0 = X[:, 0]**2 + X[:, 1]**2\n",
    "g0 = np.sin(X[:, 0] * np.pi) + X[:, 2]\n",
    "\n",
    "true_tau = 2.5\n",
    "T = m0 + np.random.normal(0, 1, n)\n",
    "Y = true_tau * T + g0 + np.random.normal(0, 1, n)\n",
    "\n",
    "l0 = true_tau * m0 + g0  # E[Y|X]\n",
    "\n",
    "print(f\"True τ = {true_tau}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_dml(Y, T, X, model, use_cv=False):\n",
    "    \"\"\"\n",
    "    DML with or without cross-fitting.\n",
    "    \n",
    "    use_cv=False: In-sample predictions (BIASED)\n",
    "    use_cv=True:  Cross-validated predictions (CORRECT)\n",
    "    \"\"\"\n",
    "    if use_cv:\n",
    "        # Cross-validated predictions\n",
    "        T_pred = cross_val_predict(model, X, T, cv=5)\n",
    "        Y_pred = cross_val_predict(model, X, Y, cv=5)\n",
    "    else:\n",
    "        # In-sample predictions (overfitting!)\n",
    "        model_T = clone(model).fit(X, T)\n",
    "        T_pred = model_T.predict(X)\n",
    "        model_Y = clone(model).fit(X, Y)\n",
    "        Y_pred = model_Y.predict(X)\n",
    "    \n",
    "    T_resid = T - T_pred\n",
    "    Y_resid = Y - Y_pred\n",
    "    \n",
    "    tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "    return tau_hat, T_resid, Y_resid\n",
    "\n",
    "\n",
    "# Compare\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "tau_insample, T_resid_in, Y_resid_in = naive_dml(Y, T, X, rf, use_cv=False)\n",
    "tau_cv, T_resid_out, Y_resid_out = naive_dml(Y, T, X, rf, use_cv=True)\n",
    "\n",
    "print(\"Effect of Cross-Fitting on τ̂\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"In-sample (no CV):   τ̂ = {tau_insample:.3f}  (bias = {tau_insample - true_tau:+.3f})\")\n",
    "print(f\"Cross-validated:     τ̂ = {tau_cv:.3f}  (bias = {tau_cv - true_tau:+.3f})\")\n",
    "print(f\"True:                τ  = {true_tau:.3f}\")\n",
    "print()\n",
    "print(f\"Bias reduction: {abs(tau_insample - true_tau) / abs(tau_cv - true_tau):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Sample Splitting\n",
    "\n",
    "The simplest solution is **2-fold sample splitting**:\n",
    "\n",
    "```\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║                    2-FOLD SAMPLE SPLITTING                    ║\n",
    "╠═══════════════════════════════════════════════════════════════╣\n",
    "║                                                               ║\n",
    "║  1. Split data into two halves: I₁ and I₂                   ║\n",
    "║                                                               ║\n",
    "║  2. On I₁: Train nuisance models m̂₁, ℓ̂₁                    ║\n",
    "║     On I₂: Compute residuals using m̂₁, ℓ̂₁                  ║\n",
    "║                                                               ║\n",
    "║  3. On I₂: Train nuisance models m̂₂, ℓ̂₂                    ║\n",
    "║     On I₁: Compute residuals using m̂₂, ℓ̂₂                  ║\n",
    "║                                                               ║\n",
    "║  4. Pool residuals → Estimate τ̂                             ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "**Key property**: For each observation $i$, the nuisance models used to compute its residuals were trained on **different data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_fold_dml(Y, T, X, model):\n",
    "    \"\"\"\n",
    "    Simple 2-fold sample splitting DML.\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    idx = np.random.permutation(n)\n",
    "    I1, I2 = idx[:n//2], idx[n//2:]\n",
    "    \n",
    "    # Train on I1, predict on I2\n",
    "    model_T1 = clone(model).fit(X[I1], T[I1])\n",
    "    model_Y1 = clone(model).fit(X[I1], Y[I1])\n",
    "    T_resid_2 = T[I2] - model_T1.predict(X[I2])\n",
    "    Y_resid_2 = Y[I2] - model_Y1.predict(X[I2])\n",
    "    \n",
    "    # Train on I2, predict on I1\n",
    "    model_T2 = clone(model).fit(X[I2], T[I2])\n",
    "    model_Y2 = clone(model).fit(X[I2], Y[I2])\n",
    "    T_resid_1 = T[I1] - model_T2.predict(X[I1])\n",
    "    Y_resid_1 = Y[I1] - model_Y2.predict(X[I1])\n",
    "    \n",
    "    # Pool residuals\n",
    "    T_resid = np.concatenate([T_resid_1, T_resid_2])\n",
    "    Y_resid = np.concatenate([Y_resid_1, Y_resid_2])\n",
    "    \n",
    "    tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "    return tau_hat\n",
    "\n",
    "tau_2fold = two_fold_dml(Y, T, X, rf)\n",
    "print(f\"2-fold DML: τ̂ = {tau_2fold:.3f}  (bias = {tau_2fold - true_tau:+.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. K-Fold Cross-Fitting\n",
    "\n",
    "The problem with 2-fold splitting: **only 50% of data** is used to train each nuisance model.\n",
    "\n",
    "**K-fold cross-fitting** improves efficiency:\n",
    "\n",
    "```\n",
    "╔═══════════════════════════════════════════════════════════════╗\n",
    "║                    K-FOLD CROSS-FITTING                       ║\n",
    "╠═══════════════════════════════════════════════════════════════╣\n",
    "║                                                               ║\n",
    "║  Split data into K folds: I₁, I₂, ..., I_K                   ║\n",
    "║                                                               ║\n",
    "║  For each fold k = 1, ..., K:                                ║\n",
    "║    1. Train nuisance on complement: I^{(-k)} = ∪_{j≠k} I_j   ║\n",
    "║    2. Predict on fold k                                      ║\n",
    "║    3. Compute residuals on fold k                            ║\n",
    "║                                                               ║\n",
    "║  Pool all residuals → Estimate τ̂                            ║\n",
    "║                                                               ║\n",
    "╚═══════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "**Data efficiency**:\n",
    "- K=2: 50% of data for training\n",
    "- K=5: 80% of data for training (most common)\n",
    "- K=10: 90% of data for training\n",
    "- K=n (LOO): ~100% but computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_fit_dml(Y, T, X, model, n_folds=5):\n",
    "    \"\"\"\n",
    "    K-fold cross-fitting DML.\n",
    "    \n",
    "    This is the standard DML implementation.\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    T_resid = np.zeros(n)\n",
    "    Y_resid = np.zeros(n)\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # Train on complement\n",
    "        model_T = clone(model).fit(X[train_idx], T[train_idx])\n",
    "        model_Y = clone(model).fit(X[train_idx], Y[train_idx])\n",
    "        \n",
    "        # Predict on fold\n",
    "        T_resid[test_idx] = T[test_idx] - model_T.predict(X[test_idx])\n",
    "        Y_resid[test_idx] = Y[test_idx] - model_Y.predict(X[test_idx])\n",
    "    \n",
    "    tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "    return tau_hat, T_resid, Y_resid\n",
    "\n",
    "\n",
    "# Compare different K values\n",
    "print(\"Effect of K on DML Estimate\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True τ = {true_tau}\\n\")\n",
    "\n",
    "for K in [2, 3, 5, 10]:\n",
    "    tau_k, _, _ = cross_fit_dml(Y, T, X, rf, n_folds=K)\n",
    "    data_used = (K-1)/K * 100\n",
    "    print(f\"K = {K:2d}: τ̂ = {tau_k:.3f}  (bias = {tau_k - true_tau:+.3f}, {data_used:.0f}% data for training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-fitting process\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(12, 4))\n",
    "\n",
    "n_folds = 5\n",
    "fold_colors = [COLORS['blue'], COLORS['green'], COLORS['purple'], COLORS['orange'], COLORS['red']]\n",
    "\n",
    "# Draw folds\n",
    "for i in range(n_folds):\n",
    "    for j in range(n_folds):\n",
    "        if i == j:\n",
    "            # Test fold\n",
    "            rect = plt.Rectangle((j * 2 + 0.1, n_folds - i - 1 + 0.1), 1.8, 0.8, \n",
    "                                  fc='white', ec=fold_colors[i], lw=2, hatch='///')\n",
    "        else:\n",
    "            # Training fold\n",
    "            rect = plt.Rectangle((j * 2 + 0.1, n_folds - i - 1 + 0.1), 1.8, 0.8,\n",
    "                                  fc=fold_colors[i], ec='white', alpha=0.5)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "# Labels\n",
    "ax.set_xlim(0, n_folds * 2)\n",
    "ax.set_ylim(0, n_folds)\n",
    "ax.set_xticks([1 + 2*i for i in range(n_folds)])\n",
    "ax.set_xticklabels([f'Fold {i+1}' for i in range(n_folds)])\n",
    "ax.set_yticks([0.5 + i for i in range(n_folds)])\n",
    "ax.set_yticklabels([f'Round {n_folds - i}' for i in range(n_folds)])\n",
    "ax.set_title('5-Fold Cross-Fitting: Each round uses one fold for prediction (hatched)', fontweight='bold')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='gray', edgecolor='white', alpha=0.5, label='Training data (80%)'),\n",
    "    Patch(facecolor='white', edgecolor='black', hatch='///', label='Test data (20%)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1.05, 0.5), frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "Here's a clean, reusable implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_fit_residuals(Y, T, X, model_Y, model_T, n_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute cross-fitted residuals for DML.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array-like (n,)\n",
    "        Outcome variable.\n",
    "    T : array-like (n,)\n",
    "        Treatment variable.\n",
    "    X : array-like (n, p)\n",
    "        Confounders.\n",
    "    model_Y : sklearn estimator\n",
    "        Model for E[Y|X]. Must have fit() and predict().\n",
    "    model_T : sklearn estimator\n",
    "        Model for E[T|X]. Must have fit() and predict().\n",
    "    n_folds : int\n",
    "        Number of cross-fitting folds. Default 5.\n",
    "    random_state : int\n",
    "        Random seed for fold splitting.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Y_resid : array (n,)\n",
    "        Cross-fitted outcome residuals.\n",
    "    T_resid : array (n,)\n",
    "        Cross-fitted treatment residuals.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    For observation i in fold k, residuals are computed using\n",
    "    models trained on all folds EXCEPT k.\n",
    "    \"\"\"\n",
    "    n = len(Y)\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    Y_resid = np.zeros(n)\n",
    "    T_resid = np.zeros(n)\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # Clone models to avoid state leakage\n",
    "        mY = clone(model_Y).fit(X[train_idx], Y[train_idx])\n",
    "        mT = clone(model_T).fit(X[train_idx], T[train_idx])\n",
    "        \n",
    "        # Out-of-fold predictions\n",
    "        Y_resid[test_idx] = Y[test_idx] - mY.predict(X[test_idx])\n",
    "        T_resid[test_idx] = T[test_idx] - mT.predict(X[test_idx])\n",
    "    \n",
    "    return Y_resid, T_resid\n",
    "\n",
    "\n",
    "def dml_ate(Y, T, X, model_Y, model_T, n_folds=5):\n",
    "    \"\"\"\n",
    "    Double Machine Learning ATE estimator with cross-fitting.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tau_hat : float\n",
    "        Estimated average treatment effect.\n",
    "    se : float\n",
    "        Standard error (Eicker-Huber-White).\n",
    "    ci_lower, ci_upper : float\n",
    "        95% confidence interval bounds.\n",
    "    \"\"\"\n",
    "    Y_resid, T_resid = cross_fit_residuals(Y, T, X, model_Y, model_T, n_folds)\n",
    "    \n",
    "    # Point estimate\n",
    "    tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "    \n",
    "    # Standard error (influence function based)\n",
    "    n = len(Y)\n",
    "    psi = T_resid * (Y_resid - tau_hat * T_resid) / np.mean(T_resid**2)\n",
    "    se = np.sqrt(np.var(psi) / n)\n",
    "    \n",
    "    # 95% CI\n",
    "    ci_lower = tau_hat - 1.96 * se\n",
    "    ci_upper = tau_hat + 1.96 * se\n",
    "    \n",
    "    return tau_hat, se, ci_lower, ci_upper\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "rf_Y = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_T = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "tau_hat, se, ci_lo, ci_hi = dml_ate(Y, T, X, rf_Y, rf_T, n_folds=5)\n",
    "\n",
    "print(\"DML with Cross-Fitting: Full Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"τ̂  = {tau_hat:.4f}\")\n",
    "print(f\"SE = {se:.4f}\")\n",
    "print(f\"95% CI: [{ci_lo:.4f}, {ci_hi:.4f}]\")\n",
    "print(f\"True τ = {true_tau:.4f}\")\n",
    "print(f\"\\nTrue value in CI? {'Yes ✓' if ci_lo < true_tau < ci_hi else 'No ✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Why It Works\n",
    "\n",
    "### The Technical Explanation\n",
    "\n",
    "Cross-fitting breaks the **dependence** between:\n",
    "1. Nuisance estimation error\n",
    "2. Score evaluation\n",
    "\n",
    "Without cross-fitting, the model \"sees\" the data twice:\n",
    "- First when fitting nuisance\n",
    "- Second when computing residuals\n",
    "\n",
    "This creates **Donsker class violations** — the empirical process theory that underlies asymptotic normality breaks down.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "```\n",
    "★ Why Cross-Fitting Works ───────────────────────────────────\n",
    "  \n",
    "  For observation i in fold k:\n",
    "  \n",
    "  • Nuisance model m̂^{(-k)} was trained on OTHER data\n",
    "  • So m̂^{(-k)} doesn't \"know\" about observation i's noise\n",
    "  • The prediction error m̂^{(-k)}(X_i) - m₀(X_i) is\n",
    "    INDEPENDENT of the noise term εᵢ\n",
    "  \n",
    "  This independence prevents overfitting bias from\n",
    "  propagating to the treatment effect estimate.\n",
    "─────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo demonstration: coverage probability\n",
    "def coverage_simulation(n=500, n_sims=200, use_cv=True, n_folds=5):\n",
    "    \"\"\"Check if 95% CI achieves nominal coverage.\"\"\"\n",
    "    true_tau = 2.5\n",
    "    covers = []\n",
    "    biases = []\n",
    "    \n",
    "    for sim in range(n_sims):\n",
    "        np.random.seed(sim)\n",
    "        \n",
    "        X = np.random.randn(n, 10)\n",
    "        m0 = X[:, 0]**2 + X[:, 1]\n",
    "        g0 = np.sin(X[:, 0] * np.pi)\n",
    "        \n",
    "        T = m0 + np.random.normal(0, 1, n)\n",
    "        Y = true_tau * T + g0 + np.random.normal(0, 1, n)\n",
    "        \n",
    "        rf = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=sim)\n",
    "        \n",
    "        if use_cv:\n",
    "            tau_hat, se, ci_lo, ci_hi = dml_ate(Y, T, X, rf, rf, n_folds)\n",
    "        else:\n",
    "            # Naive (no cross-fitting)\n",
    "            rf_T = clone(rf).fit(X, T)\n",
    "            rf_Y = clone(rf).fit(X, Y)\n",
    "            T_resid = T - rf_T.predict(X)\n",
    "            Y_resid = Y - rf_Y.predict(X)\n",
    "            tau_hat = np.sum(T_resid * Y_resid) / np.sum(T_resid**2)\n",
    "            psi = T_resid * (Y_resid - tau_hat * T_resid) / np.mean(T_resid**2)\n",
    "            se = np.sqrt(np.var(psi) / n)\n",
    "            ci_lo, ci_hi = tau_hat - 1.96*se, tau_hat + 1.96*se\n",
    "        \n",
    "        covers.append(ci_lo < true_tau < ci_hi)\n",
    "        biases.append(tau_hat - true_tau)\n",
    "    \n",
    "    return np.mean(covers), np.mean(biases), np.std(biases)\n",
    "\n",
    "print(\"Coverage Probability (200 simulations)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"(Target: 95%)\\n\")\n",
    "\n",
    "cov_naive, bias_naive, sd_naive = coverage_simulation(use_cv=False)\n",
    "cov_cv, bias_cv, sd_cv = coverage_simulation(use_cv=True)\n",
    "\n",
    "print(f\"Naive (no CV):  Coverage = {cov_naive:.1%}, Bias = {bias_naive:+.3f}, SD = {sd_naive:.3f}\")\n",
    "print(f\"Cross-fitted:   Coverage = {cov_cv:.1%}, Bias = {bias_cv:+.3f}, SD = {sd_cv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Practical Considerations\n",
    "\n",
    "### Choosing K\n",
    "\n",
    "| K | Training Data | Computation | Variance |\n",
    "|---|---------------|-------------|----------|\n",
    "| 2 | 50% | Low | Higher |\n",
    "| 5 | 80% | Moderate | Standard |\n",
    "| 10 | 90% | Higher | Lower |\n",
    "\n",
    "**Recommendation**: K=5 is the default choice (good balance).\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "1. **Using in-sample predictions**: Always use out-of-fold!\n",
    "2. **Not shuffling**: Folds should be randomly assigned\n",
    "3. **Leaking fold info**: Don't use test fold data in training\n",
    "4. **Ignoring SE correction**: Standard errors need influence function adjustment\n",
    "\n",
    "### When Cross-Fitting Matters Less\n",
    "\n",
    "- **Simple models** (OLS): No overfitting, CV not needed\n",
    "- **Strong regularization**: Already prevents memorization\n",
    "- **Large n, small model**: Overfitting minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "```\n",
    "★ Summary ──────────────────────────────────────────────────\n",
    "\n",
    "1. THE OVERFITTING PROBLEM\n",
    "   - Flexible models memorize training noise\n",
    "   - In-sample predictions are too accurate\n",
    "   - Residuals are too small → biased τ̂\n",
    "\n",
    "2. SAMPLE SPLITTING (2-fold)\n",
    "   - Train on half, predict on other half\n",
    "   - Simple but inefficient (only 50% data)\n",
    "\n",
    "3. CROSS-FITTING (K-fold)\n",
    "   - Train on K-1 folds, predict on holdout\n",
    "   - K=5 uses 80% of data (standard choice)\n",
    "   - Pool residuals across all folds\n",
    "\n",
    "4. WHY IT WORKS\n",
    "   - Breaks dependence between nuisance and score\n",
    "   - Prediction error independent of noise\n",
    "   - Restores valid inference\n",
    "\n",
    "5. PRACTICAL TIPS\n",
    "   - Always use out-of-fold predictions\n",
    "   - K=5 is the default\n",
    "   - Use influence functions for SE\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Interview Question\n",
    "\n",
    "**Q (Amazon, Applied Scientist)**: *\"Why is cross-fitting necessary in DML? What happens without it?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **The problem**: When using flexible ML models (Random Forest, Neural Nets) for nuisance estimation, they tend to **overfit** to the training data.\n",
    "\n",
    "2. **What overfitting does**:\n",
    "   - In-sample predictions are too good (model memorizes noise)\n",
    "   - Residuals $\\tilde{T} = T - \\hat{m}(X)$ are too small\n",
    "   - The correlation between residuals is distorted\n",
    "   - $\\hat{\\tau}$ becomes biased\n",
    "\n",
    "3. **Without cross-fitting** (same data for training and evaluation):\n",
    "   - The nuisance model \"knows\" about observation $i$'s noise\n",
    "   - The prediction error is correlated with the true noise\n",
    "   - This violates the independence needed for valid inference\n",
    "   - Confidence intervals have wrong coverage (often too narrow)\n",
    "\n",
    "4. **With cross-fitting**:\n",
    "   - For each observation, use a model trained on OTHER data\n",
    "   - The model cannot have memorized this observation's noise\n",
    "   - Prediction error is independent of true noise\n",
    "   - Valid inference restored\n",
    "\n",
    "5. **Technical term**: Without cross-fitting, we violate \"Donsker class\" conditions needed for empirical process theory.\n",
    "\n",
    "**One-liner**: \"Cross-fitting prevents the nuisance model from seeing the same observation twice, breaking the dependence between estimation error and evaluation that causes overfitting bias.\"\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 9. References\n",
    "\n",
    "[^1]: Chernozhukov, V. et al. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68. Section 3 (Cross-fitting).\n",
    "\n",
    "[^2]: Newey, W. K. and Robins, J. M. (2018). Cross-fitting and fast remainder rates for semiparametric estimation. *arXiv:1801.09138*.\n",
    "\n",
    "[^3]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 22.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [05. DML Implementation](./05_dml_implementation.ipynb) — Putting it all together with full worked examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
