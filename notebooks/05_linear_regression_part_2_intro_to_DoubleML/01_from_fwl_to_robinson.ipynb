{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 01. From FWL to Robinson: The Bridge to Double ML\n",
    "\n",
    "**Part 2**: Linear Regression → Double Machine Learning Bridge  \n",
    "**Notebook**: 01 - From FWL to Robinson  \n",
    "**Tier**: C (Intuition) — Visual explanations, heavy plots, minimal equations  \n",
    "**Prerequisites**: Chapter 5.2 (FWL Theorem)  \n",
    "**Forward Reference**: Chapter 22 (Debiased ML)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **FWL Recap**: How \"partialling out\" works in linear regression\n",
    "2. **The Limitation**: Why linear partialling fails with nonlinear confounding\n",
    "3. **Robinson's Insight**: Replace linear expectations with *any* functions\n",
    "4. **The Partial Linear Model**: Foundation of Double Machine Learning\n",
    "5. **Visual Demonstration**: Bias from linear vs. accuracy from flexible methods\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [The FWL Insight: Partialling Out](#1-the-fwl-insight-partialling-out)\n",
    "2. [When Linear Fails: Nonlinear Confounding](#2-when-linear-fails-nonlinear-confounding)\n",
    "3. [Robinson's Transformation](#3-robinsons-transformation)\n",
    "4. [Visual Demonstration](#4-visual-demonstration)\n",
    "5. [Key Takeaways](#5-key-takeaways)\n",
    "6. [Interview Question](#6-interview-question)\n",
    "7. [References](#7-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "# Additional imports for this notebook\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The FWL Insight: Partialling Out\n",
    "\n",
    "In Chapter 5.2, we proved the **Frisch-Waugh-Lovell Theorem**:\n",
    "\n",
    "> To find the coefficient on treatment $T$ in a regression with controls $X$, we can:\n",
    "> 1. Regress $Y$ on $X$ → get residuals $\\tilde{Y}$\n",
    "> 2. Regress $T$ on $X$ → get residuals $\\tilde{T}$\n",
    "> 3. Regress $\\tilde{Y}$ on $\\tilde{T}$ → the coefficient equals $\\hat{\\beta}_T$\n",
    "\n",
    "### Why Does This Work?\n",
    "\n",
    "The residuals $\\tilde{T} = T - \\hat{E}[T|X]$ represent the part of treatment that **cannot be predicted by confounders**. By construction, $\\tilde{T}$ is orthogonal to $X$.\n",
    "\n",
    "```\n",
    "★ Key Insight ─────────────────────────────────────────────\n",
    "  If confounders X satisfy conditional ignorability,\n",
    "  then T̃ is \"as good as random\" — it's uncorrelated\n",
    "  with any function of X.\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "Let's see this visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple confounded data where LINEAR adjustment works\n",
    "n = 500\n",
    "np.random.seed(42)\n",
    "\n",
    "# Confounder\n",
    "X = np.random.normal(0, 1, n)\n",
    "\n",
    "# Treatment: linearly affected by X\n",
    "T = 2 + 1.5 * X + np.random.normal(0, 1, n)\n",
    "\n",
    "# Outcome: true effect τ = 2.0, also linearly affected by X\n",
    "true_tau = 2.0\n",
    "Y = 5 + true_tau * T + 3 * X + np.random.normal(0, 1, n)\n",
    "\n",
    "print(f\"True treatment effect: τ = {true_tau}\")\n",
    "print(f\"Sample size: n = {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FWL: Partial out the confounder\n",
    "X_const = sm.add_constant(X)\n",
    "\n",
    "# Residualize T\n",
    "T_tilde = T - sm.OLS(T, X_const).fit().fittedvalues\n",
    "\n",
    "# Residualize Y  \n",
    "Y_tilde = Y - sm.OLS(Y, X_const).fit().fittedvalues\n",
    "\n",
    "# Estimate tau from residuals\n",
    "tau_fwl = np.cov(Y_tilde, T_tilde)[0, 1] / np.var(T_tilde, ddof=1)\n",
    "\n",
    "# Compare to naive (biased) estimate\n",
    "tau_naive = sm.OLS(Y, sm.add_constant(T)).fit().params[1]\n",
    "\n",
    "print(f\"Naive estimate (ignoring X): τ̂ = {tau_naive:.3f}  (biased!)\")\n",
    "print(f\"FWL estimate (linear):       τ̂ = {tau_fwl:.3f}  (correct ✓)\")\n",
    "print(f\"True effect:                 τ  = {true_tau:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the FWL process\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Panel 1: Raw relationship (confounded)\n",
    "ax = axes[0]\n",
    "ax.scatter(T, Y, alpha=0.4, s=25, c=TUFTE_PALETTE['primary'], edgecolor='white', linewidth=0.3)\n",
    "z = np.polyfit(T, Y, 1)\n",
    "T_line = np.linspace(T.min(), T.max(), 100)\n",
    "ax.plot(T_line, np.polyval(z, T_line), c=COLORS['red'], lw=2.5, label=f'slope = {z[0]:.2f} (biased)')\n",
    "ax.set_xlabel('Treatment T')\n",
    "ax.set_ylabel('Outcome Y')\n",
    "ax.set_title('(a) Raw Data: Confounded', fontweight='bold')\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "# Panel 2: Residualized relationship\n",
    "ax = axes[1]\n",
    "ax.scatter(T_tilde, Y_tilde, alpha=0.4, s=25, c=TUFTE_PALETTE['primary'], edgecolor='white', linewidth=0.3)\n",
    "z = np.polyfit(T_tilde, Y_tilde, 1)\n",
    "T_line = np.linspace(T_tilde.min(), T_tilde.max(), 100)\n",
    "ax.plot(T_line, np.polyval(z, T_line), c=COLORS['green'], lw=2.5, label=f'slope = {z[0]:.2f} (debiased)')\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5, alpha=0.5)\n",
    "ax.axvline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5, alpha=0.5)\n",
    "ax.set_xlabel('Treatment Residual T̃')\n",
    "ax.set_ylabel('Outcome Residual Ỹ')\n",
    "ax.set_title('(b) After FWL: Debiased', fontweight='bold')\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "# Panel 3: Coefficient comparison\n",
    "ax = axes[2]\n",
    "methods = ['Naive\\n(biased)', 'FWL\\n(correct)', 'True\\nEffect']\n",
    "values = [tau_naive, tau_fwl, true_tau]\n",
    "colors = [COLORS['red'], COLORS['green'], COLORS['blue']]\n",
    "bars = ax.bar(methods, values, color=colors, width=0.6, edgecolor='white', linewidth=1.5)\n",
    "ax.axhline(true_tau, c=COLORS['blue'], ls='--', lw=1.5, alpha=0.7)\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, val + 0.08, f'{val:.2f}', \n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Estimated τ')\n",
    "ax.set_title('(c) Estimates', fontweight='bold')\n",
    "ax.set_ylim(0, 3.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "**The linear FWL works perfectly here!** The naive estimate was biased upward (2.74 vs. 2.00), but partialling out $X$ linearly recovered the true effect.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. When Linear Fails: Nonlinear Confounding\n",
    "\n",
    "But what if the confounders affect treatment and outcome **nonlinearly**?\n",
    "\n",
    "```\n",
    "★ The Problem ─────────────────────────────────────────────\n",
    "  Real-world confounding is rarely linear.\n",
    "  \n",
    "  Example: Income affects both:\n",
    "  - Health spending (nonlinear - diminishing returns)\n",
    "  - Health outcomes (nonlinear - threshold effects)\n",
    "  \n",
    "  Linear partialling leaves nonlinear confounding UNREMOVED.\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "Let's create data with **quadratic and cubic** confounding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with NONLINEAR confounding\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Confounders\n",
    "X1 = np.random.uniform(-2, 2, n)\n",
    "X2 = np.random.uniform(-2, 2, n)\n",
    "X = np.column_stack([X1, X2])\n",
    "\n",
    "# Treatment: NONLINEAR function of confounders\n",
    "# T = X1² + X2 + noise\n",
    "T_nonlin = X1**2 + X2 + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# Outcome: true effect τ = 2.5, NONLINEAR confounding\n",
    "# Y = τ*T + sin(X1) + X2³ + noise\n",
    "true_tau_nonlin = 2.5\n",
    "Y_nonlin = true_tau_nonlin * T_nonlin + np.sin(X1 * np.pi) + 0.5 * X2**3 + np.random.normal(0, 0.5, n)\n",
    "\n",
    "print(f\"True treatment effect: τ = {true_tau_nonlin}\")\n",
    "print(f\"Treatment model: T = X₁² + X₂ + ε\")\n",
    "print(f\"Outcome model: Y = {true_tau_nonlin}·T + sin(πX₁) + 0.5·X₂³ + ε\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try LINEAR FWL on nonlinear data\n",
    "X_const_nonlin = sm.add_constant(X)\n",
    "\n",
    "# Linear residualization\n",
    "T_tilde_linear = T_nonlin - sm.OLS(T_nonlin, X_const_nonlin).fit().fittedvalues\n",
    "Y_tilde_linear = Y_nonlin - sm.OLS(Y_nonlin, X_const_nonlin).fit().fittedvalues\n",
    "\n",
    "# FWL estimate\n",
    "tau_fwl_linear = np.cov(Y_tilde_linear, T_tilde_linear)[0, 1] / np.var(T_tilde_linear, ddof=1)\n",
    "\n",
    "# Naive estimate\n",
    "tau_naive_nonlin = sm.OLS(Y_nonlin, sm.add_constant(T_nonlin)).fit().params[1]\n",
    "\n",
    "print(f\"Naive estimate:         τ̂ = {tau_naive_nonlin:.3f}\")\n",
    "print(f\"Linear FWL estimate:    τ̂ = {tau_fwl_linear:.3f}  ← Still biased!\")\n",
    "print(f\"True effect:            τ  = {true_tau_nonlin:.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"Bias from linear FWL: {tau_fwl_linear - true_tau_nonlin:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "**The linear FWL fails!** It only removes the linear component of confounding. The quadratic and cubic parts remain, causing bias.\n",
    "\n",
    "### Visualizing the Residual Confounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that linear residuals are NOT independent of X\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Panel 1: T residuals vs X1² (should be zero if properly adjusted)\n",
    "ax = axes[0]\n",
    "ax.scatter(X1**2, T_tilde_linear, alpha=0.3, s=20, c=TUFTE_PALETTE['primary'])\n",
    "# Add LOWESS smooth\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "smooth = lowess(T_tilde_linear, X1**2, frac=0.3)\n",
    "ax.plot(smooth[:, 0], smooth[:, 1], c=COLORS['red'], lw=2.5, label='Remaining pattern')\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5)\n",
    "ax.set_xlabel('X₁²')\n",
    "ax.set_ylabel('Treatment Residual T̃')\n",
    "ax.set_title('(a) Linear residuals still depend on X₁²!', fontweight='bold')\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# Panel 2: Y residuals vs X2³\n",
    "ax = axes[1]\n",
    "ax.scatter(X2**3, Y_tilde_linear, alpha=0.3, s=20, c=TUFTE_PALETTE['primary'])\n",
    "smooth = lowess(Y_tilde_linear, X2**3, frac=0.3)\n",
    "ax.plot(smooth[:, 0], smooth[:, 1], c=COLORS['red'], lw=2.5, label='Remaining pattern')\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5)\n",
    "ax.set_xlabel('X₂³')\n",
    "ax.set_ylabel('Outcome Residual Ỹ')\n",
    "ax.set_title('(b) Linear residuals still depend on X₂³!', fontweight='bold')\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n★ The residuals show clear patterns — linear partialling didn't remove all confounding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Robinson's Transformation\n",
    "\n",
    "In 1988, Peter Robinson asked: **What if we could estimate $E[T|X]$ and $E[Y|X]$ flexibly?**[^1]\n",
    "\n",
    "### The Partial Linear Model\n",
    "\n",
    "Consider:\n",
    "\n",
    "$$Y = \\tau \\cdot T + g_0(X) + \\varepsilon$$\n",
    "$$T = m_0(X) + v$$\n",
    "\n",
    "where:\n",
    "- $\\tau$ is the treatment effect (our target)\n",
    "- $g_0(X)$ is an **arbitrary** function of confounders → outcome\n",
    "- $m_0(X)$ is an **arbitrary** function of confounders → treatment\n",
    "- $\\varepsilon, v$ are noise terms\n",
    "\n",
    "### The Robinson Insight\n",
    "\n",
    "```\n",
    "★ Robinson's Key Insight (1988) ───────────────────────────\n",
    "  \n",
    "  Take expectations conditional on X:\n",
    "  \n",
    "  E[Y|X] = τ · E[T|X] + g₀(X)\n",
    "  \n",
    "  Subtract from the original equation:\n",
    "  \n",
    "  Y - E[Y|X] = τ · (T - E[T|X]) + ε\n",
    "        Ỹ    = τ ·      T̃       + ε\n",
    "  \n",
    "  The nuisance function g₀(X) cancels out!\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "This is exactly FWL, but now $E[Y|X]$ and $E[T|X]$ can be **any functions** — not just linear!\n",
    "\n",
    "### The Estimator\n",
    "\n",
    "$$\\hat{\\tau} = \\frac{\\sum_i (Y_i - \\hat{\\ell}(X_i))(T_i - \\hat{m}(X_i))}{\\sum_i (T_i - \\hat{m}(X_i))^2}$$\n",
    "\n",
    "where:\n",
    "- $\\hat{\\ell}(X)$ estimates $E[Y|X]$ (e.g., Random Forest)\n",
    "- $\\hat{m}(X)$ estimates $E[T|X]$ (e.g., Random Forest)\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robinson_estimator(Y, T, X, model=None):\n",
    "    \"\"\"\n",
    "    Robinson (1988) semiparametric estimator.\n",
    "    \n",
    "    Uses flexible ML model to estimate E[Y|X] and E[T|X],\n",
    "    then regresses residuals to get treatment effect.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array-like\n",
    "        Outcome variable\n",
    "    T : array-like  \n",
    "        Treatment variable\n",
    "    X : array-like\n",
    "        Confounders (n x p matrix)\n",
    "    model : sklearn estimator, optional\n",
    "        Model for nuisance estimation. Default: RandomForest.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tau_hat : float\n",
    "        Estimated treatment effect\n",
    "    Y_resid : array\n",
    "        Outcome residuals\n",
    "    T_resid : array\n",
    "        Treatment residuals\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, random_state=42)\n",
    "    \n",
    "    # Estimate E[Y|X] using cross-validation to avoid overfitting\n",
    "    Y_hat = cross_val_predict(model, X, Y, cv=5)\n",
    "    \n",
    "    # Estimate E[T|X] using cross-validation\n",
    "    T_hat = cross_val_predict(model, X, T, cv=5)\n",
    "    \n",
    "    # Compute residuals\n",
    "    Y_resid = Y - Y_hat\n",
    "    T_resid = T - T_hat\n",
    "    \n",
    "    # Robinson formula: regress Ỹ on T̃\n",
    "    tau_hat = np.sum(Y_resid * T_resid) / np.sum(T_resid**2)\n",
    "    \n",
    "    return tau_hat, Y_resid, T_resid\n",
    "\n",
    "print(\"Robinson estimator defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Robinson estimator to nonlinear data\n",
    "tau_robinson, Y_resid_rf, T_resid_rf = robinson_estimator(Y_nonlin, T_nonlin, X)\n",
    "\n",
    "print(\"\\nComparison of Estimators\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Naive (no adjustment):      τ̂ = {tau_naive_nonlin:.3f}  (bias = {tau_naive_nonlin - true_tau_nonlin:+.3f})\")\n",
    "print(f\"Linear FWL:                 τ̂ = {tau_fwl_linear:.3f}  (bias = {tau_fwl_linear - true_tau_nonlin:+.3f})\")\n",
    "print(f\"Robinson (Random Forest):   τ̂ = {tau_robinson:.3f}  (bias = {tau_robinson - true_tau_nonlin:+.3f})\")\n",
    "print(f\"True effect:                τ  = {true_tau_nonlin:.3f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "**Robinson with Random Forest works!** The flexible ML model captures the nonlinear confounding that linear FWL missed.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Visual Demonstration\n",
    "\n",
    "Let's create a comprehensive visualization showing why this matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "for ax in axes.flat:\n",
    "    apply_tufte_style(ax)\n",
    "\n",
    "# ===== Row 1: The Data =====\n",
    "\n",
    "# 1a: Nonlinear T vs X1\n",
    "ax = axes[0, 0]\n",
    "scatter_idx = np.random.choice(n, 300, replace=False)  # Subsample for clarity\n",
    "ax.scatter(X1[scatter_idx], T_nonlin[scatter_idx], alpha=0.4, s=20, c=TUFTE_PALETTE['primary'])\n",
    "X1_grid = np.linspace(-2, 2, 100)\n",
    "ax.plot(X1_grid, X1_grid**2, c=COLORS['red'], lw=2.5, label='True: X₁²')\n",
    "ax.plot(X1_grid, np.polyval(np.polyfit(X1, T_nonlin, 1), X1_grid), \n",
    "        c=COLORS['orange'], lw=2, ls='--', label='Linear fit')\n",
    "ax.set_xlabel('X₁')\n",
    "ax.set_ylabel('Treatment T')\n",
    "ax.set_title('(a) T vs X₁: Quadratic Relationship', fontweight='bold')\n",
    "ax.legend(loc='upper center', frameon=False, fontsize=9)\n",
    "\n",
    "# 1b: Nonlinear Y vs X2\n",
    "ax = axes[0, 1]\n",
    "Y_partial = Y_nonlin - true_tau_nonlin * T_nonlin - np.sin(X1 * np.pi)  # Isolate X2 effect\n",
    "ax.scatter(X2[scatter_idx], Y_partial[scatter_idx], alpha=0.4, s=20, c=TUFTE_PALETTE['primary'])\n",
    "X2_grid = np.linspace(-2, 2, 100)\n",
    "ax.plot(X2_grid, 0.5 * X2_grid**3, c=COLORS['red'], lw=2.5, label='True: 0.5·X₂³')\n",
    "ax.plot(X2_grid, np.polyval(np.polyfit(X2, Y_partial, 1), X2_grid), \n",
    "        c=COLORS['orange'], lw=2, ls='--', label='Linear fit')\n",
    "ax.set_xlabel('X₂')\n",
    "ax.set_ylabel('Y (partial)')\n",
    "ax.set_title('(b) Y vs X₂: Cubic Relationship', fontweight='bold')\n",
    "ax.legend(loc='upper left', frameon=False, fontsize=9)\n",
    "\n",
    "# 1c: Estimate comparison\n",
    "ax = axes[0, 2]\n",
    "methods = ['Naive', 'Linear\\nFWL', 'Robinson\\n(RF)']\n",
    "estimates = [tau_naive_nonlin, tau_fwl_linear, tau_robinson]\n",
    "biases = [e - true_tau_nonlin for e in estimates]\n",
    "colors = [COLORS['red'], COLORS['orange'], COLORS['green']]\n",
    "\n",
    "bars = ax.bar(methods, estimates, color=colors, width=0.6, edgecolor='white', linewidth=1.5)\n",
    "ax.axhline(true_tau_nonlin, c=COLORS['blue'], ls='--', lw=2, label=f'True τ = {true_tau_nonlin}')\n",
    "ax.fill_between([-0.5, 2.5], true_tau_nonlin - 0.1, true_tau_nonlin + 0.1, \n",
    "                alpha=0.2, color=COLORS['blue'])\n",
    "\n",
    "for bar, est, bias in zip(bars, estimates, biases):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, est + 0.1, \n",
    "            f'{est:.2f}\\n(bias: {bias:+.2f})', \n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_ylabel('Estimated τ')\n",
    "ax.set_title('(c) Estimator Comparison', fontweight='bold')\n",
    "ax.set_ylim(0, 4)\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "\n",
    "# ===== Row 2: The Residuals =====\n",
    "\n",
    "# 2a: Linear residuals (T̃ vs X1²) - BAD\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(X1**2, T_tilde_linear, alpha=0.3, s=15, c=COLORS['red'])\n",
    "smooth = lowess(T_tilde_linear, X1**2, frac=0.3)\n",
    "ax.plot(smooth[:, 0], smooth[:, 1], c='black', lw=2.5)\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5)\n",
    "ax.set_xlabel('X₁²')\n",
    "ax.set_ylabel('T̃ (linear)')\n",
    "ax.set_title('(d) Linear: Residual confounding remains!', fontweight='bold', color=COLORS['red'])\n",
    "\n",
    "# 2b: RF residuals (T̃ vs X1²) - GOOD\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(X1**2, T_resid_rf, alpha=0.3, s=15, c=COLORS['green'])\n",
    "smooth = lowess(T_resid_rf, X1**2, frac=0.3)\n",
    "ax.plot(smooth[:, 0], smooth[:, 1], c='black', lw=2.5)\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5)\n",
    "ax.set_xlabel('X₁²')\n",
    "ax.set_ylabel('T̃ (RF)')\n",
    "ax.set_title('(e) Robinson: No remaining pattern ✓', fontweight='bold', color=COLORS['green'])\n",
    "\n",
    "# 2c: Final scatter of Ỹ vs T̃\n",
    "ax = axes[1, 2]\n",
    "ax.scatter(T_resid_rf, Y_resid_rf, alpha=0.3, s=15, c=COLORS['green'])\n",
    "z = np.polyfit(T_resid_rf, Y_resid_rf, 1)\n",
    "T_line = np.linspace(T_resid_rf.min(), T_resid_rf.max(), 100)\n",
    "ax.plot(T_line, np.polyval(z, T_line), c='black', lw=2.5, \n",
    "        label=f'slope = {tau_robinson:.3f} ≈ τ')\n",
    "ax.axhline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5)\n",
    "ax.axvline(0, c=TUFTE_PALETTE['spine'], ls='--', lw=0.5)\n",
    "ax.set_xlabel('T̃ (RF residual)')\n",
    "ax.set_ylabel('Ỹ (RF residual)')\n",
    "ax.set_title('(f) Robinson Estimate', fontweight='bold')\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### The Variance Reduction Theorem\n",
    "\n",
    "An important side effect of partialling out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate variance reduction\n",
    "print(\"Variance Reduction from Partialling Out\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Var(T):       {np.var(T_nonlin):.3f}\")\n",
    "print(f\"Var(T̃ linear): {np.var(T_tilde_linear):.3f}  (reduction: {1 - np.var(T_tilde_linear)/np.var(T_nonlin):.1%})\")\n",
    "print(f\"Var(T̃ RF):     {np.var(T_resid_rf):.3f}  (reduction: {1 - np.var(T_resid_rf)/np.var(T_nonlin):.1%})\")\n",
    "print()\n",
    "print(\"The RF partialling removes MORE variance because it captures nonlinear patterns.\")\n",
    "print(f\"\\nR² of T ~ X (linear): {1 - np.var(T_tilde_linear)/np.var(T_nonlin):.3f}\")\n",
    "print(f\"R² of T ~ X (RF):     {1 - np.var(T_resid_rf)/np.var(T_nonlin):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "```\n",
    "★ Variance Reduction Theorem ──────────────────────────────\n",
    "  \n",
    "  Var(T̃) = Var(T) - Var(E[T|X]) ≤ Var(T)\n",
    "  \n",
    "  Better estimation of E[T|X] → More variance reduction\n",
    "  → Smaller standard errors on τ̂\n",
    "  \n",
    "  This is why we want flexible ML models!\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Takeaways\n",
    "\n",
    "| Concept | FWL (Linear) | Robinson (Flexible) |\n",
    "|---------|--------------|--------------------|\n",
    "| $E[T\\|X]$ | Linear in $X$ | Any function (ML) |\n",
    "| $E[Y\\|X]$ | Linear in $X$ | Any function (ML) |\n",
    "| Handles nonlinear confounding | ❌ No | ✓ Yes |\n",
    "| Computational cost | Low | Higher |\n",
    "| Risk of overfitting | None | Needs care (cross-fitting) |\n",
    "\n",
    "```\n",
    "★ The Bridge to Double ML ────────────────────────────────\n",
    "  \n",
    "  Robinson (1988) showed: Replace linear with flexible.\n",
    "  \n",
    "  But two problems remain:\n",
    "  1. ML models are biased (regularization bias)\n",
    "  2. Same data for nuisance and score → overfitting\n",
    "  \n",
    "  These are solved by:\n",
    "  - Neyman orthogonality (Notebook 02)\n",
    "  - Cross-fitting (Notebook 04)\n",
    "  \n",
    "  → Double Machine Learning (Chapter 22)\n",
    "─────────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interview Question\n",
    "\n",
    "**Q (Senior DS)**: *\"How does FWL relate to Double Machine Learning?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points to hit:**\n",
    "\n",
    "1. **FWL is the foundation**: Both FWL and DML use \"partialling out\" — regressing residuals on residuals.\n",
    "\n",
    "2. **FWL limitation**: FWL uses *linear* expectations $E[T|X] = X\\beta$. This fails when confounding is nonlinear.\n",
    "\n",
    "3. **Robinson's extension (1988)**: Replace linear with arbitrary functions. Use flexible estimators like Random Forest.\n",
    "\n",
    "4. **DML additions**: Robinson alone isn't enough because:\n",
    "   - ML models have regularization bias\n",
    "   - Same data for nuisance estimation and score evaluation → overfitting\n",
    "\n",
    "5. **DML solutions**:\n",
    "   - *Neyman orthogonality*: Use score that is insensitive to first-order nuisance errors\n",
    "   - *Cross-fitting*: Estimate nuisance on different folds than evaluation\n",
    "\n",
    "6. **Result**: DML achieves $\\sqrt{n}$-consistency even with slow-rate ML nuisance estimators.\n",
    "\n",
    "**One-liner**: \"FWL is DML's linear special case; DML extends it with flexible ML, orthogonality, and cross-fitting.\"\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. References\n",
    "\n",
    "[^1]: Robinson, P. M. (1988). Root-N-Consistent Semiparametric Regression. *Econometrica*, 56(4), 931-954.\n",
    "\n",
    "[^2]: Frisch, R. and Waugh, F. V. (1933). Partial Time Regressions. *Econometrica*, 1(4), 387-401.\n",
    "\n",
    "[^3]: Chernozhukov, V. et al. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^4]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 5 & 22.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [02. Neyman Orthogonality](./02_neyman_orthogonality.ipynb) — Why DML works despite biased nuisance estimators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
