{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDID Estimation: Unit and Time Weights\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Chapter 25 | Notebook 2 of 3**\n",
    "\n",
    "This notebook provides a deep dive into the weight estimation procedures for\n",
    "Synthetic Difference-in-Differences, using convex optimization (cvxpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from facure_augment.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "# Convex optimization\n",
    "import cvxpy as cp\n",
    "from scipy.optimize import fmin_slsqp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### Why Convex Optimization?\n",
    "\n",
    "The SDID weight estimation problems are **convex quadratic programs**:\n",
    "\n",
    "1. **Objective**: Quadratic (sum of squares + L2 penalty)\n",
    "2. **Constraints**: Linear (sum to 1) and box (non-negative)\n",
    "\n",
    "Convex optimization guarantees:\n",
    "- Global optimum (no local minima traps)\n",
    "- Efficient algorithms (interior point, ADMM)\n",
    "- Numerical stability\n",
    "\n",
    "> **Key Insight**: The simplex constraints ($\\sum w_i = 1$, $w_i \\geq 0$) ensure\n",
    "> interpretable weights that form a valid convex combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 Unit Weight Problem (Primal Form)\n",
    "\n",
    "$$\\min_{w_0, w} \\sum_{t=1}^{T_{pre}} \\left( w_0 + \\sum_{i=1}^{N_{co}} w_i Y_{it} - \\bar{Y}_{tr,t} \\right)^2 + \\zeta^2 T_{pre} \\|w\\|_2^2$$\n",
    "\n",
    "Subject to:\n",
    "- $\\sum_{i=1}^{N_{co}} w_i = 1$ (weights sum to 1)\n",
    "- $w_i \\geq 0$ for all $i$ (non-negative weights)\n",
    "\n",
    "**Matrix form**:\n",
    "$$\\min_{w_0, w} \\|w_0 \\mathbf{1} + X w - y\\|_2^2 + \\zeta^2 T_{pre} \\|w\\|_2^2$$\n",
    "\n",
    "Where:\n",
    "- $X \\in \\mathbb{R}^{T_{pre} \\times N_{co}}$: Control outcomes in pre-period\n",
    "- $y \\in \\mathbb{R}^{T_{pre}}$: Treated outcomes in pre-period\n",
    "- $\\mathbf{1}$: Vector of ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Time Weight Problem (Primal Form)\n",
    "\n",
    "$$\\min_{\\lambda_0, \\lambda} \\sum_{i=1}^{N_{co}} \\left( \\lambda_0 + \\sum_{t=1}^{T_{pre}} \\lambda_t Y_{it} - \\bar{Y}_i^{post} \\right)^2$$\n",
    "\n",
    "Subject to:\n",
    "- $\\sum_{t=1}^{T_{pre}} \\lambda_t = 1$\n",
    "- $\\lambda_t \\geq 0$ for all $t$\n",
    "\n",
    "**Note**: No regularization on time weights—the problem is already well-posed\n",
    "because we're predicting across units (rows), not time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Regularization Parameter\n",
    "\n",
    "The regularization parameter $\\zeta$ is calibrated from the data:\n",
    "\n",
    "$$\\zeta = (N_{tr} \\cdot T_{post})^{1/4} \\cdot \\hat{\\sigma}$$\n",
    "\n",
    "Where $\\hat{\\sigma}$ is the standard deviation of first-differences:\n",
    "\n",
    "$$\\hat{\\sigma} = \\sqrt{\\frac{1}{N_{co}(T_{pre}-1)} \\sum_{i \\in co} \\sum_{t=2}^{T_{pre}} (Y_{it} - Y_{i,t-1})^2}$$\n",
    "\n",
    "**Intuition**: $\\zeta$ scales with:\n",
    "1. Treatment intensity ($N_{tr} \\cdot T_{post}$): More treatment exposure → more regularization\n",
    "2. Outcome volatility ($\\hat{\\sigma}$): More volatile outcomes → more regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load smoking data\n",
    "cigar = load_facure_data(\"smoking.csv\")\n",
    "\n",
    "# Setup\n",
    "calif_state = 3\n",
    "treatment_year = 1989\n",
    "\n",
    "# Pivot to wide format\n",
    "Y_wide = cigar.pivot(index='state', columns='year', values='cigsale')\n",
    "\n",
    "# Extract blocks\n",
    "pre_years = [y for y in Y_wide.columns if y < treatment_year]\n",
    "post_years = [y for y in Y_wide.columns if y >= treatment_year]\n",
    "\n",
    "Y_co_pre = Y_wide.loc[Y_wide.index != calif_state, pre_years].values\n",
    "Y_co_post = Y_wide.loc[Y_wide.index != calif_state, post_years].values\n",
    "Y_tr_pre = Y_wide.loc[calif_state, pre_years].values\n",
    "Y_tr_post = Y_wide.loc[calif_state, post_years].values\n",
    "\n",
    "N_co, T_pre = Y_co_pre.shape\n",
    "T_post = Y_co_post.shape[1]\n",
    "N_tr = 1\n",
    "\n",
    "print(f\"Data dimensions:\")\n",
    "print(f\"  N_co = {N_co}, N_tr = {N_tr}\")\n",
    "print(f\"  T_pre = {T_pre}, T_post = {T_post}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Regularization Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First differences for control units\n",
    "delta = np.diff(Y_co_pre, axis=1)  # (N_co, T_pre-1)\n",
    "\n",
    "# Standard deviation of first differences\n",
    "sigma_delta = np.std(delta, ddof=1)\n",
    "\n",
    "# Regularization parameter\n",
    "zeta = (N_tr * T_post) ** 0.25 * sigma_delta\n",
    "\n",
    "print(f\"Regularization components:\")\n",
    "print(f\"  sigma(delta) = {sigma_delta:.4f}\")\n",
    "print(f\"  (N_tr * T_post)^(1/4) = {(N_tr * T_post)**0.25:.4f}\")\n",
    "print(f\"  zeta = {zeta:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Weight Estimation with cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdid_unit_weights_cvxpy(X, y, zeta, T_pre):\n",
    "    \"\"\"\n",
    "    Estimate SDID unit weights using cvxpy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray (T_pre, N_co)\n",
    "        Control outcomes in pre-period\n",
    "    y : ndarray (T_pre,)\n",
    "        Treated outcomes in pre-period\n",
    "    zeta : float\n",
    "        Regularization parameter\n",
    "    T_pre : int\n",
    "        Number of pre-treatment periods\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w0 : float\n",
    "        Intercept\n",
    "    w : ndarray (N_co,)\n",
    "        Unit weights\n",
    "    \"\"\"\n",
    "    N_co = X.shape[1]\n",
    "    \n",
    "    # Decision variables\n",
    "    w = cp.Variable(N_co)\n",
    "    w0 = cp.Variable()\n",
    "    \n",
    "    # Objective: MSE + L2 penalty\n",
    "    pred = w0 + X @ w\n",
    "    mse = cp.sum_squares(pred - y)\n",
    "    penalty = zeta**2 * T_pre * cp.sum_squares(w)\n",
    "    objective = cp.Minimize(mse + penalty)\n",
    "    \n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        cp.sum(w) == 1,  # Weights sum to 1\n",
    "        w >= 0           # Non-negative\n",
    "    ]\n",
    "    \n",
    "    # Solve\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.OSQP, verbose=False)\n",
    "    \n",
    "    if prob.status != cp.OPTIMAL:\n",
    "        raise ValueError(f\"Optimization failed: {prob.status}\")\n",
    "    \n",
    "    return w0.value, w.value\n",
    "\n",
    "# Estimate unit weights\n",
    "X_unit = Y_co_pre.T  # (T_pre, N_co)\n",
    "y_unit = Y_tr_pre    # (T_pre,)\n",
    "\n",
    "w0_hat, w_hat = sdid_unit_weights_cvxpy(X_unit, y_unit, zeta, T_pre)\n",
    "\n",
    "print(f\"Unit Weight Results:\")\n",
    "print(f\"  Intercept (w0): {w0_hat:.4f}\")\n",
    "print(f\"  Non-zero weights: {np.sum(w_hat > 1e-4)}\")\n",
    "print(f\"  Sum of weights: {np.sum(w_hat):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize unit weights\n",
    "state_ids = Y_wide.index[Y_wide.index != calif_state].values\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Weight distribution\n",
    "ax = axes[0]\n",
    "sorted_idx = np.argsort(w_hat)[::-1]\n",
    "top_k = 15\n",
    "ax.barh(range(top_k), w_hat[sorted_idx[:top_k]], color=COLORS['blue'], alpha=0.7)\n",
    "ax.set_yticks(range(top_k))\n",
    "ax.set_yticklabels([f\"State {state_ids[i]}\" for i in sorted_idx[:top_k]])\n",
    "ax.set_xlabel('Weight')\n",
    "ax.set_title(f'Top {top_k} SDID Unit Weights')\n",
    "ax.invert_yaxis()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Right: Pre-treatment fit\n",
    "ax = axes[1]\n",
    "synth_pre = w0_hat + X_unit @ w_hat\n",
    "ax.plot(pre_years, y_unit, 'o-', color=COLORS['red'], label='California', linewidth=2)\n",
    "ax.plot(pre_years, synth_pre, 's--', color=COLORS['blue'], \n",
    "        label='SDID Synthetic', alpha=0.7)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Cigarette Sales')\n",
    "ax.set_title('Pre-treatment Fit (Unit Weights)')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Fit quality\n",
    "pre_mse = np.mean((y_unit - synth_pre)**2)\n",
    "print(f\"\\nPre-treatment MSE: {pre_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Weight Estimation with cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdid_time_weights_cvxpy(Y_co_pre, Y_co_post):\n",
    "    \"\"\"\n",
    "    Estimate SDID time weights using cvxpy.\n",
    "    \n",
    "    Find weights on pre-periods that best predict post-period mean for controls.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_co_pre : ndarray (N_co, T_pre)\n",
    "        Control outcomes in pre-period\n",
    "    Y_co_post : ndarray (N_co, T_post)\n",
    "        Control outcomes in post-period\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    lambda0 : float\n",
    "        Intercept\n",
    "    lam : ndarray (T_pre,)\n",
    "        Time weights\n",
    "    \"\"\"\n",
    "    N_co, T_pre = Y_co_pre.shape\n",
    "    \n",
    "    # Target: post-period mean for each control\n",
    "    y_target = Y_co_post.mean(axis=1)  # (N_co,)\n",
    "    \n",
    "    # Decision variables\n",
    "    lam = cp.Variable(T_pre)\n",
    "    lambda0 = cp.Variable()\n",
    "    \n",
    "    # Objective: MSE (no penalty for time weights)\n",
    "    pred = lambda0 + Y_co_pre @ lam\n",
    "    objective = cp.Minimize(cp.sum_squares(pred - y_target))\n",
    "    \n",
    "    # Constraints\n",
    "    constraints = [\n",
    "        cp.sum(lam) == 1,\n",
    "        lam >= 0\n",
    "    ]\n",
    "    \n",
    "    # Solve\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.OSQP, verbose=False)\n",
    "    \n",
    "    if prob.status != cp.OPTIMAL:\n",
    "        raise ValueError(f\"Optimization failed: {prob.status}\")\n",
    "    \n",
    "    return lambda0.value, lam.value\n",
    "\n",
    "# Estimate time weights\n",
    "lambda0_hat, lambda_hat = sdid_time_weights_cvxpy(Y_co_pre, Y_co_post)\n",
    "\n",
    "print(f\"Time Weight Results:\")\n",
    "print(f\"  Intercept (lambda0): {lambda0_hat:.4f}\")\n",
    "print(f\"  Non-zero weights: {np.sum(lambda_hat > 1e-4)}\")\n",
    "print(f\"  Sum of weights: {np.sum(lambda_hat):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Time weight distribution\n",
    "ax = axes[0]\n",
    "ax.bar(pre_years, lambda_hat, color=COLORS['green'], alpha=0.7, edgecolor='black')\n",
    "ax.axhline(1/T_pre, color=COLORS['red'], linestyle='--', \n",
    "           label=f'Equal weight: {1/T_pre:.3f}')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Time Weight')\n",
    "ax.set_title('SDID Time Weights')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Right: Prediction fit for controls\n",
    "ax = axes[1]\n",
    "y_target = Y_co_post.mean(axis=1)\n",
    "y_pred = lambda0_hat + Y_co_pre @ lambda_hat\n",
    "\n",
    "ax.scatter(y_target, y_pred, alpha=0.6, color=COLORS['blue'])\n",
    "ax.plot([y_target.min(), y_target.max()], [y_target.min(), y_target.max()], \n",
    "        'k--', alpha=0.5, label='Perfect fit')\n",
    "ax.set_xlabel('Actual Post-Period Mean')\n",
    "ax.set_ylabel('Predicted (from weighted pre-period)')\n",
    "ax.set_title('Time Weight Fit Quality')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R-squared\n",
    "ss_res = np.sum((y_target - y_pred)**2)\n",
    "ss_tot = np.sum((y_target - y_target.mean())**2)\n",
    "r2 = 1 - ss_res / ss_tot\n",
    "print(f\"\\nTime weight prediction R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Full SDID Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDIDEstimator:\n",
    "    \"\"\"\n",
    "    Synthetic Difference-in-Differences estimator.\n",
    "    \n",
    "    Implements Arkhangelsky et al. (2021) using cvxpy for weight estimation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, zeta=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        zeta : float, optional\n",
    "            Regularization parameter. If None, computed from data.\n",
    "        \"\"\"\n",
    "        self.zeta = zeta\n",
    "        self.w0_ = None\n",
    "        self.w_ = None\n",
    "        self.lambda0_ = None\n",
    "        self.lambda_ = None\n",
    "        self.tau_ = None\n",
    "        \n",
    "    def _compute_zeta(self, Y_co_pre, N_tr, T_post):\n",
    "        \"\"\"Compute regularization parameter from data.\"\"\"\n",
    "        delta = np.diff(Y_co_pre, axis=1)\n",
    "        sigma = np.std(delta, ddof=1)\n",
    "        return (N_tr * T_post) ** 0.25 * sigma\n",
    "    \n",
    "    def _estimate_unit_weights(self, X, y, zeta, T_pre):\n",
    "        \"\"\"Estimate unit weights via cvxpy.\"\"\"\n",
    "        N_co = X.shape[1]\n",
    "        w = cp.Variable(N_co)\n",
    "        w0 = cp.Variable()\n",
    "        \n",
    "        pred = w0 + X @ w\n",
    "        objective = cp.Minimize(\n",
    "            cp.sum_squares(pred - y) + zeta**2 * T_pre * cp.sum_squares(w)\n",
    "        )\n",
    "        constraints = [cp.sum(w) == 1, w >= 0]\n",
    "        \n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve(solver=cp.OSQP, verbose=False)\n",
    "        \n",
    "        return w0.value, w.value\n",
    "    \n",
    "    def _estimate_time_weights(self, Y_co_pre, Y_co_post):\n",
    "        \"\"\"Estimate time weights via cvxpy.\"\"\"\n",
    "        N_co, T_pre = Y_co_pre.shape\n",
    "        y_target = Y_co_post.mean(axis=1)\n",
    "        \n",
    "        lam = cp.Variable(T_pre)\n",
    "        lambda0 = cp.Variable()\n",
    "        \n",
    "        pred = lambda0 + Y_co_pre @ lam\n",
    "        objective = cp.Minimize(cp.sum_squares(pred - y_target))\n",
    "        constraints = [cp.sum(lam) == 1, lam >= 0]\n",
    "        \n",
    "        prob = cp.Problem(objective, constraints)\n",
    "        prob.solve(solver=cp.OSQP, verbose=False)\n",
    "        \n",
    "        return lambda0.value, lam.value\n",
    "    \n",
    "    def fit(self, Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post):\n",
    "        \"\"\"\n",
    "        Fit the SDID estimator.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y_tr_pre : ndarray (T_pre,) or (N_tr, T_pre)\n",
    "            Treated unit(s) pre-treatment outcomes\n",
    "        Y_tr_post : ndarray (T_post,) or (N_tr, T_post)\n",
    "            Treated unit(s) post-treatment outcomes\n",
    "        Y_co_pre : ndarray (N_co, T_pre)\n",
    "            Control units pre-treatment outcomes\n",
    "        Y_co_post : ndarray (N_co, T_post)\n",
    "            Control units post-treatment outcomes\n",
    "        \"\"\"\n",
    "        # Ensure correct shapes\n",
    "        Y_tr_pre = np.atleast_1d(Y_tr_pre).flatten()\n",
    "        Y_tr_post = np.atleast_1d(Y_tr_post).flatten()\n",
    "        \n",
    "        N_co, T_pre = Y_co_pre.shape\n",
    "        T_post = Y_co_post.shape[1]\n",
    "        N_tr = 1  # Currently supports single treated unit\n",
    "        \n",
    "        # Compute zeta if not provided\n",
    "        if self.zeta is None:\n",
    "            self.zeta = self._compute_zeta(Y_co_pre, N_tr, T_post)\n",
    "        \n",
    "        # Estimate unit weights\n",
    "        X_unit = Y_co_pre.T  # (T_pre, N_co)\n",
    "        self.w0_, self.w_ = self._estimate_unit_weights(\n",
    "            X_unit, Y_tr_pre, self.zeta, T_pre\n",
    "        )\n",
    "        \n",
    "        # Estimate time weights\n",
    "        self.lambda0_, self.lambda_ = self._estimate_time_weights(\n",
    "            Y_co_pre, Y_co_post\n",
    "        )\n",
    "        \n",
    "        # Compute SDID estimate\n",
    "        # Post difference\n",
    "        Y_tr_post_mean = Y_tr_post.mean()\n",
    "        Y_co_post_weighted = (Y_co_post.T @ self.w_).mean()\n",
    "        \n",
    "        # Pre difference (time-weighted)\n",
    "        Y_tr_pre_weighted = Y_tr_pre @ self.lambda_\n",
    "        Y_co_pre_weighted = (Y_co_pre.T @ self.w_) @ self.lambda_\n",
    "        \n",
    "        # Double difference\n",
    "        self.tau_ = (Y_tr_post_mean - Y_co_post_weighted) - \\\n",
    "                    (Y_tr_pre_weighted - Y_co_pre_weighted)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def ate(self):\n",
    "        \"\"\"Average treatment effect estimate.\"\"\"\n",
    "        return self.tau_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the estimator\n",
    "sdid = SDIDEstimator()\n",
    "sdid.fit(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post)\n",
    "\n",
    "print(f\"SDID Estimate: {sdid.ate:.2f} packs/capita\")\n",
    "print(f\"\")\n",
    "print(f\"Estimated parameters:\")\n",
    "print(f\"  Regularization (zeta): {sdid.zeta:.4f}\")\n",
    "print(f\"  Unit intercept (w0): {sdid.w0_:.4f}\")\n",
    "print(f\"  Non-zero unit weights: {np.sum(sdid.w_ > 1e-4)}\")\n",
    "print(f\"  Time intercept (lambda0): {sdid.lambda0_:.4f}\")\n",
    "print(f\"  Non-zero time weights: {np.sum(sdid.lambda_ > 1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: SDID vs DiD vs SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiD estimate (equal weights)\n",
    "tau_did = (Y_tr_post.mean() - Y_tr_pre.mean()) - \\\n",
    "          (Y_co_post.mean() - Y_co_pre.mean())\n",
    "\n",
    "# SC estimate (unit weights only, no time weights)\n",
    "def sc_estimate(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post):\n",
    "    X = Y_co_pre.T\n",
    "    y = Y_tr_pre\n",
    "    N_co = X.shape[1]\n",
    "    \n",
    "    w = cp.Variable(N_co)\n",
    "    objective = cp.Minimize(cp.sum_squares(X @ w - y))\n",
    "    constraints = [cp.sum(w) == 1, w >= 0]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.OSQP, verbose=False)\n",
    "    \n",
    "    w_sc = w.value\n",
    "    Y_synth_post = Y_co_post.T @ w_sc\n",
    "    return Y_tr_post.mean() - Y_synth_post.mean()\n",
    "\n",
    "tau_sc = sc_estimate(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"ESTIMATOR COMPARISON\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"DiD:  {tau_did:.2f} packs/capita\")\n",
    "print(f\"SC:   {tau_sc:.2f} packs/capita\")\n",
    "print(f\"SDID: {sdid.ate:.2f} packs/capita\")\n",
    "print(f\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does tau change with different zeta values?\n",
    "zeta_grid = np.linspace(0.1, 50, 50)\n",
    "tau_grid = []\n",
    "\n",
    "for z in zeta_grid:\n",
    "    sdid_z = SDIDEstimator(zeta=z)\n",
    "    sdid_z.fit(Y_tr_pre, Y_tr_post, Y_co_pre, Y_co_post)\n",
    "    tau_grid.append(sdid_z.ate)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(zeta_grid, tau_grid, 'o-', color=COLORS['blue'], markersize=3, label='SDID')\n",
    "ax.axhline(tau_did, color=COLORS['red'], linestyle='--', label=f'DiD: {tau_did:.1f}')\n",
    "ax.axhline(tau_sc, color=COLORS['green'], linestyle='--', label=f'SC: {tau_sc:.1f}')\n",
    "ax.axvline(sdid.zeta, color='gray', linestyle=':', alpha=0.7, \n",
    "           label=f'Data-driven zeta: {sdid.zeta:.1f}')\n",
    "\n",
    "ax.set_xlabel(r'Regularization Parameter ($\\zeta$)')\n",
    "ax.set_ylabel('Treatment Effect Estimate')\n",
    "ax.set_title('SDID Sensitivity to Regularization')\n",
    "ax.legend(loc='best')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Insight: As zeta increases, SDID converges toward DiD (uniform weights).\")\n",
    "print(f\"As zeta decreases, SDID converges toward SC (sparse weights).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: Why does SDID use an intercept in the unit weight problem?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Level shift flexibility**: The intercept $w_0$ allows a constant shift between\n",
    "   treated and synthetic control. This is important because:\n",
    "   - SC without intercept forces exact level matching\n",
    "   - SDID with intercept allows different levels, focuses on matching *changes*\n",
    "\n",
    "2. **Connection to DiD**: DiD implicitly has an intercept (unit fixed effects).\n",
    "   The SDID intercept maintains this feature.\n",
    "\n",
    "3. **Reduced sensitivity**: Without intercept, weights must compensate for level\n",
    "   differences. With intercept, weights focus on trend matching.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Explain the L2 penalty in SDID unit weights. Why not L1?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **L2 (Ridge) properties**:\n",
    "   - Shrinks weights toward uniformity (DiD-like)\n",
    "   - All weights remain non-zero\n",
    "   - Smooth interpolation between SC and DiD\n",
    "\n",
    "2. **Why not L1 (Lasso)?**:\n",
    "   - L1 would encourage sparsity (few non-zero weights)\n",
    "   - Already constrained to simplex (sum=1, non-negative)\n",
    "   - L2 + simplex gives desired interpolation behavior\n",
    "\n",
    "3. **Theoretical motivation**:\n",
    "   - L2 penalty ensures weights are \"spread out\" when zeta is large\n",
    "   - This recovers DiD in the limit (all controls weighted equally)\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: What is the role of time weights in SDID?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "1. **Purpose**: Find pre-treatment periods that best predict post-treatment\n",
    "   outcomes for control units.\n",
    "\n",
    "2. **Why this matters**:\n",
    "   - Not all pre-periods are equally informative\n",
    "   - Some periods may be outliers or structurally different\n",
    "   - Time weights emphasize \"relevant\" pre-periods\n",
    "\n",
    "3. **Connection to parallel trends**:\n",
    "   - If parallel trends hold exactly, all pre-periods are equally informative\n",
    "   - Time weights adapt when some periods are more/less relevant\n",
    "   - This makes SDID more robust to partial parallel trends violations\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Arkhangelsky, D., Athey, S., Hirshberg, D., Imbens, G., & Wager, S. (2021). \n",
    "      Synthetic Difference-in-Differences. *American Economic Review*, 111(12), 4088-4118.\n",
    "\n",
    "[^2]: Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, Chapter 25."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
