{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing with Orthogonalization: FWL Refresher\n",
    "\n",
    "## Table of Contents\n",
    "1. [Intuition](#intuition)\n",
    "2. [Formal Treatment](#formal)\n",
    "3. [Numeric Demonstration](#numeric)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Interview Appendix](#interview)\n",
    "6. [References](#references)\n",
    "\n",
    "---\n",
    "\n",
    "**Appendix A2 | Notebook 1 of 3**\n",
    "\n",
    "The Frisch-Waugh-Lovell theorem is the foundation for modern causal ML.\n",
    "This notebook reviews the theorem and its geometric interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "module_path = str(Path.cwd().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from facure_augment.common import *\n",
    "set_notebook_style()\n",
    "\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Intuition {#intuition}\n",
    "\n",
    "### Linear Regression Reborn\n",
    "\n",
    "The Frisch-Waugh-Lovell (FWL) theorem, discovered in 1933, reveals the inner\n",
    "workings of linear regression. It states:\n",
    "\n",
    "> You can decompose any multivariate regression into three stages,\n",
    "> and get the **same** coefficient estimates.\n",
    "\n",
    "### Why This Matters for Causal Inference\n",
    "\n",
    "FWL shows that regression estimates a coefficient by:\n",
    "1. **Debiasing**: Removing confounding variation from the treatment\n",
    "2. **Denoising**: Removing extraneous variation from the outcome\n",
    "3. **Estimating**: Regressing the \"clean\" outcome on the \"clean\" treatment\n",
    "\n",
    "This is exactly what we need for causal inference from observational data!\n",
    "\n",
    "### The Core Insight\n",
    "\n",
    "Each regression coefficient answers:\n",
    "> \"How much does Y change when I increase X, **holding all else fixed**?\"\n",
    "\n",
    "FWL reveals HOW regression achieves this \"holding fixed\" operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment {#formal}\n",
    "\n",
    "### 2.1 The FWL Theorem\n",
    "\n",
    "**Setup**: Partition features into two sets $X_1$ and $X_2$.\n",
    "\n",
    "Full regression:\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon$$\n",
    "\n",
    "**Theorem**: The coefficient $\\hat{\\beta}_2$ from the full regression equals\n",
    "the coefficient from the following three-stage procedure:\n",
    "\n",
    "**Stage 1** (Denoise Y):\n",
    "$$Y = \\theta_0 + \\theta_1 X_1 + e_Y$$\n",
    "$$Y^* = Y - (\\hat{\\theta}_0 + \\hat{\\theta}_1 X_1)$$\n",
    "\n",
    "**Stage 2** (Debias X₂):\n",
    "$$X_2 = \\gamma_0 + \\gamma_1 X_1 + e_X$$\n",
    "$$X_2^* = X_2 - (\\hat{\\gamma}_0 + \\hat{\\gamma}_1 X_1)$$\n",
    "\n",
    "**Stage 3** (Estimate):\n",
    "$$Y^* = \\alpha_0 + \\beta_2 X_2^* + \\epsilon$$\n",
    "\n",
    "**Result**: $\\hat{\\beta}_2^{\\text{FWL}} = \\hat{\\beta}_2^{\\text{Full}}$\n",
    "\n",
    "### 2.2 The Residual Maker Matrix\n",
    "\n",
    "Define the residual maker (annihilator):\n",
    "$$M_1 = I - X_1(X_1'X_1)^{-1}X_1'$$\n",
    "\n",
    "This matrix \"projects out\" $X_1$ from any vector:\n",
    "- $M_1 Y$ gives residuals of regressing $Y$ on $X_1$\n",
    "- $M_1 X_2$ gives residuals of regressing $X_2$ on $X_1$\n",
    "\n",
    "**FWL in matrix form**:\n",
    "$$\\hat{\\beta}_2 = (X_2' M_1 X_2)^{-1} X_2' M_1 Y$$\n",
    "\n",
    "### 2.3 Causal Interpretation\n",
    "\n",
    "For causal inference, let:\n",
    "- $X_2 = T$ (treatment)\n",
    "- $X_1 = X$ (confounders)\n",
    "\n",
    "Then FWL says:\n",
    "$$\\hat{\\tau} = \\frac{\\text{Cov}(\\tilde{Y}, \\tilde{T})}{\\text{Var}(\\tilde{T})}$$\n",
    "\n",
    "where:\n",
    "- $\\tilde{Y} = Y - E[Y|X]$ (outcome residual)\n",
    "- $\\tilde{T} = T - E[T|X]$ (treatment residual)\n",
    "\n",
    "**Key insight**: $\\tilde{T}$ is the \"as-if-random\" variation in treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Numeric Demonstration {#numeric}\n",
    "\n",
    "### Ice Cream Pricing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ice cream data\n",
    "prices = load_facure_data(\"ice_cream_sales.csv\")\n",
    "print(f\"Data shape: {prices.shape}\")\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations - note positive price-sales correlation (confounded!)\n",
    "print(\"Correlation matrix:\")\n",
    "print(prices.corr().round(3))\n",
    "\n",
    "print(\"\\n⚠️  Price-sales correlation is POSITIVE (0.080)!\")\n",
    "print(\"   This makes no economic sense - clearly confounded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Regression vs FWL Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Full regression\n",
    "full_model = smf.ols(\"sales ~ price + cost + temp + C(weekday)\", data=prices).fit()\n",
    "beta_price_full = full_model.params['price']\n",
    "\n",
    "print(\"Full Regression Coefficient on Price:\")\n",
    "print(f\"  β_price = {beta_price_full:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: FWL three-stage procedure\n",
    "\n",
    "# Stage 1: Regress Y on confounders, get residuals\n",
    "model_y = smf.ols(\"sales ~ cost + temp + C(weekday)\", data=prices).fit()\n",
    "Y_star = prices['sales'] - model_y.predict(prices)\n",
    "\n",
    "# Stage 2: Regress T on confounders, get residuals\n",
    "model_t = smf.ols(\"price ~ cost + temp + C(weekday)\", data=prices).fit()\n",
    "T_star = prices['price'] - model_t.predict(prices)\n",
    "\n",
    "# Stage 3: Regress Y* on T*\n",
    "residual_data = pd.DataFrame({'Y_star': Y_star, 'T_star': T_star})\n",
    "final_model = smf.ols(\"Y_star ~ T_star\", data=residual_data).fit()\n",
    "beta_price_fwl = final_model.params['T_star']\n",
    "\n",
    "print(\"FWL Three-Stage Coefficient on Price:\")\n",
    "print(f\"  β_price = {beta_price_fwl:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify they match\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"  Full regression:  {beta_price_full:.10f}\")\n",
    "print(f\"  FWL three-stage:  {beta_price_fwl:.10f}\")\n",
    "print(f\"  Difference:       {abs(beta_price_full - beta_price_fwl):.2e}\")\n",
    "print(f\"\\n✓ Coefficients match to numerical precision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs debiased data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original: confounded\n",
    "ax = axes[0]\n",
    "sample = prices.sample(1000, random_state=42)\n",
    "for wd in sorted(prices['weekday'].unique()):\n",
    "    wd_data = sample[sample['weekday'] == wd]\n",
    "    label = 'Weekend' if wd >= 5 else f'Day {wd}'\n",
    "    ax.scatter(wd_data['price'], wd_data['sales'], alpha=0.3, s=10, label=label if wd in [1, 6] else None)\n",
    "ax.set_xlabel('Price')\n",
    "ax.set_ylabel('Sales')\n",
    "ax.set_title('Original Data (Confounded)\\nWeekend → Higher prices AND sales')\n",
    "ax.legend()\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "# Debiased: residualized\n",
    "ax = axes[1]\n",
    "residual_sample = pd.DataFrame({\n",
    "    'T_star': T_star.iloc[sample.index],\n",
    "    'Y_star': Y_star.iloc[sample.index],\n",
    "    'weekday': sample['weekday'].values\n",
    "})\n",
    "for wd in sorted(prices['weekday'].unique()):\n",
    "    wd_data = residual_sample[residual_sample['weekday'] == wd]\n",
    "    ax.scatter(wd_data['T_star'], wd_data['Y_star'], alpha=0.3, s=10)\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Price Residual (T*)')\n",
    "ax.set_ylabel('Sales Residual (Y*)')\n",
    "ax.set_title('Debiased Data (FWL Residuals)\\nConfounding removed: clear negative slope')\n",
    "apply_tufte_style(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations in debiased data\n",
    "print(\"Original correlations:\")\n",
    "print(f\"  Corr(price, sales) = {prices['price'].corr(prices['sales']):.4f}\")\n",
    "print(f\"  Corr(weekday, price) = {prices['weekday'].corr(prices['price']):.4f}\")\n",
    "\n",
    "print(\"\\nDebiased correlations:\")\n",
    "print(f\"  Corr(T*, Y*) = {T_star.corr(Y_star):.4f}  ← Now NEGATIVE!\")\n",
    "print(f\"  Corr(weekday, T*) = {prices['weekday'].corr(T_star):.6f}  ← Near zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Insight ─────────────────────────────────────────────────────\n",
    "FWL reveals what regression is actually doing:\n",
    "\n",
    "1. **Debiasing T**: Remove confounder influence from treatment\n",
    "   - T* = T - E[T|X] has no correlation with X\n",
    "   - T* is \"as-if-random\" variation\n",
    "\n",
    "2. **Denoising Y**: Remove confounder influence from outcome\n",
    "   - Y* = Y - E[Y|X] has reduced variance\n",
    "   - Easier to see treatment effect\n",
    "\n",
    "3. **Estimating**: Simple regression of Y* on T*\n",
    "   - Only the \"clean\" variation remains\n",
    "   - Coefficient = causal effect (under CIA)\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Implementation {#implementation}\n",
    "\n",
    "### Residual Maker Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_maker(X):\n",
    "    \"\"\"Compute the residual maker (annihilator) matrix.\n",
    "    \n",
    "    M = I - X(X'X)^{-1}X'\n",
    "    \n",
    "    For any vector v: Mv gives the residuals from regressing v on X.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    n = X.shape[0]\n",
    "    I = np.eye(n)\n",
    "    # Add constant if not present\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    X_with_const = np.column_stack([np.ones(n), X])\n",
    "    M = I - X_with_const @ np.linalg.inv(X_with_const.T @ X_with_const) @ X_with_const.T\n",
    "    return M\n",
    "\n",
    "# Demonstration on simple data\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "X = np.random.randn(n)\n",
    "Y = 2 + 3*X + np.random.randn(n)\n",
    "\n",
    "M = residual_maker(X)\n",
    "Y_resid_matrix = M @ Y\n",
    "\n",
    "# Compare to statsmodels\n",
    "Y_resid_sm = smf.ols(\"Y ~ X\", data=pd.DataFrame({'X': X, 'Y': Y})).fit().resid\n",
    "\n",
    "print(\"Residual maker verification:\")\n",
    "print(f\"  Max difference from statsmodels: {np.max(np.abs(Y_resid_matrix - Y_resid_sm)):.2e}\")\n",
    "print(f\"  ✓ Residual maker produces correct residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWL Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwl_coefficient(Y, T, X):\n",
    "    \"\"\"Compute coefficient on T using FWL decomposition.\n",
    "    \n",
    "    This is equivalent to the coefficient from:\n",
    "        Y ~ T + X\n",
    "    \n",
    "    But computed via residualization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : array-like\n",
    "        Outcome variable\n",
    "    T : array-like  \n",
    "        Treatment variable\n",
    "    X : array-like\n",
    "        Control variables\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Coefficient on T\n",
    "    \"\"\"\n",
    "    Y, T, X = np.asarray(Y), np.asarray(T), np.asarray(X)\n",
    "    \n",
    "    # Residual maker for X\n",
    "    M = residual_maker(X)\n",
    "    \n",
    "    # Residualize Y and T\n",
    "    Y_star = M @ Y\n",
    "    T_star = M @ T\n",
    "    \n",
    "    # Regress Y* on T*\n",
    "    beta = np.sum(T_star * Y_star) / np.sum(T_star**2)\n",
    "    \n",
    "    return beta\n",
    "\n",
    "# Verify on ice cream data\n",
    "X_controls = prices[['cost', 'temp']].values  # Simplified for demo\n",
    "beta_fwl_func = fwl_coefficient(prices['sales'].values, prices['price'].values, X_controls)\n",
    "\n",
    "# Compare to statsmodels\n",
    "sm_model = smf.ols(\"sales ~ price + cost + temp\", data=prices).fit()\n",
    "\n",
    "print(f\"FWL function: {beta_fwl_func:.6f}\")\n",
    "print(f\"statsmodels:  {sm_model.params['price']:.6f}\")\n",
    "print(f\"Difference:   {abs(beta_fwl_func - sm_model.params['price']):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "★ Key Takeaway ────────────────────────────────────────────────\n",
    "FWL provides the theoretical foundation for:\n",
    "\n",
    "1. **Double/Debiased ML (DML)**: Replace linear residualization with ML\n",
    "2. **Partial regression plots**: Visualize coefficient relationships\n",
    "3. **Understanding control variables**: What \"controlling for\" means\n",
    "4. **Cross-fitting**: Why sample splitting is necessary for ML\n",
    "\n",
    "The next notebooks extend FWL to nonparametric settings.\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix {#interview}\n",
    "\n",
    "### Q1: State the Frisch-Waugh-Lovell theorem and explain its significance.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Statement**: For regression $Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon$:\n",
    "\n",
    "$$\\hat{\\beta}_2 = (X_2' M_1 X_2)^{-1} X_2' M_1 Y$$\n",
    "\n",
    "where $M_1 = I - X_1(X_1'X_1)^{-1}X_1'$ is the residual maker.\n",
    "\n",
    "**Equivalently**: $\\hat{\\beta}_2$ from the full regression equals the coefficient from:\n",
    "1. Regress Y on X₁, get residuals Y*\n",
    "2. Regress X₂ on X₁, get residuals X₂*\n",
    "3. Regress Y* on X₂*\n",
    "\n",
    "**Significance**:\n",
    "1. Shows what regression actually does to estimate coefficients\n",
    "2. Reveals \"controlling for\" as residualization\n",
    "3. Foundation for Double/Debiased ML\n",
    "4. Enables nonparametric extensions (replace OLS with ML)\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q2: Why do we residualize both Y AND T? Isn't residualizing T enough?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Residualizing T** (debiasing):\n",
    "- Removes confounding from treatment\n",
    "- Creates \"as-if-random\" treatment variation\n",
    "- **This is necessary for identification**\n",
    "\n",
    "**Residualizing Y** (denoising):\n",
    "- Removes variance in Y explained by controls\n",
    "- Reduces residual variance\n",
    "- **This improves precision but isn't necessary for unbiasedness**\n",
    "\n",
    "**Mathematical proof**:\n",
    "- If we only residualize T: $\\hat{\\beta} = \\frac{\\text{Cov}(Y, T^*)}{\\text{Var}(T^*)}$\n",
    "- With both residualized: $\\hat{\\beta} = \\frac{\\text{Cov}(Y^*, T^*)}{\\text{Var}(T^*)}$\n",
    "- These are equal! But $\\text{Var}(Y^*) < \\text{Var}(Y)$, so SE is lower.\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q3: How does FWL relate to omitted variable bias?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Without FWL (omitting X₁)**:\n",
    "$$\\hat{\\beta}_2^{\\text{short}} = \\beta_2 + \\gamma \\cdot \\delta$$\n",
    "where $\\delta = \\text{Cov}(X_1, X_2)/\\text{Var}(X_2)$\n",
    "\n",
    "**With FWL**:\n",
    "- Residualization removes X₁'s influence from X₂\n",
    "- $X_2^* = X_2 - E[X_2|X_1]$ is orthogonal to X₁\n",
    "- Hence $\\delta^* = 0$ (no omitted variable bias)\n",
    "\n",
    "**Key insight**: FWL achieves control by orthogonalization.\n",
    "- We don't need X₁ in the final regression\n",
    "- Its influence has been \"projected out\"\n",
    "- This is what \"controlling for\" fundamentally means\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q4: Explain why FWL is the foundation for Double ML.\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**FWL insight**: Coefficient estimation can be decomposed into:\n",
    "1. Predict Y from X → get Y residuals\n",
    "2. Predict T from X → get T residuals\n",
    "3. Regress Y residuals on T residuals\n",
    "\n",
    "**Double ML extension**:\n",
    "- Replace linear predictions with ML models\n",
    "- Use flexible estimators (forests, neural nets)\n",
    "- Handle high-dimensional, nonlinear confounding\n",
    "\n",
    "**Why it works**:\n",
    "- ML estimates $E[Y|X]$ and $E[T|X]$ nonparametrically\n",
    "- Residuals are still valid for causal estimation\n",
    "- Neyman orthogonality protects against ML bias\n",
    "\n",
    "**Critical requirement**: Cross-fitting to avoid overfitting\n",
    "\n",
    "</details>\n",
    "\n",
    "### Q5: What is the \"residual maker\" or \"annihilator\" matrix?\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Definition**:\n",
    "$$M_X = I - X(X'X)^{-1}X' = I - P_X$$\n",
    "\n",
    "where $P_X = X(X'X)^{-1}X'$ is the projection (hat) matrix.\n",
    "\n",
    "**Properties**:\n",
    "1. $M_X \\cdot v$ = residuals from regressing v on X\n",
    "2. $M_X^2 = M_X$ (idempotent)\n",
    "3. $M_X X = 0$ (annihilates X)\n",
    "4. $\\text{rank}(M_X) = n - p$ (dimension reduction)\n",
    "\n",
    "**Geometric interpretation**:\n",
    "- $P_X$ projects onto column space of X\n",
    "- $M_X$ projects onto orthogonal complement\n",
    "- Residuals lie in the null space of X'\n",
    "\n",
    "**Use in FWL**:\n",
    "$$\\hat{\\beta}_2 = (X_2' M_1 X_2)^{-1} X_2' M_1 Y$$\n",
    "\n",
    "We're regressing M₁Y on M₁X₂ (both projected orthogonal to X₁).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References {#references}\n",
    "\n",
    "[^1]: Frisch, R., & Waugh, F. V. (1933). Partial Time Regressions as Compared with\n",
    "      Individual Trends. *Econometrica*, 1(4), 387-401.\n",
    "\n",
    "[^2]: Lovell, M. C. (1963). Seasonal Adjustment of Economic Time Series and Multiple\n",
    "      Regression Analysis. *Journal of the American Statistical Association*, 58(304), 993-1010.\n",
    "\n",
    "[^3]: Facure, M. (2022). *Causal Inference for the Brave and True*, Appendix:\n",
    "      Debiasing with Orthogonalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
