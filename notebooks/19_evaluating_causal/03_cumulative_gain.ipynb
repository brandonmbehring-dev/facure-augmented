{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 19.3 Cumulative Gain Curves\n",
    "\n",
    "**Chapter**: 19 - Evaluating Causal Models  \n",
    "**Section**: 3 - Cumulative Gain Curves  \n",
    "**Facure Source**: 19-Evaluating-Causal-Models.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-16\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [From Bands to Curves](#11-from-bands-to-curves)\n",
    "   - 1.2 [The Cumulative Idea](#12-the-cumulative-idea)\n",
    "2. [Formal Treatment](#2-formal-treatment)\n",
    "   - 2.1 [Cumulative Sensitivity Curve](#21-cumulative-sensitivity-curve)\n",
    "   - 2.2 [Cumulative Gain Curve](#22-cumulative-gain-curve)\n",
    "3. [Implementation](#3-implementation)\n",
    "4. [Numeric Demonstration](#4-numeric-demonstration)\n",
    "5. [Confidence Intervals](#5-confidence-intervals)\n",
    "6. [Interview Appendix](#6-interview-appendix)\n",
    "7. [References](#7-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from toolz import curry\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(123)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: The cumulative gain curve is the causal equivalent of the ROC curve. Understanding this is essential for applied causal work.\n",
    "\n",
    "### 1.1 From Bands to Curves\n",
    "\n",
    "The sensitivity-by-band approach shows **discrete** buckets. But we want:\n",
    "1. A **continuous** evaluation\n",
    "2. A **single metric** to compare models\n",
    "\n",
    "The **cumulative gain curve** provides both.\n",
    "\n",
    "### 1.2 The Cumulative Idea\n",
    "\n",
    "1. **Sort** units by predicted CATE (highest first)\n",
    "2. **Accumulate**: For each k from 1 to N:\n",
    "   - Take the top k units\n",
    "   - Estimate their average treatment effect\n",
    "3. **Plot**: k/N on x-axis, cumulative effect on y-axis\n",
    "\n",
    "```\n",
    "Cumulative Sensitivity Curve ────────────────────────────\n",
    "\n",
    "  Y-axis: Sensitivity of top k% units\n",
    "  X-axis: % of population (top k%)\n",
    "  \n",
    "  Good model: \n",
    "    - Starts HIGH (top units have high effect)\n",
    "    - Slowly decreases toward ATE\n",
    "    \n",
    "  Random model:\n",
    "    - Flat line at ATE\n",
    "    - No ordering ability\n",
    "─────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "prices = load_facure_data('ice_cream_sales.csv')  # Non-random (training)\n",
    "prices_rnd = load_facure_data('ice_cream_sales_rnd.csv')  # Random (evaluation)\n",
    "\n",
    "# Build models\n",
    "X = [\"temp\", \"weekday\", \"cost\", \"price\"]\n",
    "y = \"sales\"\n",
    "\n",
    "# CATE model\n",
    "m1 = smf.ols(\"sales ~ price*cost + price*C(weekday) + price*temp\", data=prices).fit()\n",
    "\n",
    "# Predictive model\n",
    "m2 = GradientBoostingRegressor(max_depth=5, n_estimators=100, random_state=42)\n",
    "m2.fit(prices[X], prices[y])\n",
    "\n",
    "# Generate predictions\n",
    "def predict_sensitivity(model, df, h=0.01):\n",
    "    df_plus = df.copy()\n",
    "    df_plus['price'] = df['price'] + h\n",
    "    return (model.predict(df_plus) - model.predict(df)) / h\n",
    "\n",
    "prices_rnd_pred = prices_rnd.copy()\n",
    "prices_rnd_pred['sensitivity_m_pred'] = predict_sensitivity(m1, prices_rnd)\n",
    "prices_rnd_pred['pred_m_pred'] = m2.predict(prices_rnd[X])\n",
    "prices_rnd_pred['rand_m_pred'] = np.random.uniform(size=len(prices_rnd))\n",
    "\n",
    "print(\"Data and models ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Formal Treatment\n",
    "\n",
    "### 2.1 Cumulative Sensitivity Curve\n",
    "\n",
    "For the top k units (sorted by predicted CATE), the **cumulative sensitivity** is:\n",
    "\n",
    "$$\\hat{\\tau}_k = \\frac{\\sum_{i=1}^{k}(T_i - \\bar{T})(Y_i - \\bar{Y})}{\\sum_{i=1}^{k}(T_i - \\bar{T})^2}$$\n",
    "\n",
    "**Properties**:\n",
    "- At k = N (all units): $\\hat{\\tau}_N$ = overall ATE\n",
    "- Good model: $\\hat{\\tau}_1 > \\hat{\\tau}_2 > ... > \\hat{\\tau}_N$\n",
    "\n",
    "### 2.2 Cumulative Gain Curve\n",
    "\n",
    "The **cumulative gain** normalizes by sample proportion:\n",
    "\n",
    "$$G_k = \\hat{\\tau}_k \\times \\frac{k}{N}$$\n",
    "\n",
    "**Why normalize?**\n",
    "- All curves start at 0 and end at ATE\n",
    "- Easier to compare models visually\n",
    "- Random model = straight line from (0,0) to (1, ATE)\n",
    "\n",
    "**Interpretation**:\n",
    "- X-axis: Fraction of population treated\n",
    "- Y-axis: Total gain (effect × proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@curry\n",
    "def sensitivity(data, y, t):\n",
    "    \"\"\"Estimate sensitivity (slope) using simple linear regression.\"\"\"\n",
    "    t_bar = data[t].mean()\n",
    "    y_bar = data[y].mean()\n",
    "    cov = np.sum((data[t] - t_bar) * (data[y] - y_bar))\n",
    "    var = np.sum((data[t] - t_bar) ** 2)\n",
    "    return cov / var if var > 0 else 0\n",
    "\n",
    "def cumulative_sensitivity_curve(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    \"\"\"\n",
    "    Compute cumulative sensitivity curve.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : DataFrame\n",
    "    prediction : str, column with predictions (higher = higher expected effect)\n",
    "    y : str, outcome column\n",
    "    t : str, treatment column\n",
    "    min_periods : int, minimum sample size to start\n",
    "    steps : int, number of points on curve\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array : cumulative sensitivity at each point\n",
    "    \"\"\"\n",
    "    size = dataset.shape[0]\n",
    "    \n",
    "    # Sort by prediction (highest first)\n",
    "    ordered_df = dataset.sort_values(prediction, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Create sequence of sample sizes\n",
    "    n_rows = list(range(min_periods, size, size // steps)) + [size]\n",
    "    \n",
    "    # Compute cumulative sensitivity at each point\n",
    "    return np.array([sensitivity(ordered_df.head(rows), y, t) for rows in n_rows])\n",
    "\n",
    "# Test\n",
    "overall_sens = sensitivity(prices_rnd_pred, 'sales', 'price')\n",
    "print(f\"Overall sensitivity (ATE): {overall_sens:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_gain(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    \"\"\"\n",
    "    Compute cumulative gain curve.\n",
    "    \n",
    "    Same as cumulative sensitivity but multiplied by (k/N).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array : cumulative gain at each point\n",
    "    \"\"\"\n",
    "    size = dataset.shape[0]\n",
    "    ordered_df = dataset.sort_values(prediction, ascending=False).reset_index(drop=True)\n",
    "    n_rows = list(range(min_periods, size, size // steps)) + [size]\n",
    "    \n",
    "    # Add normalization factor (rows/size)\n",
    "    return np.array([sensitivity(ordered_df.head(rows), y, t) * (rows/size) \n",
    "                     for rows in n_rows])\n",
    "\n",
    "# Test\n",
    "gain_test = cumulative_gain(prices_rnd_pred, 'sensitivity_m_pred', 'sales', 'price')\n",
    "print(f\"Cumulative gain curve computed: {len(gain_test)} points\")\n",
    "print(f\"Final value (should ≈ ATE): {gain_test[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Numeric Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cumulative sensitivity curves for all models\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = ['sensitivity_m_pred', 'pred_m_pred', 'rand_m_pred']\n",
    "names = ['CATE Model', 'Predictive Model', 'Random Model']\n",
    "colors = [COLORS['green'], COLORS['blue'], TUFTE_PALETTE['secondary']]\n",
    "\n",
    "# Panel 1: Cumulative Sensitivity Curve\n",
    "ax = axes[0]\n",
    "for model, name, color in zip(models, names, colors):\n",
    "    curve = cumulative_sensitivity_curve(prices_rnd_pred, model, 'sales', 'price', \n",
    "                                         min_periods=100, steps=100)\n",
    "    x = np.linspace(0, 1, len(curve))\n",
    "    ax.plot(x, curve, c=color, lw=2.5, label=name)\n",
    "\n",
    "ax.axhline(overall_sens, color='black', linestyle='--', lw=2, label=f'ATE = {overall_sens:.2f}')\n",
    "ax.set_xlabel('% of Top Units (by prediction)', fontsize=11)\n",
    "ax.set_ylabel('Cumulative Sensitivity', fontsize=11)\n",
    "ax.set_title('(a) Cumulative Sensitivity Curve', fontweight='bold')\n",
    "ax.legend(frameon=False, loc='upper right')\n",
    "\n",
    "# Panel 2: Cumulative Gain Curve\n",
    "ax = axes[1]\n",
    "for model, name, color in zip(models, names, colors):\n",
    "    curve = cumulative_gain(prices_rnd_pred, model, 'sales', 'price', \n",
    "                            min_periods=50, steps=100)\n",
    "    x = np.linspace(0, 1, len(curve))\n",
    "    ax.plot(x, curve, c=color, lw=2.5, label=name)\n",
    "\n",
    "# Random baseline (straight line)\n",
    "ax.plot([0, 1], [0, overall_sens], 'k--', lw=2, label='Random Baseline')\n",
    "\n",
    "ax.set_xlabel('% of Population', fontsize=11)\n",
    "ax.set_ylabel('Cumulative Gain', fontsize=11)\n",
    "ax.set_title('(b) Cumulative Gain Curve', fontweight='bold')\n",
    "ax.legend(frameon=False, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "(a) Cumulative Sensitivity Curve:\n",
    "    - CATE model (green): Starts negative, converges to ATE\n",
    "      -> Top units have the most negative sensitivity (high price effect)\n",
    "    - Predictive model (blue): Starts near ATE, stays flat\n",
    "      -> No ability to order by sensitivity\n",
    "    - Random (gray): Flat at ATE throughout\n",
    "      -> Expected for random ordering\n",
    "\n",
    "(b) Cumulative Gain Curve:\n",
    "    - CATE model: Below the random line, then crosses\n",
    "      -> Can identify high-sensitivity units (more negative = better for pricing)\n",
    "    - Predictive and Random: Follow the diagonal\n",
    "      -> No gain over random targeting\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute AUUC (Area Under Uplift Curve)\n",
    "def auuc(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    \"\"\"\n",
    "    Compute Area Under Uplift Curve.\n",
    "    \n",
    "    Higher = better at ordering units by treatment effect.\n",
    "    \"\"\"\n",
    "    curve = cumulative_gain(dataset, prediction, y, t, min_periods, steps)\n",
    "    x = np.linspace(0, 1, len(curve))\n",
    "    return np.trapz(curve, x)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AREA UNDER UPLIFT CURVE (AUUC)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nHigher value = better at ordering units by treatment effect\")\n",
    "print(f\"Random baseline: {0.5 * overall_sens:.4f}\")\n",
    "print()\n",
    "\n",
    "for model, name in zip(models, names):\n",
    "    auuc_val = auuc(prices_rnd_pred, model, 'sales', 'price')\n",
    "    improvement = (auuc_val - 0.5 * overall_sens) / (0.5 * abs(overall_sens)) * 100\n",
    "    print(f\"{name}: AUUC = {auuc_val:.4f} ({improvement:+.1f}% vs random)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: For price sensitivity, more negative is \"better\" (more sensitive)\n",
    "# Let's flip the sign for a cleaner interpretation\n",
    "print(\"\\nNote: For price sensitivity, we want to identify the MOST NEGATIVE effect.\")\n",
    "print(\"The CATE model correctly identifies units where price increases hurt sales most.\")\n",
    "print(\"\\nThis is useful for:\")\n",
    "print(\"  - Keeping prices LOW on high-sensitivity days\")\n",
    "print(\"  - Raising prices on low-sensitivity days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Confidence Intervals\n",
    "\n",
    "We should account for sampling variability in our estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_ci(df, y, t, z=1.96):\n",
    "    \"\"\"\n",
    "    Compute confidence interval for sensitivity estimate.\n",
    "    \n",
    "    Uses standard OLS formula for the slope CI.\n",
    "    \"\"\"\n",
    "    n = df.shape[0]\n",
    "    t_bar = df[t].mean()\n",
    "    beta1 = sensitivity(df, y, t)\n",
    "    beta0 = df[y].mean() - beta1 * t_bar\n",
    "    \n",
    "    # Residuals\n",
    "    e = df[y] - (beta0 + beta1 * df[t])\n",
    "    \n",
    "    # Standard error of beta1\n",
    "    se = np.sqrt(((1 / (n - 2)) * np.sum(e ** 2)) / np.sum((df[t] - t_bar) ** 2))\n",
    "    \n",
    "    return np.array([beta1 - z * se, beta1 + z * se])\n",
    "\n",
    "def cumulative_gain_ci(dataset, prediction, y, t, min_periods=30, steps=100):\n",
    "    \"\"\"\n",
    "    Compute cumulative gain curve with confidence intervals.\n",
    "    \"\"\"\n",
    "    size = dataset.shape[0]\n",
    "    ordered_df = dataset.sort_values(prediction, ascending=False).reset_index(drop=True)\n",
    "    n_rows = list(range(min_periods, size, size // steps)) + [size]\n",
    "    \n",
    "    return np.array([sensitivity_ci(ordered_df.head(rows), y, t) * (rows / size) \n",
    "                     for rows in n_rows])\n",
    "\n",
    "# Test\n",
    "ci = sensitivity_ci(prices_rnd_pred, 'sales', 'price')\n",
    "print(f\"Overall sensitivity: {overall_sens:.3f}\")\n",
    "print(f\"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with confidence intervals\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Compute curve with CI\n",
    "gain_ci = cumulative_gain_ci(prices_rnd_pred, 'sensitivity_m_pred', 'sales', 'price', \n",
    "                             min_periods=50, steps=200)\n",
    "x = np.linspace(0, 1, len(gain_ci))\n",
    "\n",
    "# Plot CI as shaded region\n",
    "ax.fill_between(x, gain_ci[:, 0], gain_ci[:, 1], alpha=0.3, color=COLORS['green'])\n",
    "ax.plot(x, (gain_ci[:, 0] + gain_ci[:, 1]) / 2, c=COLORS['green'], lw=2.5, label='CATE Model')\n",
    "\n",
    "# Random baseline\n",
    "ax.plot([0, 1], [0, overall_sens], 'k--', lw=2, label='Random Baseline')\n",
    "\n",
    "ax.set_xlabel('% of Population', fontsize=11)\n",
    "ax.set_ylabel('Cumulative Gain', fontsize=11)\n",
    "ax.set_title('Cumulative Gain Curve with 95% Confidence Interval', fontweight='bold')\n",
    "ax.legend(frameon=False, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCI gets NARROWER as we include more data (right side of curve).\")\n",
    "print(\"CI is WIDER at the start (small sample size, noisier estimates).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "```\n",
    "Key Takeaways ───────────────────────────────────────────\n",
    "\n",
    "1. Cumulative Sensitivity Curve\n",
    "   - Shows sensitivity of top k% units\n",
    "   - Good model: starts extreme, converges to ATE\n",
    "   - Random model: flat at ATE\n",
    "\n",
    "2. Cumulative Gain Curve\n",
    "   - Normalized: gain × proportion\n",
    "   - All curves: 0 to ATE\n",
    "   - Random baseline: diagonal line\n",
    "   - \"ROC curve for causal inference\"\n",
    "\n",
    "3. AUUC (Area Under Uplift Curve)\n",
    "   - Single metric for comparison\n",
    "   - Higher = better ordering ability\n",
    "   - Compare to random baseline\n",
    "   \n",
    "4. Confidence Intervals\n",
    "   - Use OLS formula for slope CI\n",
    "   - Multiply by (k/N) for gain CI\n",
    "   - Important for small samples\n",
    "─────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Meta E5, DS)**: *\"Explain the cumulative gain curve for CATE model evaluation.\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**The cumulative gain curve**:\n",
    "\n",
    "1. **Sort** units by predicted CATE (highest first)\n",
    "2. **For each k** from 1 to N:\n",
    "   - Take top k units\n",
    "   - Estimate their treatment effect: $\\hat{\\tau}_k$\n",
    "   - Compute gain: $G_k = \\hat{\\tau}_k \\times (k/N)$\n",
    "3. **Plot**: X = k/N, Y = $G_k$\n",
    "\n",
    "**Properties**:\n",
    "- All curves start at (0, 0)\n",
    "- All curves end at (1, ATE)\n",
    "- Random model: straight diagonal line\n",
    "\n",
    "**Interpretation**:\n",
    "- Area above random line = model's ability to find high-effect units\n",
    "- Curves that diverge more from diagonal = better models\n",
    "\n",
    "**Analogy**: Like ROC curve for classification, but for causal effect ordering.\n",
    "\n",
    "**Metric**: AUUC (Area Under Uplift Curve) summarizes the curve.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Amazon L6, Econ)**: *\"What is the relationship between cumulative gain curve and the Qini curve?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Qini curve** (original, for binary treatment):\n",
    "- Sort by predicted uplift\n",
    "- Plot: treated outcome - control outcome (normalized)\n",
    "- Popular in uplift modeling literature\n",
    "\n",
    "**Cumulative gain curve** (generalized):\n",
    "- Works for continuous treatment (via regression slope)\n",
    "- Same principle: order by prediction, estimate actual effect\n",
    "- More general framework\n",
    "\n",
    "**Relationship**:\n",
    "- Qini is the binary treatment version\n",
    "- Cumulative gain extends to continuous treatments\n",
    "- Both measure \"ordering quality\" for treatment targeting\n",
    "\n",
    "**Key insight**: The normalization (multiplying by k/N) ensures:\n",
    "- All curves start at 0 and end at ATE\n",
    "- Random baseline is a straight line\n",
    "- Easier visual comparison\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Google L5, Quant)**: *\"How do you compute confidence intervals for the cumulative gain curve?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**For each point k on the curve**:\n",
    "\n",
    "1. **Estimate sensitivity** in top k units:\n",
    "   $$\\hat{\\tau}_k = \\text{Cov}(T, Y) / \\text{Var}(T)$$\n",
    "\n",
    "2. **Standard error** (OLS formula):\n",
    "   $$SE(\\hat{\\tau}_k) = \\sqrt{\\frac{\\sum e_i^2 / (n-2)}{\\sum (T_i - \\bar{T})^2}}$$\n",
    "   where $e_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\tau}_k T_i)$\n",
    "\n",
    "3. **Confidence interval for sensitivity**:\n",
    "   $$\\hat{\\tau}_k \\pm z_{\\alpha/2} \\times SE(\\hat{\\tau}_k)$$\n",
    "\n",
    "4. **Cumulative gain CI** (multiply by k/N):\n",
    "   $$(\\hat{\\tau}_k \\pm z \\times SE) \\times \\frac{k}{N}$$\n",
    "\n",
    "**Properties**:\n",
    "- CI is **wide** at small k (few observations)\n",
    "- CI **narrows** as k increases\n",
    "- At k = N: CI for overall ATE\n",
    "\n",
    "**Alternative**: Bootstrap the entire curve for non-parametric CI.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 19: \"Evaluating Causal Models.\"\n",
    "\n",
    "[^2]: Gutierrez, P. and Gérardy, J. Y. (2017). Causal Inference and Uplift Modeling: A Review of the Literature. *JMLR Workshop and Conference Proceedings*, 67, 1-13.\n",
    "\n",
    "[^3]: Radcliffe, N. J. (2007). Using Control Groups to Target on Predicted Lift. *Direct Market Journal*, 3, 14-21.\n",
    "\n",
    "[^4]: Breiman, L. (2001). Statistical Modeling: The Two Cultures. *Statistical Science*, 16(3), 199-231.\n",
    "\n",
    "---\n",
    "\n",
    "**Chapter Complete**: You now have tools to build AND evaluate CATE models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
