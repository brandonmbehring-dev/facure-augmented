{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 21.1 S-Learner and T-Learner\n",
    "\n",
    "**Chapter**: 21 - Meta-Learners  \n",
    "**Section**: 1 - S-Learner and T-Learner  \n",
    "**Facure Source**: 21-Meta-Learners.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-16\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Facure's Intuition](#1-facures-intuition)\n",
    "   - 1.1 [From ATE to CATE](#11-from-ate-to-cate)\n",
    "   - 1.2 [Why Meta-Learners?](#12-why-meta-learners)\n",
    "2. [The S-Learner](#2-the-s-learner)\n",
    "   - 2.1 [Concept](#21-concept)\n",
    "   - 2.2 [Implementation](#22-implementation)\n",
    "   - 2.3 [The Regularization Bias Problem](#23-the-regularization-bias-problem)\n",
    "3. [The T-Learner](#3-the-t-learner)\n",
    "   - 3.1 [Concept](#31-concept)\n",
    "   - 3.2 [Implementation](#32-implementation)\n",
    "   - 3.3 [Sample Imbalance Problem](#33-sample-imbalance-problem)\n",
    "4. [Comparison](#4-comparison)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports via common module\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "# ML imports\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Facure's Intuition\n",
    "\n",
    "> **Interview Relevance**: Meta-learners are the foundation of modern CATE estimation. Understanding S and T learners is essential before moving to more sophisticated methods like X-learner and R-learner.\n",
    "\n",
    "### 1.1 From ATE to CATE\n",
    "\n",
    "So far we've focused on estimating the **Average Treatment Effect (ATE)**:\n",
    "\n",
    "$$\\tau = E[Y(1) - Y(0)]$$\n",
    "\n",
    "But often we care about **heterogeneity** — how the treatment effect varies across individuals:\n",
    "\n",
    "$$\\tau(x) = E[Y(1) - Y(0) | X = x]$$\n",
    "\n",
    "This is the **Conditional Average Treatment Effect (CATE)**.\n",
    "\n",
    "**Why does CATE matter?**\n",
    "\n",
    "1. **Resource allocation**: If you have limited budget for discounts, target those with highest CATE\n",
    "2. **Policy design**: Understand who benefits vs who might be harmed\n",
    "3. **Scientific understanding**: Mechanism discovery through effect heterogeneity\n",
    "\n",
    "### 1.2 Why Meta-Learners?\n",
    "\n",
    "Meta-learners let us use **off-the-shelf ML models** (XGBoost, Random Forest, Neural Nets) for CATE estimation. The \"meta\" comes from the fact that we're using prediction models as building blocks for causal estimation.\n",
    "\n",
    "```\n",
    "Meta-Learner Landscape ───────────────────────────────────\n",
    "  \n",
    "  S-Learner: Single model, treatment as feature\n",
    "  T-Learner: Two models, one per treatment level\n",
    "  X-Learner: Two stages + propensity weighting\n",
    "  R-Learner: Residual-on-residual (orthogonal)\n",
    "  \n",
    "  Complexity: S < T < X < R\n",
    "  Robustness: Generally reverse order\n",
    "─────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the investment email data\n",
    "# Training: biased (observational)\n",
    "# Testing: randomized (for validation)\n",
    "train = load_facure_data('invest_email_biased.csv')\n",
    "test = load_facure_data('invest_email_rnd.csv')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INVESTMENT EMAIL CAMPAIGN DATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining set (biased): {len(train):,} customers\")\n",
    "print(f\"Test set (randomized): {len(test):,} customers\")\n",
    "print(f\"\\nColumns: {list(train.columns)}\")\n",
    "print(f\"\\nOutcome: 'converted' (0/1)\")\n",
    "print(f\"Treatment: 'em1' (email type 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "y = 'converted'  # Outcome\n",
    "T = 'em1'        # Treatment (binary)\n",
    "X = ['age', 'income', 'insurance', 'invested']  # Features\n",
    "\n",
    "print(\"Training data sample:\")\n",
    "print(train.head())\n",
    "\n",
    "print(f\"\\nTreatment rate in train: {train[T].mean():.1%}\")\n",
    "print(f\"Treatment rate in test:  {test[T].mean():.1%}\")\n",
    "print(f\"Conversion rate (train): {train[y].mean():.1%}\")\n",
    "print(f\"Conversion rate (test):  {test[y].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The S-Learner\n",
    "\n",
    "### 2.1 Concept\n",
    "\n",
    "The **S-Learner** (Single learner) is the simplest meta-learner:\n",
    "\n",
    "1. Train a **single** ML model to predict $Y$ using both $X$ **and** $T$:\n",
    "   $$\\hat{\\mu}(x, t) = \\hat{E}[Y | X=x, T=t]$$\n",
    "\n",
    "2. Estimate CATE by differencing predictions:\n",
    "   $$\\hat{\\tau}(x) = \\hat{\\mu}(x, 1) - \\hat{\\mu}(x, 0)$$\n",
    "\n",
    "```\n",
    "S-Learner Architecture ───────────────────────────────────\n",
    "  \n",
    "            [X, T]\n",
    "               │\n",
    "               ▼\n",
    "      ┌────────────────┐\n",
    "      │  Single Model  │\n",
    "      │   μ̂(X, T)     │\n",
    "      └────────────────┘\n",
    "               │\n",
    "       ┌───────┴───────┐\n",
    "       ▼               ▼\n",
    "   μ̂(X, T=1)      μ̂(X, T=0)\n",
    "       │               │\n",
    "       └───────┬───────┘\n",
    "               │\n",
    "               ▼\n",
    "        τ̂(X) = difference\n",
    "─────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Simple to implement\n",
    "- Works with continuous treatments\n",
    "- Uses all data for a single model\n",
    "\n",
    "**Disadvantages**:\n",
    "- Regularization can shrink treatment effect toward zero\n",
    "- May drop treatment entirely if it's a weak predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### 2.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S-Learner implementation\n",
    "np.random.seed(123)\n",
    "\n",
    "s_learner = LGBMRegressor(\n",
    "    max_depth=3, \n",
    "    min_child_samples=30,\n",
    "    n_estimators=100,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train on X AND T\n",
    "s_learner.fit(train[X + [T]], train[y])\n",
    "\n",
    "print(\"S-Learner trained successfully\")\n",
    "print(f\"Feature importances: {dict(zip(X + [T], s_learner.feature_importances_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate CATE: predict under both treatment regimes\n",
    "def s_learner_cate(model, df, X, T):\n",
    "    \"\"\"\n",
    "    Compute CATE using S-learner.\n",
    "    \n",
    "    CATE(x) = E[Y|X=x, T=1] - E[Y|X=x, T=0]\n",
    "    \"\"\"\n",
    "    # Predict under treatment\n",
    "    df_t1 = df[X].copy()\n",
    "    df_t1[T] = 1\n",
    "    mu1 = model.predict(df_t1)\n",
    "    \n",
    "    # Predict under control\n",
    "    df_t0 = df[X].copy()\n",
    "    df_t0[T] = 0\n",
    "    mu0 = model.predict(df_t0)\n",
    "    \n",
    "    return mu1 - mu0\n",
    "\n",
    "# Compute CATE\n",
    "cate_s_train = s_learner_cate(s_learner, train, X, T)\n",
    "cate_s_test = s_learner_cate(s_learner, test, X, T)\n",
    "\n",
    "print(f\"S-Learner CATE statistics (test set):\")\n",
    "print(f\"  Mean:   {cate_s_test.mean():.4f}\")\n",
    "print(f\"  Std:    {cate_s_test.std():.4f}\")\n",
    "print(f\"  Min:    {cate_s_test.min():.4f}\")\n",
    "print(f\"  Max:    {cate_s_test.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2.3 The Regularization Bias Problem\n",
    "\n",
    "The S-learner has a fundamental problem: **regularization shrinks the treatment effect toward zero**.\n",
    "\n",
    "This happens because:\n",
    "1. ML models use regularization to prevent overfitting\n",
    "2. If treatment is a weak predictor, regularization may drop it entirely\n",
    "3. Even if kept, the coefficient is shrunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate S-learner regularization bias with simulation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data with known CATE\n",
    "n = 2000\n",
    "X_sim = np.random.uniform(0, 1, (n, 4))\n",
    "T_sim = np.random.binomial(1, 0.5, n)\n",
    "\n",
    "# True CATE depends on X0: tau(x) = 0.5 + 2*X0\n",
    "true_cate = 0.5 + 2 * X_sim[:, 0]\n",
    "Y_sim = X_sim[:, 0] + 0.5 * X_sim[:, 1] + T_sim * true_cate + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# Fit S-learner with varying regularization\n",
    "max_depths = [2, 3, 5, 10, None]\n",
    "s_learner_biases = []\n",
    "\n",
    "for md in max_depths:\n",
    "    model = LGBMRegressor(max_depth=md, n_estimators=100, verbose=-1, random_state=42)\n",
    "    df_sim = pd.DataFrame(X_sim, columns=['X0', 'X1', 'X2', 'X3'])\n",
    "    df_sim['T'] = T_sim\n",
    "    model.fit(df_sim, Y_sim)\n",
    "    \n",
    "    # Estimate CATE\n",
    "    cate_hat = s_learner_cate(model, df_sim, ['X0', 'X1', 'X2', 'X3'], 'T')\n",
    "    bias = (cate_hat - true_cate).mean()\n",
    "    s_learner_biases.append(bias)\n",
    "    print(f\"max_depth={str(md):4} | Mean CATE: {cate_hat.mean():.3f} | True: {true_cate.mean():.3f} | Bias: {bias:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the regularization bias\n",
    "fig, axes = create_tufte_figure(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Panel 1: CATE vs True CATE\n",
    "ax = axes[0]\n",
    "# Fit with medium regularization\n",
    "model_reg = LGBMRegressor(max_depth=3, n_estimators=100, verbose=-1, random_state=42)\n",
    "df_sim = pd.DataFrame(X_sim, columns=['X0', 'X1', 'X2', 'X3'])\n",
    "df_sim['T'] = T_sim\n",
    "model_reg.fit(df_sim, Y_sim)\n",
    "cate_reg = s_learner_cate(model_reg, df_sim, ['X0', 'X1', 'X2', 'X3'], 'T')\n",
    "\n",
    "ax.scatter(true_cate, cate_reg, alpha=0.3, s=15, c=TUFTE_PALETTE['primary'])\n",
    "ax.plot([0, 3], [0, 3], 'k--', lw=1.5, label='Perfect calibration')\n",
    "ax.set_xlabel('True CATE')\n",
    "ax.set_ylabel('Estimated CATE (S-learner)')\n",
    "ax.set_title('(a) S-Learner Shrinks CATE Toward Zero', fontweight='bold')\n",
    "ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "# Panel 2: Bias by regularization strength\n",
    "ax = axes[1]\n",
    "depth_labels = ['2', '3', '5', '10', 'None']\n",
    "colors = [COLORS['red'] if b < -0.1 else COLORS['green'] for b in s_learner_biases]\n",
    "bars = ax.bar(depth_labels, s_learner_biases, color=colors, width=0.6, edgecolor='white')\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel('max_depth (regularization)')\n",
    "ax.set_ylabel('Bias (estimated - true)')\n",
    "ax.set_title('(b) More Regularization = More Bias', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: S-learner underestimates CATE when regularization is strong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The T-Learner\n",
    "\n",
    "### 3.1 Concept\n",
    "\n",
    "The **T-Learner** (Two learners) addresses the S-learner's regularization problem by **forcing the split on treatment**:\n",
    "\n",
    "1. Train model $\\hat{\\mu}_0(x)$ on **control** group: $E[Y | X=x, T=0]$\n",
    "2. Train model $\\hat{\\mu}_1(x)$ on **treated** group: $E[Y | X=x, T=1]$\n",
    "3. CATE is the difference:\n",
    "   $$\\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)$$\n",
    "\n",
    "```\n",
    "T-Learner Architecture ───────────────────────────────────\n",
    "  \n",
    "         Data split by T\n",
    "       ┌────────┴────────┐\n",
    "       │                 │\n",
    "   [X, T=0]          [X, T=1]\n",
    "       │                 │\n",
    "       ▼                 ▼\n",
    "   ┌────────┐       ┌────────┐\n",
    "   │ Model  │       │ Model  │\n",
    "   │  μ̂₀   │       │  μ̂₁   │\n",
    "   └────────┘       └────────┘\n",
    "       │                 │\n",
    "       └────────┬────────┘\n",
    "                │\n",
    "                ▼\n",
    "         τ̂(X) = μ̂₁(X) - μ̂₀(X)\n",
    "─────────────────────────────────────────────────────────\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Guarantees treatment is considered (avoids dropping it)\n",
    "- Simple conceptually\n",
    "\n",
    "**Disadvantages**:\n",
    "- Each model uses only subset of data\n",
    "- Imbalanced treatment groups cause problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 3.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Learner implementation\n",
    "np.random.seed(123)\n",
    "\n",
    "# Model for control (T=0)\n",
    "m0 = LGBMRegressor(max_depth=3, min_child_samples=30, n_estimators=100, verbose=-1)\n",
    "# Model for treated (T=1)\n",
    "m1 = LGBMRegressor(max_depth=3, min_child_samples=30, n_estimators=100, verbose=-1)\n",
    "\n",
    "# Fit on separate subsets\n",
    "m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y])\n",
    "m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y])\n",
    "\n",
    "print(f\"Control model trained on {(train[T]==0).sum():,} samples\")\n",
    "print(f\"Treated model trained on {(train[T]==1).sum():,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate CATE with T-learner\n",
    "def t_learner_cate(m0, m1, df, X):\n",
    "    \"\"\"\n",
    "    Compute CATE using T-learner.\n",
    "    \n",
    "    CATE(x) = E[Y|X=x, T=1] - E[Y|X=x, T=0]\n",
    "            = mu1(x) - mu0(x)\n",
    "    \"\"\"\n",
    "    return m1.predict(df[X]) - m0.predict(df[X])\n",
    "\n",
    "# Compute CATE\n",
    "cate_t_train = t_learner_cate(m0, m1, train, X)\n",
    "cate_t_test = t_learner_cate(m0, m1, test, X)\n",
    "\n",
    "print(f\"T-Learner CATE statistics (test set):\")\n",
    "print(f\"  Mean:   {cate_t_test.mean():.4f}\")\n",
    "print(f\"  Std:    {cate_t_test.std():.4f}\")\n",
    "print(f\"  Min:    {cate_t_test.min():.4f}\")\n",
    "print(f\"  Max:    {cate_t_test.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### 3.3 Sample Imbalance Problem\n",
    "\n",
    "The T-learner suffers when treatment groups have **different sample sizes**:\n",
    "\n",
    "- Small treated group → heavily regularized $\\hat{\\mu}_1$ (simple model)\n",
    "- Large control group → less regularized $\\hat{\\mu}_0$ (complex model)\n",
    "- Differencing these creates **spurious heterogeneity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate T-learner sample imbalance problem\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create imbalanced data: 10% treated, 90% control\n",
    "n = 2000\n",
    "X_imb = np.random.uniform(0, 1, (n, 1))\n",
    "T_imb = np.random.binomial(1, 0.1, n)  # Only 10% treated!\n",
    "\n",
    "# CONSTANT true CATE = 1.0 (no heterogeneity!)\n",
    "true_cate_const = 1.0\n",
    "\n",
    "# Outcome has NONLINEAR relationship with X\n",
    "# Y = sin(2*pi*X) + T*1.0 + noise\n",
    "Y_imb = np.sin(2 * np.pi * X_imb[:, 0]) + T_imb * true_cate_const + np.random.normal(0, 0.3, n)\n",
    "\n",
    "print(f\"Treated: {T_imb.sum()} ({T_imb.mean():.1%})\")\n",
    "print(f\"Control: {n - T_imb.sum()} ({1 - T_imb.mean():.1%})\")\n",
    "print(f\"True CATE: {true_cate_const} (constant everywhere!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit T-learner on imbalanced data\n",
    "df_imb = pd.DataFrame({'X': X_imb[:, 0], 'T': T_imb, 'Y': Y_imb})\n",
    "\n",
    "m0_imb = LGBMRegressor(max_depth=5, n_estimators=100, verbose=-1, random_state=42)\n",
    "m1_imb = LGBMRegressor(max_depth=5, n_estimators=100, verbose=-1, random_state=42)\n",
    "\n",
    "m0_imb.fit(df_imb.query('T==0')[['X']], df_imb.query('T==0')['Y'])\n",
    "m1_imb.fit(df_imb.query('T==1')[['X']], df_imb.query('T==1')['Y'])\n",
    "\n",
    "# Estimate CATE\n",
    "cate_t_imb = m1_imb.predict(df_imb[['X']]) - m0_imb.predict(df_imb[['X']])\n",
    "\n",
    "print(f\"T-Learner CATE estimate: mean={cate_t_imb.mean():.3f}, std={cate_t_imb.std():.3f}\")\n",
    "print(f\"True CATE:              mean={true_cate_const:.3f}, std=0.000\")\n",
    "print(f\"\\nProblem: T-learner finds heterogeneity where there is none!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the T-learner imbalance problem\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "X_grid = np.linspace(0, 1, 200).reshape(-1, 1)\n",
    "\n",
    "# Panel 1: The two models\n",
    "ax = axes[0]\n",
    "mu0_pred = m0_imb.predict(X_grid)\n",
    "mu1_pred = m1_imb.predict(X_grid)\n",
    "\n",
    "ax.plot(X_grid, mu0_pred, c=COLORS['blue'], lw=2.5, label=r'$\\hat{\\mu}_0(x)$ (control, n=1800)')\n",
    "ax.plot(X_grid, mu1_pred, c=COLORS['red'], lw=2.5, label=r'$\\hat{\\mu}_1(x)$ (treated, n=200)')\n",
    "ax.scatter(X_imb[T_imb==0, 0], Y_imb[T_imb==0], alpha=0.1, s=10, c=COLORS['blue'])\n",
    "ax.scatter(X_imb[T_imb==1, 0], Y_imb[T_imb==1], alpha=0.3, s=20, c=COLORS['red'])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('(a) Two Models', fontweight='bold')\n",
    "ax.legend(loc='upper right', frameon=False, fontsize=9)\n",
    "\n",
    "# Panel 2: CATE estimates\n",
    "ax = axes[1]\n",
    "cate_grid = m1_imb.predict(X_grid) - m0_imb.predict(X_grid)\n",
    "ax.plot(X_grid, cate_grid, c=COLORS['green'], lw=2.5, label='T-Learner CATE')\n",
    "ax.axhline(true_cate_const, c='black', ls='--', lw=2, label=f'True CATE = {true_cate_const}')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('CATE')\n",
    "ax.set_title('(b) Spurious Heterogeneity!', fontweight='bold', color=COLORS['red'])\n",
    "ax.legend(loc='upper right', frameon=False)\n",
    "ax.set_ylim(-0.5, 2.5)\n",
    "\n",
    "# Panel 3: The explanation\n",
    "ax = axes[2]\n",
    "ax.text(0.5, 0.8, 'The Problem:', fontsize=14, fontweight='bold', \n",
    "        ha='center', transform=ax.transAxes)\n",
    "ax.text(0.5, 0.6, r'$\\hat{\\mu}_0$ captures nonlinearity (many samples)', fontsize=11, \n",
    "        ha='center', transform=ax.transAxes)\n",
    "ax.text(0.5, 0.45, r'$\\hat{\\mu}_1$ is linear (few samples, regularized)', fontsize=11, \n",
    "        ha='center', transform=ax.transAxes)\n",
    "ax.text(0.5, 0.25, r'$\\hat{\\tau} = \\hat{\\mu}_1 - \\hat{\\mu}_0$', fontsize=12, \n",
    "        ha='center', transform=ax.transAxes)\n",
    "ax.text(0.5, 0.1, 'inherits nonlinearity from control model!', fontsize=11, \n",
    "        ha='center', transform=ax.transAxes, style='italic')\n",
    "ax.axis('off')\n",
    "ax.set_title('(c) Explanation', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Comparison\n",
    "\n",
    "Let's compare S-learner and T-learner on our original investment email data using **cumulative gain curves**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_gain(df, cate_col, y_col, t_col, steps=100):\n",
    "    \"\"\"\n",
    "    Compute cumulative gain curve for CATE model evaluation.\n",
    "    \n",
    "    Orders by predicted CATE and computes cumulative effect.\n",
    "    Higher curve = better targeting.\n",
    "    \"\"\"\n",
    "    # Sort by predicted CATE (highest first)\n",
    "    df_sorted = df.sort_values(cate_col, ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    n = len(df_sorted)\n",
    "    gains = []\n",
    "    \n",
    "    for pct in np.linspace(0, 100, steps):\n",
    "        if pct == 0:\n",
    "            gains.append(0)\n",
    "            continue\n",
    "        \n",
    "        n_top = int(n * pct / 100)\n",
    "        if n_top == 0:\n",
    "            gains.append(0)\n",
    "            continue\n",
    "        \n",
    "        top = df_sorted.head(n_top)\n",
    "        \n",
    "        # Treatment effect in top group\n",
    "        treated = top[top[t_col] == 1][y_col]\n",
    "        control = top[top[t_col] == 0][y_col]\n",
    "        \n",
    "        if len(treated) > 0 and len(control) > 0:\n",
    "            effect = treated.mean() - control.mean()\n",
    "        else:\n",
    "            effect = 0\n",
    "        \n",
    "        # Cumulative gain = effect * fraction of population\n",
    "        gains.append(effect * (pct / 100))\n",
    "    \n",
    "    return np.array(gains)\n",
    "\n",
    "# ATE baseline\n",
    "ate_test = test[test[T]==1][y].mean() - test[test[T]==0][y].mean()\n",
    "print(f\"ATE in test set: {ate_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cumulative gains\n",
    "test_s = test.assign(cate=cate_s_test)\n",
    "test_t = test.assign(cate=cate_t_test)\n",
    "\n",
    "gain_s = cumulative_gain(test_s, 'cate', y, T)\n",
    "gain_t = cumulative_gain(test_t, 'cate', y, T)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 6))\n",
    "\n",
    "pct = np.linspace(0, 100, 100)\n",
    "ax.plot(pct, gain_s, c=COLORS['blue'], lw=2.5, label='S-Learner')\n",
    "ax.plot(pct, gain_t, c=COLORS['green'], lw=2.5, label='T-Learner')\n",
    "ax.plot([0, 100], [0, ate_test], c='black', ls='--', lw=2, label=f'Random (ATE={ate_test:.3f})')\n",
    "\n",
    "ax.set_xlabel('% of Population Treated (ordered by CATE)', fontsize=11)\n",
    "ax.set_ylabel('Cumulative Gain', fontsize=11)\n",
    "ax.set_title('Cumulative Gain Curves: S-Learner vs T-Learner', fontweight='bold')\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "ax.fill_between(pct, gain_s, gain_t, alpha=0.2, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nArea under curve (higher = better targeting):\")\n",
    "print(f\"  S-Learner: {np.trapz(gain_s, pct):.4f}\")\n",
    "print(f\"  T-Learner: {np.trapz(gain_t, pct):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "```\n",
    "S-Learner vs T-Learner Summary ───────────────────────────\n",
    "\n",
    "| Aspect              | S-Learner                | T-Learner              |\n",
    "|---------------------|--------------------------|------------------------|\n",
    "| Models              | 1 (treatment as feature) | 2 (one per group)      |\n",
    "| Continuous T        | Yes                      | No (discrete only)     |\n",
    "| Regularization bias | Can drop T entirely      | Forces T consideration |\n",
    "| Sample efficiency   | Uses all data            | Splits data by T       |\n",
    "| Imbalanced groups   | OK                       | Problematic            |\n",
    "| Best when           | T is strong predictor    | Balanced groups        |\n",
    "\n",
    "Neither is universally best → X-Learner addresses both problems!\n",
    "─────────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Meta E5, DS)**: *\"Explain the difference between S-learner and T-learner for CATE estimation.\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**S-Learner**:\n",
    "\n",
    "- Trains a **single** model with treatment T as a feature\n",
    "- $\\hat{\\mu}(x, t) = \\hat{E}[Y | X, T]$\n",
    "- CATE: $\\hat{\\tau}(x) = \\hat{\\mu}(x, 1) - \\hat{\\mu}(x, 0)$\n",
    "\n",
    "**Pros**: Simple, uses all data, handles continuous T  \n",
    "**Cons**: Regularization can shrink/drop treatment effect\n",
    "\n",
    "**T-Learner**:\n",
    "\n",
    "- Trains **two** models, one for each treatment level\n",
    "- $\\hat{\\mu}_0(x) = \\hat{E}[Y | X, T=0]$, $\\hat{\\mu}_1(x) = \\hat{E}[Y | X, T=1]$\n",
    "- CATE: $\\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)$\n",
    "\n",
    "**Pros**: Guarantees treatment is considered  \n",
    "**Cons**: Each model uses subset of data; imbalanced groups cause spurious heterogeneity\n",
    "\n",
    "**When to use each**:\n",
    "- S-learner: Treatment is strong predictor, or continuous\n",
    "- T-learner: Balanced groups, weak treatment effect in S-learner\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Amazon L6, Econ)**: *\"Why does the S-learner tend to bias the treatment effect toward zero?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Root cause: Regularization**\n",
    "\n",
    "1. ML models use regularization (L1, L2, max_depth, etc.) to prevent overfitting\n",
    "\n",
    "2. Regularization shrinks coefficients toward zero\n",
    "\n",
    "3. If treatment T is a **weak predictor** of Y compared to X:\n",
    "   - The model prioritizes features with higher predictive power\n",
    "   - Treatment coefficient gets shrunk more\n",
    "   - In extreme cases (strong regularization), T is dropped entirely\n",
    "\n",
    "4. **Mathematical intuition**: In LASSO,\n",
    "   $$\\hat{\\beta} = \\arg\\min \\sum (Y_i - X_i\\beta)^2 + \\lambda\\|\\beta\\|_1$$\n",
    "   \n",
    "   The penalty term pushes all coefficients toward zero. Features that don't reduce MSE enough get zeroed out.\n",
    "\n",
    "5. **For CATE**: If $\\beta_T$ is shrunk, then\n",
    "   $$\\hat{\\tau}(x) = \\hat{\\mu}(x, 1) - \\hat{\\mu}(x, 0) \\approx 0$$\n",
    "\n",
    "**Solution**: T-learner forces treatment consideration; X-learner and R-learner are more robust.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q3 (Google L5, Quant)**: *\"When would you choose T-learner over S-learner, and what's the main failure mode of T-learner?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Choose T-learner when**:\n",
    "\n",
    "1. S-learner drops or severely shrinks the treatment variable\n",
    "2. Treatment groups are roughly balanced (similar sample sizes)\n",
    "3. You want to explicitly model each treatment regime\n",
    "\n",
    "**T-learner failure mode: Sample imbalance**\n",
    "\n",
    "When treatment groups are imbalanced (e.g., 10% treated, 90% control):\n",
    "\n",
    "1. $\\hat{\\mu}_0$ is trained on many samples → can be complex (captures nonlinearity)\n",
    "2. $\\hat{\\mu}_1$ is trained on few samples → must be simple (regularized to avoid overfitting)\n",
    "3. $\\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)$:\n",
    "   - If true CATE is constant, the difference of a simple and complex model creates **spurious heterogeneity**\n",
    "   - The nonlinearity in $\\hat{\\mu}_0$ shows up in $\\hat{\\tau}$ even though it shouldn't\n",
    "\n",
    "**Example**: True outcome is $Y = \\sin(x) + T \\cdot 1$. CATE is constant = 1.\n",
    "- $\\hat{\\mu}_0$ captures $\\sin(x)$\n",
    "- $\\hat{\\mu}_1 \\approx ax + b$ (linear, regularized)\n",
    "- $\\hat{\\tau}(x) = ax + b - \\sin(x)$ looks heterogeneous!\n",
    "\n",
    "**Solution**: X-learner uses propensity scores to down-weight unreliable estimates.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 21: \"Meta-Learners.\"\n",
    "\n",
    "[^2]: Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. *PNAS*, 116(10), 4156-4165.\n",
    "\n",
    "[^3]: Chernozhukov, V. et al. (2018). Double/Debiased Machine Learning for Treatment and Structural Parameters. *The Econometrics Journal*, 21(1), C1-C68.\n",
    "\n",
    "[^4]: Athey, S. and Imbens, G. W. (2016). Recursive partitioning for heterogeneous causal effects. *PNAS*, 113(27), 7353-7360.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [02. X-Learner](./02_x_learner.ipynb) — Addressing sample imbalance with two-stage estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
