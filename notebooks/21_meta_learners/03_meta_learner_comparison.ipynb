{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 21.3 Meta-Learner Comparison\n",
    "\n",
    "**Chapter**: 21 - Meta-Learners  \n",
    "**Section**: 3 - Comparing S, T, and X Learners  \n",
    "**Facure Source**: 21-Meta-Learners.ipynb  \n",
    "**Version**: 1.0.0  \n",
    "**Last Validated**: 2026-01-16\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Evaluation Framework](#1-evaluation-framework)\n",
    "2. [Cumulative Gain Curves](#2-cumulative-gain-curves)\n",
    "3. [Head-to-Head Comparison](#3-head-to-head-comparison)\n",
    "4. [Learner Selection Guidelines](#4-learner-selection-guidelines)\n",
    "5. [Interview Appendix](#5-interview-appendix)\n",
    "6. [References](#6-references)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from facure_augment.common import (\n",
    "    np, pd, plt, sm, stats,\n",
    "    load_facure_data,\n",
    "    set_notebook_style,\n",
    "    create_tufte_figure,\n",
    "    apply_tufte_style,\n",
    "    TUFTE_PALETTE,\n",
    "    COLORS,\n",
    ")\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "set_notebook_style()\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = load_facure_data('invest_email_biased.csv')\n",
    "test = load_facure_data('invest_email_rnd.csv')\n",
    "\n",
    "y = 'converted'\n",
    "T = 'em1'\n",
    "X = ['age', 'income', 'insurance', 'invested']\n",
    "\n",
    "print(f\"Training: {len(train):,} (biased/observational)\")\n",
    "print(f\"Testing:  {len(test):,} (randomized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Evaluation Framework\n",
    "\n",
    "> **The fundamental problem**: We can't observe individual treatment effects $\\tau_i = Y_i(1) - Y_i(0)$.\n",
    "\n",
    "So how do we evaluate CATE estimators?\n",
    "\n",
    "### Evaluation Approaches\n",
    "\n",
    "| Method | Requirements | What It Measures |\n",
    "|--------|-------------|------------------|\n",
    "| Cumulative Gain | RCT test data | Targeting quality |\n",
    "| AUUC | RCT test data | Area under uplift curve |\n",
    "| Qini Coefficient | RCT test data | Similar to AUUC |\n",
    "| Simulation | Known DGP | Direct RMSE on $\\tau(x)$ |\n",
    "\n",
    "**Key insight**: With RCT data, we can evaluate *ranking* quality even without observing individual $\\tau_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_gain(df, cate_col, y_col, t_col, steps=100):\n",
    "    \"\"\"\n",
    "    Compute cumulative gain curve.\n",
    "    \n",
    "    Orders units by predicted CATE (highest first), then computes\n",
    "    cumulative treatment effect as we include more units.\n",
    "    \n",
    "    Higher curve = better targeting.\n",
    "    \"\"\"\n",
    "    df_sorted = df.sort_values(cate_col, ascending=False).reset_index(drop=True)\n",
    "    n = len(df_sorted)\n",
    "    gains = [0]\n",
    "    \n",
    "    for pct in np.linspace(1, 100, steps-1):\n",
    "        n_top = max(1, int(n * pct / 100))\n",
    "        top = df_sorted.head(n_top)\n",
    "        \n",
    "        treated = top[top[t_col] == 1][y_col]\n",
    "        control = top[top[t_col] == 0][y_col]\n",
    "        \n",
    "        if len(treated) > 0 and len(control) > 0:\n",
    "            effect = treated.mean() - control.mean()\n",
    "        else:\n",
    "            effect = 0\n",
    "        \n",
    "        gains.append(effect * (pct / 100))\n",
    "    \n",
    "    return np.array(gains)\n",
    "\n",
    "\n",
    "def auuc(gains):\n",
    "    \"\"\"Area Under Uplift Curve.\"\"\"\n",
    "    return np.trapz(gains, np.linspace(0, 100, len(gains)))\n",
    "\n",
    "\n",
    "# ATE baseline\n",
    "ate_test = test[test[T]==1][y].mean() - test[test[T]==0][y].mean()\n",
    "print(f\"ATE in test set: {ate_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Cumulative Gain Curves\n",
    "\n",
    "### Implementing All Three Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S-Learner\n",
    "np.random.seed(123)\n",
    "s_model = LGBMRegressor(max_depth=3, min_child_samples=30, n_estimators=100, verbose=-1)\n",
    "s_model.fit(train[X + [T]], train[y])\n",
    "\n",
    "def s_learner_predict(model, df, X, T):\n",
    "    df_t1 = df[X].copy()\n",
    "    df_t1[T] = 1\n",
    "    df_t0 = df[X].copy()\n",
    "    df_t0[T] = 0\n",
    "    return model.predict(df_t1) - model.predict(df_t0)\n",
    "\n",
    "cate_s = s_learner_predict(s_model, test, X, T)\n",
    "print(f\"S-Learner: mean CATE = {cate_s.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Learner\n",
    "np.random.seed(123)\n",
    "m0 = LGBMRegressor(max_depth=3, min_child_samples=30, n_estimators=100, verbose=-1)\n",
    "m1 = LGBMRegressor(max_depth=3, min_child_samples=30, n_estimators=100, verbose=-1)\n",
    "\n",
    "m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y])\n",
    "m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y])\n",
    "\n",
    "cate_t = m1.predict(test[X]) - m0.predict(test[X])\n",
    "print(f\"T-Learner: mean CATE = {cate_t.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-Learner\n",
    "np.random.seed(123)\n",
    "\n",
    "# Stage 1 (reuse T-learner models)\n",
    "g = LogisticRegression(max_iter=1000)\n",
    "g.fit(train[X], train[T])\n",
    "\n",
    "# Stage 2\n",
    "d_train = np.where(\n",
    "    train[T] == 0,\n",
    "    m1.predict(train[X]) - train[y],\n",
    "    train[y] - m0.predict(train[X])\n",
    ")\n",
    "\n",
    "mx0 = LGBMRegressor(max_depth=3, min_child_samples=30, n_estimators=100, verbose=-1)\n",
    "mx1 = LGBMRegressor(max_depth=3, min_child_samples=30, n_estimators=100, verbose=-1)\n",
    "mx0.fit(train.query(f\"{T}==0\")[X], d_train[train[T]==0])\n",
    "mx1.fit(train.query(f\"{T}==1\")[X], d_train[train[T]==1])\n",
    "\n",
    "ps = g.predict_proba(test[X])[:, 1]\n",
    "cate_x = ps * mx0.predict(test[X]) + (1 - ps) * mx1.predict(test[X])\n",
    "print(f\"X-Learner: mean CATE = {cate_x.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cumulative gains\n",
    "test_s = test.assign(cate=cate_s)\n",
    "test_t = test.assign(cate=cate_t)\n",
    "test_x = test.assign(cate=cate_x)\n",
    "\n",
    "gain_s = cumulative_gain(test_s, 'cate', y, T)\n",
    "gain_t = cumulative_gain(test_t, 'cate', y, T)\n",
    "gain_x = cumulative_gain(test_x, 'cate', y, T)\n",
    "\n",
    "print(\"Area Under Uplift Curve (AUUC):\")\n",
    "print(f\"  S-Learner: {auuc(gain_s):.4f}\")\n",
    "print(f\"  T-Learner: {auuc(gain_t):.4f}\")\n",
    "print(f\"  X-Learner: {auuc(gain_x):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, ax = create_tufte_figure(1, 1, figsize=(10, 6))\n",
    "\n",
    "pct = np.linspace(0, 100, 100)\n",
    "ax.plot(pct, gain_s, c=COLORS['blue'], lw=2.5, label=f'S-Learner (AUUC={auuc(gain_s):.3f})')\n",
    "ax.plot(pct, gain_t, c=COLORS['green'], lw=2.5, label=f'T-Learner (AUUC={auuc(gain_t):.3f})')\n",
    "ax.plot(pct, gain_x, c=COLORS['purple'], lw=2.5, label=f'X-Learner (AUUC={auuc(gain_x):.3f})')\n",
    "ax.plot([0, 100], [0, ate_test], c='black', ls='--', lw=2, label=f'Random (ATE={ate_test:.3f})')\n",
    "\n",
    "ax.set_xlabel('% of Population Treated (ordered by CATE)', fontsize=11)\n",
    "ax.set_ylabel('Cumulative Gain', fontsize=11)\n",
    "ax.set_title('Cumulative Gain Curves: Meta-Learner Comparison', fontweight='bold')\n",
    "ax.legend(loc='lower right', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Head-to-Head Comparison\n",
    "\n",
    "Let's look at how the learners rank individuals differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CATE distributions\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, cate, name, color in [\n",
    "    (axes[0], cate_s, 'S-Learner', COLORS['blue']),\n",
    "    (axes[1], cate_t, 'T-Learner', COLORS['green']),\n",
    "    (axes[2], cate_x, 'X-Learner', COLORS['purple']),\n",
    "]:\n",
    "    ax.hist(cate, bins=30, color=color, alpha=0.7, edgecolor='white')\n",
    "    ax.axvline(cate.mean(), c='black', ls='--', lw=2)\n",
    "    ax.set_xlabel('Predicted CATE')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{name}\\nmean={cate.mean():.3f}, std={cate.std():.3f}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank correlation between learners\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "corr_st, _ = spearmanr(cate_s, cate_t)\n",
    "corr_sx, _ = spearmanr(cate_s, cate_x)\n",
    "corr_tx, _ = spearmanr(cate_t, cate_x)\n",
    "\n",
    "print(\"Spearman Rank Correlations:\")\n",
    "print(f\"  S vs T: {corr_st:.3f}\")\n",
    "print(f\"  S vs X: {corr_sx:.3f}\")\n",
    "print(f\"  T vs X: {corr_tx:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "fig, axes = create_tufte_figure(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, (c1, c2, n1, n2) in zip(axes, [\n",
    "    (cate_s, cate_t, 'S-Learner', 'T-Learner'),\n",
    "    (cate_s, cate_x, 'S-Learner', 'X-Learner'),\n",
    "    (cate_t, cate_x, 'T-Learner', 'X-Learner'),\n",
    "]):\n",
    "    ax.scatter(c1, c2, alpha=0.3, s=15, c=TUFTE_PALETTE['primary'])\n",
    "    ax.plot([c1.min(), c1.max()], [c1.min(), c1.max()], 'k--', lw=1.5, alpha=0.5)\n",
    "    ax.set_xlabel(n1)\n",
    "    ax.set_ylabel(n2)\n",
    "    corr, _ = spearmanr(c1, c2)\n",
    "    ax.set_title(f'Rank Corr: {corr:.3f}', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Learner Selection Guidelines\n",
    "\n",
    "```\n",
    "Meta-Learner Selection Guide ─────────────────────────────────\n",
    "\n",
    "┌─────────────────┐\n",
    "│ Is treatment    │\n",
    "│ continuous?     │\n",
    "└────────┬────────┘\n",
    "         │\n",
    "    ┌────┴────┐\n",
    "    │         │\n",
    "   YES        NO\n",
    "    │         │\n",
    "    ▼         ▼\n",
    "┌────────┐ ┌─────────────────┐\n",
    "│S-Learner│ │ Are treatment   │\n",
    "│or       │ │ groups balanced?│\n",
    "│R-Learner│ └────────┬────────┘\n",
    "└─────────┘          │\n",
    "                ┌────┴────┐\n",
    "                │         │\n",
    "               YES        NO\n",
    "                │         │\n",
    "                ▼         ▼\n",
    "         ┌──────────┐ ┌────────┐\n",
    "         │T-Learner │ │X-Learner│\n",
    "         │or        │ │(handles │\n",
    "         │S-Learner │ │imbalance)│\n",
    "         └──────────┘ └─────────┘\n",
    "\n",
    "Additional Considerations:\n",
    "\n",
    "• S-Learner:\n",
    "  - Try first (simplest)\n",
    "  - Check if treatment feature is kept/used\n",
    "  - Watch for regularization bias\n",
    "\n",
    "• T-Learner:\n",
    "  - Good when groups similar size\n",
    "  - Each model uses less data\n",
    "  - Can overfit with small groups\n",
    "\n",
    "• X-Learner:\n",
    "  - Best for imbalanced groups\n",
    "  - More complex, more robust\n",
    "  - Requires propensity model\n",
    "\n",
    "• R-Learner (next notebook):\n",
    "  - Most robust (orthogonal)\n",
    "  - Works with continuous T\n",
    "  - Connection to DML\n",
    "──────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary = pd.DataFrame({\n",
    "    'Learner': ['S-Learner', 'T-Learner', 'X-Learner'],\n",
    "    'Mean CATE': [cate_s.mean(), cate_t.mean(), cate_x.mean()],\n",
    "    'Std CATE': [cate_s.std(), cate_t.std(), cate_x.std()],\n",
    "    'AUUC': [auuc(gain_s), auuc(gain_t), auuc(gain_x)],\n",
    "    'Complexity': ['Low', 'Medium', 'High'],\n",
    "    'Continuous T': ['Yes', 'No', 'No'],\n",
    "})\n",
    "\n",
    "print(\"Meta-Learner Summary:\")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interview Appendix\n",
    "\n",
    "### Practice Questions\n",
    "\n",
    "**Q1 (Google L5)**: *\"How would you evaluate a CATE model if you have access to RCT data?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Key approaches with RCT data**:\n",
    "\n",
    "1. **Cumulative Gain / Uplift Curves**:\n",
    "   - Sort test units by predicted CATE (highest first)\n",
    "   - Compute cumulative ATE as you include more units\n",
    "   - Higher curve = better targeting\n",
    "   - AUUC (Area Under Uplift Curve) as single metric\n",
    "\n",
    "2. **Qini Coefficient**:\n",
    "   - Similar to AUUC, normalized differently\n",
    "   - Compares to random targeting baseline\n",
    "\n",
    "3. **Group-wise ATE**:\n",
    "   - Split by CATE quantiles (e.g., Q1-Q4)\n",
    "   - Compute ATE within each group\n",
    "   - Good model: High CATE group has high realized ATE\n",
    "\n",
    "4. **Calibration**:\n",
    "   - Plot predicted CATE vs. realized CATE by bins\n",
    "   - Perfect calibration: 45-degree line\n",
    "\n",
    "**Why RCT matters**: In RCT, $T \\perp\\!\\!\\!\\perp X$, so we can compute unbiased group ATEs.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "**Q2 (Netflix, Senior DS)**: *\"When would you use X-learner over T-learner?\"*\n",
    "\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "\n",
    "**Use X-learner when**:\n",
    "\n",
    "1. **Treatment groups are imbalanced** (main reason)\n",
    "   - Example: 10% treated, 90% control\n",
    "   - T-learner suffers from different regularization levels\n",
    "   - X-learner uses propensity weighting to correct\n",
    "\n",
    "2. **You have a good propensity model**\n",
    "   - X-learner requires estimating $e(x) = P(T=1|X)$\n",
    "   - If propensity is hard to model, benefit may be limited\n",
    "\n",
    "3. **True CATE has similar complexity across groups**\n",
    "   - X-learner assumes the treatment effect function is similar\n",
    "   - Propensity weighting blends two effect models\n",
    "\n",
    "**Use T-learner when**:\n",
    "- Groups are balanced\n",
    "- Outcome models differ fundamentally by treatment status\n",
    "- Simpler model is sufficient\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. References\n",
    "\n",
    "[^1]: Facure, M. (2023). *Causal Inference for the Brave and True*. Chapter 21: \"Meta-Learners.\"\n",
    "\n",
    "[^2]: Kunzel, S. R. et al. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. *PNAS*.\n",
    "\n",
    "[^3]: Gutierrez, P. and Gerardy, J. Y. (2017). Causal Inference and Uplift Modelling: A Review. *JMLR Workshop*.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [04. R-Learner Introduction](./04_r_learner_intro.ipynb) — The orthogonal learner and bridge to DML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
